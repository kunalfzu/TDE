{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c70f84-6237-48b0-9665-fc8952a5a3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb06a16-7831-4d43-b9b0-d9dda53703d0",
   "metadata": {},
   "source": [
    "## Params, Imports, Read data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d042035-6994-4d10-ab73-8ba27b10abde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, vstack, MaskedColumn\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy import units as u\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import seaborn as sns\n",
    "import george\n",
    "from george import kernels\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import iqr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from collections import defaultdict\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import viridis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import re\n",
    "import time\n",
    "import shap\n",
    "#from dask import delayed, compute\n",
    "#from dask.distributed import Client, LocalCluster\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "import gc  # For garbage collection\n",
    "\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "\n",
    "# Use get_cmap from pyplot\n",
    "get_cmap = plt.get_cmap\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='data_conversion_debug.log',  # Log file name\n",
    "    filemode='w',                           # Overwrite the log file each run\n",
    "    level=logging.DEBUG,                    # Capture all levels of logs\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Log format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1ff37-c638-4f7e-b6ac-10d4d521f54a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define LSST band colors and effective wavelengths (positions for data points along wavelength direction)\n",
    "\n",
    "lsst_bands = {\n",
    "    \"u\": 3670.69,\n",
    "    \"g\": 4826.85,\n",
    "    \"r\": 6223.24,\n",
    "    \"i\": 7545.98,\n",
    "    \"z\": 8590.90,\n",
    "    \"Y\": 9710.28\n",
    "}\n",
    "\n",
    "\n",
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"Y\"]\n",
    "\n",
    "band_colors = {'u': 'blue', 'g': 'green', 'r': 'red', 'i': 'purple', 'z': 'brown', 'Y': 'yellow'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a322dc2-9d49-4767-9f58-12bdcafc2eb6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_elasticc_file(filename):\n",
    "    if '_PHOT' in filename:\n",
    "        headname = filename.replace('_PHOT', '_HEAD')\n",
    "    else:\n",
    "        headname = filename\n",
    "        filename = filename.replace('_HEAD', '_PHOT')\n",
    "\n",
    "    # Debug prints to verify paths\n",
    "    #print(f\"reading phot file: {filename}\")\n",
    "    print(f\"reading head file: {headname}\")\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "    if not os.path.exists(headname):\n",
    "        raise FileNotFoundError(f\"File not found: {headname}\")\n",
    "\n",
    "    table = Table.read(filename)\n",
    "    head = Table.read(headname)\n",
    "\n",
    "    # Sanitize the data\n",
    "    for _ in table:\n",
    "        _['BAND'] = _['BAND'].strip()\n",
    "\n",
    "    head['SNID'] = np.int64(head['SNID'])\n",
    "    \n",
    "    # Sanity check \n",
    "    if np.sum(table['MJD'] < 0) != len(head):\n",
    "        print(filename, 'is broken:', np.sum(table['MJD'] < 0), '!=', len(head))\n",
    "        \n",
    "    # Measured mag and magerr - simulated one is in SIM_MAGOBS\n",
    "    table['mag'] = np.nan\n",
    "    table['magerr'] = np.nan\n",
    "    idx = table['FLUXCAL'] > 0\n",
    "    \n",
    "    table['mag'][idx] = 27.5 - 2.5 * np.log10(table['FLUXCAL'][idx])\n",
    "    table['magerr'][idx] = 2.5 / np.log(10) * table['FLUXCALERR'][idx] / table['FLUXCAL'][idx]\n",
    "    \n",
    "    # Augment table with SNID (light curve id) from head\n",
    "    table['SNID'] = 0\n",
    "    \n",
    "    idx = np.where(table['MJD'] < 0)[0]\n",
    "    idx = np.hstack((np.array([0]), idx))\n",
    "\n",
    "    for i in range(1, len(idx)):\n",
    "        i0, i1 = idx[i - 1], idx[i]\n",
    "        table['SNID'][i0:i1] = head['SNID'][i - 1]\n",
    "    \n",
    "    table = table[table['MJD'] > 0]\n",
    "\n",
    "    return table, head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f7977-a70f-4027-b2c6-67004f9215e7",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38a3e9-98c2-4e70-9b8e-64851c36c205",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_snid_head_sub(table, head):\n",
    "    snids = np.unique(table['SNID'])\n",
    "    shead_list = [head[head['SNID'] == snid] for snid in snids]\n",
    "    sub_list = [table[table['SNID'] == snid] for snid in snids]\n",
    "    return snids, shead_list, sub_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f20788-20a3-41fc-9976-ca62ff07cb7a",
   "metadata": {},
   "source": [
    "### compute GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd21c33-9150-4183-ac06-323fa425b05e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def try_optimization(gp, snid, neg_ln_like, grad_neg_ln_like, initial_guess, retries=3):\n",
    "    best_result = None\n",
    "    for attempt in range(retries):\n",
    "        result = minimize(neg_ln_like, initial_guess, jac=grad_neg_ln_like, method='L-BFGS-B')\n",
    "        if best_result is None or (result.success and result.fun < best_result.fun):\n",
    "            best_result = result\n",
    "        if result.success:\n",
    "            break\n",
    "        else:\n",
    "            # Slightly perturb the initial guess for the next attempt\n",
    "            initial_guess += np.random.normal(0, 1e-2, size=initial_guess.shape)\n",
    "    \n",
    "    if best_result is None or not best_result.success:\n",
    "        print(f\"All optimization attempts failed for SNID {snid}\")\n",
    "        gp.set_parameter_vector(initial_guess)  # Use the best guess available\n",
    "    else:\n",
    "        gp.set_parameter_vector(best_result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095bebe-781c-422f-b031-f6bbe7f4b3e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_gp(sub, snid, verbose=False):\n",
    "    try:\n",
    "        # Ensure inputs are numpy arrays with appropriate dtype\n",
    "        t = np.array(sub['MJD'], dtype=float)\n",
    "        flux = np.array(sub['FLUXCAL'], dtype=float)\n",
    "        fluxerr = np.array(sub['FLUXCALERR'], dtype=float)\n",
    "        band = np.array([lsst_bands.get(b) for b in sub['BAND']], dtype=float)\n",
    "        \n",
    "        # 2D positions of data points (time and wavelength)\n",
    "        x = np.vstack([t, band]).T\n",
    "        \n",
    "        # Clean the data: remove rows with NaNs, infs, and non-positive flux values\n",
    "        mask = np.isfinite(flux) & np.isfinite(fluxerr) & np.all(np.isfinite(x), axis=1) & (flux > 0)\n",
    "        x = x[mask]\n",
    "        flux = flux[mask]\n",
    "        fluxerr = fluxerr[mask]\n",
    "\n",
    "        if len(flux) < 5:  # Ensure there are enough data points\n",
    "            raise ValueError(\"Not enough data points to perform GP fitting.\")\n",
    "        \n",
    "        signal_to_noises = np.abs(flux) / np.sqrt(fluxerr ** 2 + (1e-2 * np.max(flux)) ** 2)\n",
    "        scale = np.abs(flux[np.argmax(signal_to_noises)])\n",
    "\n",
    "        # Define the kernel\n",
    "        kernel = (0.5 * scale) ** 2 * george.kernels.Matern32Kernel([100 ** 2, 6000 ** 2], ndim=2)\n",
    "        \n",
    "        # Define the GP model with HODLR solver and white noise\n",
    "        gp = george.GP(kernel, solver=george.HODLRSolver)\n",
    "        \n",
    "        # Compute the GP\n",
    "        gp.compute(x, fluxerr)\n",
    "        \n",
    "        # Define the negative log likelihood and its gradient\n",
    "        def neg_ln_like(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.log_likelihood(flux)\n",
    "        \n",
    "        def grad_neg_ln_like(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.grad_log_likelihood(flux)\n",
    "        \n",
    "        # Attempt optimization with multiple initial guesses\n",
    "        initial_guess = gp.get_parameter_vector()\n",
    "        try_optimization(gp, snid, neg_ln_like, grad_neg_ln_like, initial_guess, retries=3)\n",
    "        \n",
    "        # Return the GP, flux, data points, and final parameters\n",
    "        return gp, flux, x, gp.get_parameter_vector()\n",
    "\n",
    "    except (ValueError, np.linalg.LinAlgError, Exception) as e:\n",
    "        print(f\"GP optimization failed for SNID {snid}: {e}\")\n",
    "        return None, None, None, None  # Return None values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53235186-0cc1-4e7f-8272-b14db3cdc65f",
   "metadata": {},
   "source": [
    "### peak and rise/fade estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8113a-a6f4-4b93-b3f9-ccfd86f2a24e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_peak_and_calculate_times(gp, sub, flux, band1='g', band2='r', peak_threshold=2.512):\n",
    "    \"\"\"\n",
    "    Find the peak using Gaussian Process predictions for two input bands (band1 and band2).\n",
    "    Then calculate the rise and fade times relative to the peak for each band.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate MJD values for predictions\n",
    "    t_min, t_max = sub['MJD'].min(), sub['MJD'].max()\n",
    "\n",
    "    # Variables to hold peak information for both bands\n",
    "    peak_mjd_band1, peak_flux_band1, rise_time_band1, fade_time_band1 = None, None, None, None\n",
    "    peak_mjd_band2, peak_flux_band2, rise_time_band2, fade_time_band2 = None, None, None, None\n",
    "\n",
    "    # Loop over the two input bands (band1 and band2)\n",
    "    for band in [band1, band2]:\n",
    "        mjd_for_pred = np.linspace(t_min - 50, t_max + 75, 1000)\n",
    "        wavelength = lsst_bands[band]\n",
    "        x_pred = np.vstack([mjd_for_pred, wavelength * np.ones_like(mjd_for_pred)]).T\n",
    "\n",
    "        # Predict flux at these times using the GP model\n",
    "        mean_pred, _ = gp.predict(flux, x_pred, return_var=True)\n",
    "\n",
    "        # Find the time of peak flux\n",
    "        peak_flux_idx = np.argmax(mean_pred)\n",
    "        peak_mjd = mjd_for_pred[peak_flux_idx]\n",
    "        peak_flux = mean_pred[peak_flux_idx]\n",
    "\n",
    "        # Calculate rise and fade times\n",
    "        one_flux_fainter = peak_flux / peak_threshold  # Threshold flux for 1 mag fainter\n",
    "\n",
    "        # Rise time: time it took to rise from the faint threshold to the peak\n",
    "        rise_time = None\n",
    "        try:\n",
    "            rise_time_idx = np.where(mean_pred[:peak_flux_idx] <= one_flux_fainter)[0]\n",
    "            if len(rise_time_idx) > 0:\n",
    "                rise_time = peak_mjd - mjd_for_pred[rise_time_idx[-1]]\n",
    "        except IndexError:\n",
    "            rise_time = None  # If no valid rise time found\n",
    "\n",
    "        # Fade time: time it takes to decay from peak to the faint threshold\n",
    "        fade_time = None\n",
    "        try:\n",
    "            fade_time_idx = np.where(mean_pred[peak_flux_idx:] <= one_flux_fainter)[0]\n",
    "            if len(fade_time_idx) > 0:\n",
    "                fade_time = mjd_for_pred[peak_flux_idx + fade_time_idx[0]] - peak_mjd\n",
    "        except IndexError:\n",
    "            fade_time = None  # If no valid fade time found\n",
    "\n",
    "        # Store results for the respective band\n",
    "        if band == band1:\n",
    "            peak_mjd_band1, peak_flux_band1, rise_time_band1, fade_time_band1 = peak_mjd, peak_flux, rise_time, fade_time\n",
    "        elif band == band2:\n",
    "            peak_mjd_band2, peak_flux_band2, rise_time_band2, fade_time_band2 = peak_mjd, peak_flux, rise_time, fade_time\n",
    "\n",
    "    return (peak_mjd_band1, peak_flux_band1, rise_time_band1, fade_time_band1, \n",
    "            peak_mjd_band2, peak_flux_band2, rise_time_band2, fade_time_band2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f066dcf-7f31-4029-9c08-db8ec5ac0fce",
   "metadata": {},
   "source": [
    "### color estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544a27a-ae58-4dd3-89d2-e47b86327882",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_color(gp, flux, x, x1, band1, band2):\n",
    "    x_band1 = np.vstack([x1, lsst_bands[band1] * np.ones_like(x1)]).T\n",
    "    x_band2 = np.vstack([x1, lsst_bands[band2] * np.ones_like(x1)]).T\n",
    "\n",
    "    # Predict fluxes and their variances for both bands\n",
    "    flux_pred_band1, fluxvar_band1 = gp.predict(flux, x_band1, return_var=True)\n",
    "    flux_pred_band2, fluxvar_band2 = gp.predict(flux, x_band2, return_var=True)\n",
    "\n",
    "    # Ensure positive flux predictions by clipping at a small positive value\n",
    "    flux_pred_band1_clipped = np.clip(flux_pred_band1, 1e-10, None)\n",
    "    flux_pred_band2_clipped = np.clip(flux_pred_band2, 1e-10, None)\n",
    "\n",
    "    # Ensure that variances are non-negative\n",
    "    fluxvar_band1_clipped = np.clip(fluxvar_band1, 0, None)\n",
    "    fluxvar_band2_clipped = np.clip(fluxvar_band2, 0, None)\n",
    "\n",
    "    # Calculate magnitudes from fluxes\n",
    "    mag_band1 = -2.5 * np.log10(flux_pred_band1_clipped)\n",
    "    mag_band2 = -2.5 * np.log10(flux_pred_band2_clipped)\n",
    "    \n",
    "    # Calculate errors in magnitudes\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        magerr_band1 = 2.5 / np.log(10) * (np.sqrt(fluxvar_band1_clipped) / flux_pred_band1_clipped)\n",
    "        magerr_band2 = 2.5 / np.log(10) * (np.sqrt(fluxvar_band2_clipped) / flux_pred_band2_clipped)\n",
    "\n",
    "    # Calculate color as the difference in magnitudes\n",
    "    color = mag_band1 - mag_band2\n",
    "    \n",
    "    # Calculate the error in color\n",
    "    color_err = np.sqrt(magerr_band1**2 + magerr_band2**2)\n",
    "\n",
    "    # Create a mask to select only non-NaN and finite elements\n",
    "    valid_mask = np.isfinite(color) & np.isfinite(color_err)\n",
    "\n",
    "    # Apply the mask to color, color_err arrays, and x1\n",
    "    color = color[valid_mask]\n",
    "    color_err = color_err[valid_mask]\n",
    "    x1 = x1[valid_mask]\n",
    "\n",
    "    return x1, color, color_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74c1e2-087c-432c-8ba3-773f11bf90f4",
   "metadata": {},
   "source": [
    "### mean color and color evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5a28e-cd22-4d5f-a775-e09244e98d5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def weighted_mean_std(value, error, min_error=1e-6):\n",
    "    \"\"\"\n",
    "    Calculates the weighted mean and weighted standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "        value (array-like): Data values.\n",
    "        error (array-like): Error values corresponding to the data.\n",
    "        min_error (float): Minimum allowable error to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (weighted_mean, weighted_std)\n",
    "    \"\"\"\n",
    "    # Replace zero or negative errors with min_error\n",
    "    error_safe = np.where(error > 0, error, min_error)\n",
    "    \n",
    "    # Calculate weights\n",
    "    weight = 1 / error_safe**2\n",
    "    \n",
    "    # Handle cases where weights might still be invalid (e.g., due to overflow)\n",
    "    weight = np.where(np.isfinite(weight), weight, 0)\n",
    "    \n",
    "    # Calculate the sum of weights\n",
    "    sum_weights = np.sum(weight)\n",
    "    \n",
    "    if sum_weights == 0:\n",
    "        # If all weights are zero, return NaN\n",
    "        weighted_mean = np.nan\n",
    "        weighted_std = np.nan\n",
    "    else:\n",
    "        # Calculate weighted mean\n",
    "        weighted_mean = np.sum(value * weight) / sum_weights\n",
    "        \n",
    "        # Calculate weighted standard deviation\n",
    "        weighted_std = np.sqrt(1 / sum_weights)\n",
    "    \n",
    "    return weighted_mean, weighted_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda41916-8c20-4110-ae9e-2e41a8b9de1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_mean_colors_and_slope(sub, gp, flux, x, band1, band2, rise_time, fade_time, t_peak):\n",
    "    \"\"\"\n",
    "    Calculate mean colors, slopes, and plot color evolution for a given SNID using rise and fade times.\n",
    "    Fallback to original logic if rise_time or fade_time is not available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fallback to default rise_time and fade_time if not valid\n",
    "    calc_rise_time = 50 if rise_time is None or not np.isfinite(rise_time) else rise_time\n",
    "    calc_fade_time = 75 if fade_time is None or not np.isfinite(fade_time) else fade_time\n",
    "\n",
    "    ### Pre-peak calculations using the rise time or default window\n",
    "    try:\n",
    "        indices_pre_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                           (sub['MJD'] >= t_peak - calc_rise_time) & (sub['MJD'] <= t_peak)\n",
    "        mjd_pre_peak = sub['MJD'][indices_pre_peak]\n",
    "\n",
    "        # Calculate color and filter valid data\n",
    "        x_pre_peak, color_pre_peak, color_err_pre_peak = calc_color(gp, flux, x, mjd_pre_peak, band1, band2)\n",
    "\n",
    "        # Ensure we only apply np.isfinite() to numeric data\n",
    "        mask_pre = np.isfinite(x_pre_peak) & np.isfinite(color_pre_peak) & (color_err_pre_peak < 1)\n",
    "\n",
    "        # Apply mask to filter out invalid data\n",
    "        x_pre_peak, color_pre_peak, color_err_pre_peak = x_pre_peak[mask_pre], color_pre_peak[mask_pre], color_err_pre_peak[mask_pre]\n",
    "\n",
    "    except Exception as e:\n",
    "        # If pre-peak data fails, set all pre-peak values to None\n",
    "        x_pre_peak, color_pre_peak, color_err_pre_peak = None, None, None\n",
    "\n",
    "    ### Post-peak calculations using the fade time or default window\n",
    "    try:\n",
    "        indices_post_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                            (sub['MJD'] > t_peak) & (sub['MJD'] <= t_peak + calc_fade_time)\n",
    "        mjd_post_peak = sub['MJD'][indices_post_peak]\n",
    "\n",
    "        # Calculate color and filter valid data\n",
    "        x_post_peak, color_post_peak, color_err_post_peak = calc_color(gp, flux, x, mjd_post_peak, band1, band2)\n",
    "\n",
    "        # Ensure we only apply np.isfinite() to numeric data\n",
    "        mask_post = np.isfinite(x_post_peak) & np.isfinite(color_post_peak) & (color_err_post_peak < 1)\n",
    "\n",
    "        # Apply mask to filter out invalid data\n",
    "        x_post_peak, color_post_peak, color_err_post_peak = x_post_peak[mask_post], color_post_peak[mask_post], color_err_post_peak[mask_post]\n",
    "\n",
    "    except Exception as e:\n",
    "        # If post-peak data fails, set all post-peak values to None\n",
    "        x_post_peak, color_post_peak, color_err_post_peak = None, None, None\n",
    "\n",
    "    ### Calculate weighted mean for pre-peak and post-peak colors\n",
    "    mean_color_pre_peak, std_err_mean_color_pre_peak = None, None\n",
    "    if color_pre_peak is not None and len(color_pre_peak) > 0:\n",
    "        mean_color_pre_peak, std_err_mean_color_pre_peak = weighted_mean_std(color_pre_peak, color_err_pre_peak)\n",
    "\n",
    "    mean_color_post_peak, std_err_mean_color_post_peak = None, None\n",
    "    if color_post_peak is not None and len(color_post_peak) > 0:\n",
    "        mean_color_post_peak, std_err_mean_color_post_peak = weighted_mean_std(color_post_peak, color_err_post_peak)\n",
    "\n",
    "    ### Calculate slopes for pre-peak and post-peak\n",
    "    slope_pre_peak, slope_err_pre_peak = None, None\n",
    "    if x_pre_peak is not None and len(x_pre_peak) >= 2:\n",
    "        try:\n",
    "            # Replace zero or very small color_err_pre_peak with a minimum threshold\n",
    "            color_err_pre_peak_safe = np.where(color_err_pre_peak > 0, color_err_pre_peak, 1e-6)\n",
    "            \n",
    "            # Calculate weights safely\n",
    "            weights_pre = 1 / color_err_pre_peak_safe**2\n",
    "            weights_pre = np.where(np.isfinite(weights_pre), weights_pre, 0)\n",
    "            \n",
    "            # Check if sum of weights is zero\n",
    "            if np.sum(weights_pre) == 0:\n",
    "                slope_pre_peak, slope_err_pre_peak = np.nan, np.nan\n",
    "            else:\n",
    "                p_pre_peak, cov_pre_peak = np.polyfit(\n",
    "                    x_pre_peak, color_pre_peak, 1, w=weights_pre, cov=True\n",
    "                )\n",
    "                slope_pre_peak, _ = p_pre_peak\n",
    "                slope_err_pre_peak = np.sqrt(cov_pre_peak[0, 0])  # Standard error of the slope\n",
    "        except Exception as e:\n",
    "            slope_pre_peak, slope_err_pre_peak = np.nan, np.nan\n",
    "\n",
    "    slope_post_peak, slope_err_post_peak = None, None\n",
    "    if x_post_peak is not None and len(x_post_peak) >= 2:\n",
    "        try:\n",
    "            # Replace zero or very small color_err_post_peak with a minimum threshold\n",
    "            color_err_post_peak_safe = np.where(color_err_post_peak > 0, color_err_post_peak, 1e-6)\n",
    "            \n",
    "            # Calculate weights safely\n",
    "            weights_post = 1 / color_err_post_peak_safe**2\n",
    "            weights_post = np.where(np.isfinite(weights_post), weights_post, 0)\n",
    "            \n",
    "            # Check if sum of weights is zero\n",
    "            if np.sum(weights_post) == 0:\n",
    "                slope_post_peak, slope_err_post_peak = np.nan, np.nan\n",
    "            else:\n",
    "                p_post_peak, cov_post_peak = np.polyfit(\n",
    "                    x_post_peak, color_post_peak, 1, w=weights_post, cov=True\n",
    "                )\n",
    "                slope_post_peak, _ = p_post_peak\n",
    "                slope_err_post_peak = np.sqrt(cov_post_peak[0, 0])  # Standard error of the slope\n",
    "        except Exception as e:\n",
    "            slope_post_peak, slope_err_post_peak = np.nan, np.nan\n",
    "\n",
    "    return (\n",
    "        mean_color_pre_peak, std_err_mean_color_pre_peak, slope_pre_peak, slope_err_pre_peak,\n",
    "        mean_color_post_peak, std_err_mean_color_post_peak, slope_post_peak, slope_err_post_peak\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebd377-f4b5-4990-bc4f-f928c473c4ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "\n",
    "# Example file path (replace with the actual file path if necessary)\n",
    "example_file = \"../../../karpov/ELASTICC2/ELASTICC2_FINAL_TDE/ELASTICC2_FINAL_NONIaMODEL0-0001_HEAD.FITS.gz\"\n",
    "\n",
    "# Read the file\n",
    "with fits.open(example_file) as hdul:\n",
    "    data = hdul[1].data  # Assuming the data is in the second HDU\n",
    "    column_names = data.columns.names\n",
    "\n",
    "# Print the column names vertically\n",
    "for col_name in column_names:\n",
    "    print(col_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71ddf3-052e-46e8-b771-2f989ada6c60",
   "metadata": {},
   "source": [
    "## Main processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549da360-5f64-45e0-8f45-5345e71a938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd285fa-6c27-46fd-b091-1502094df82f",
   "metadata": {},
   "source": [
    "### multiprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2804036a-969b-426c-bf3b-016e4d8f549d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Intermediate save function for results (thread-safe)\n",
    "def save_intermediate_results(data, filename):\n",
    "    if data:\n",
    "        print(f\"Saving intermediate results to {filename}...\")\n",
    "        df = pd.DataFrame(data)\n",
    "        # Use a lock to prevent write conflicts\n",
    "        with open(filename, 'a') as f:\n",
    "            df.to_csv(f, header=f.tell()==0, index=False)\n",
    "        print(f\"Saved {len(data)} entries.\")\n",
    "    else:\n",
    "        print(f\"No data to save in {filename}.\")\n",
    "\n",
    "# Intermediate save function for errors (thread-safe)\n",
    "def save_intermediate_errors(errors, filename):\n",
    "    if errors:\n",
    "        print(f\"Saving intermediate error log to {filename}...\")\n",
    "        df = pd.DataFrame(errors, columns=['SNID', 'Error'])\n",
    "        # Use a lock to prevent write conflicts\n",
    "        with open(filename, 'a') as f:\n",
    "            df.to_csv(f, header=f.tell()==0, index=False)\n",
    "        print(f\"Saved {len(errors)} errors.\")\n",
    "    else:\n",
    "        print(f\"No errors to save in {filename}.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9956d9b-ab9b-427c-82d9-f0218c6c379c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Function to read data for a specific SNID from HDF5\n",
    "def read_data_for_snid(hdf5_filename, snid):\n",
    "    with pd.HDFStore(hdf5_filename, 'r') as store:\n",
    "        # Query the photometry data for the specific SNID\n",
    "        df_sub = store.select('photometry', where='SNID == snid')\n",
    "        # Get the header data for the SNID\n",
    "        df_shead = store.select('header', where='SNID == snid')\n",
    "\n",
    "    # Convert back to Astropy Tables\n",
    "    sub = Table.from_pandas(df_sub)\n",
    "    shead = Table.from_pandas(df_shead)\n",
    "    return sub, shead\n",
    "\n",
    "    # Convert back to Astropy Tables \n",
    "    if df_sub.empty:\n",
    "        logging.warning(f\"No photometry data found for SNID {snid} in {hdf5_filename}.\")\n",
    "        sub = None\n",
    "    else:\n",
    "        sub = Table.from_pandas(df_sub)\n",
    "\n",
    "    if df_shead.empty:\n",
    "        logging.warning(f\"No header data found for SNID {snid} in {hdf5_filename}.\")\n",
    "        shead = None\n",
    "    else:\n",
    "        shead = Table.from_pandas(df_shead)\n",
    "\n",
    "    return sub, shead\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10e503f7-4173-45b8-ab34-19560dc37fcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "def process_snid(args):\n",
    "    \"\"\"\n",
    "    Processes a single SNID: reads data, performs computations, and returns results or errors.\n",
    "\n",
    "    Parameters:\n",
    "    - args (tuple): Contains (snid, hdf5_filename, object_type)\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (entry_dict, error_tuple)\n",
    "    \"\"\"\n",
    "    snid, hdf5_filename, object_type = args\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        # Read the data for the specific SNID from the HDF5 file\n",
    "        sub, shead = read_data_for_snid(hdf5_filename, snid)\n",
    "\n",
    "        if shead is None or len(shead) == 0:\n",
    "            raise ValueError(f\"No header data found for SNID {snid}\")\n",
    "\n",
    "        if 'PEAKMJD' not in shead.colnames:\n",
    "            raise ValueError(f\"'PEAKMJD' is missing in 'shead' for SNID {snid}\")\n",
    "\n",
    "        # Compute SNR of each data point\n",
    "        sub['SNR'] = sub['FLUXCAL'] / sub['FLUXCALERR']\n",
    "\n",
    "        # DETECTION CHECK: pass only if there are at least 3 points with SNR > 5 and flux > 50 in any bands\n",
    "        snr_flux_mask = ((sub['SNR'] > 5) & (sub['FLUXCAL'] > 100)).sum() >= 3\n",
    "        if not snr_flux_mask:\n",
    "            return None, (snid, \"Failed detection check\")\n",
    "\n",
    "        # Compute the Gaussian Process (GP)\n",
    "        gp, flux, x, hyperparameters = compute_gp(sub, snid)\n",
    "        if gp is None:\n",
    "            return None, (snid, \"GP optimization failure\")\n",
    "\n",
    "        # Find the peak and calculate rise and fade times for both band1 (g) and band2 (r)\n",
    "        peak_mjd_band1, peak_flux_band1, rise_time_band1, fade_time_band1, \\\n",
    "        peak_mjd_band2, peak_flux_band2, rise_time_band2, fade_time_band2 = \\\n",
    "            find_peak_and_calculate_times(gp, sub, flux, 'g', 'r')\n",
    "\n",
    "        if peak_mjd_band1 is None or peak_mjd_band2 is None:\n",
    "            return None, (snid, \"Peak finding failure\")\n",
    "\n",
    "        # Calculate mean colors and slopes using the peak info from band1 (g) for g-r\n",
    "        mean_color_pre_peak_gr, std_err_mean_color_pre_peak_gr, slope_pre_peak_gr, slope_err_pre_peak_gr, \\\n",
    "        mean_color_post_peak_gr, std_err_mean_color_post_peak_gr, slope_post_peak_gr, slope_err_post_peak_gr = \\\n",
    "            calc_mean_colors_and_slope(sub, gp, flux, x, 'g', 'r', rise_time_band1, fade_time_band1, peak_mjd_band1)\n",
    "\n",
    "        # Calculate mean colors and slopes using the peak info from band2 (r) for r-i\n",
    "        mean_color_pre_peak_ri, std_err_mean_color_pre_peak_ri, slope_pre_peak_ri, slope_err_pre_peak_ri, \\\n",
    "        mean_color_post_peak_ri, std_err_mean_color_post_peak_ri, slope_post_peak_ri, slope_err_post_peak_ri = \\\n",
    "            calc_mean_colors_and_slope(sub, gp, flux, x, 'r', 'i', rise_time_band1, fade_time_band1, peak_mjd_band1)\n",
    "\n",
    "        # Create dictionary entry with all relevant values\n",
    "        entry = {\n",
    "            'SNID': snid,\n",
    "            'Object_Type': object_type,\n",
    "            'Amplitude': hyperparameters[0],\n",
    "            'LengthScale_Time': hyperparameters[1],\n",
    "            'LengthScale_Wavelength': hyperparameters[2],\n",
    "            'Rise_Time': rise_time_band1,\n",
    "            'Fade_Time': fade_time_band1,\n",
    "            'TruePeakMJD': shead['PEAKMJD'][0],\n",
    "            'peak_time_MJD': peak_mjd_band1,\n",
    "\n",
    "            # g-r colors and slopes\n",
    "            'Mean_Color_Pre_Peak_gr': mean_color_pre_peak_gr,\n",
    "            'Pre_Peak_Color_err_gr': std_err_mean_color_pre_peak_gr,\n",
    "            'Mean_Color_Post_Peak_gr': mean_color_post_peak_gr,\n",
    "            'Post_Peak_Color_err_gr': std_err_mean_color_post_peak_gr,\n",
    "            'Slope_Pre_Peak_gr': slope_pre_peak_gr,\n",
    "            'Slope_Err_Pre_Peak_gr': slope_err_pre_peak_gr,\n",
    "            'Slope_Post_Peak_gr': slope_post_peak_gr,\n",
    "            'Slope_Err_Post_Peak_gr': slope_err_post_peak_gr,\n",
    "\n",
    "            # r-i colors and slopes\n",
    "            'Mean_Color_Pre_Peak_ri': mean_color_pre_peak_ri,\n",
    "            'Pre_Peak_Color_err_ri': std_err_mean_color_pre_peak_ri,\n",
    "            'Mean_Color_Post_Peak_ri': mean_color_post_peak_ri,\n",
    "            'Post_Peak_Color_err_ri': std_err_mean_color_post_peak_ri,\n",
    "            'Slope_Pre_Peak_ri': slope_pre_peak_ri,\n",
    "            'Slope_Err_Pre_Peak_ri': slope_err_pre_peak_ri,\n",
    "            'Slope_Post_Peak_ri': slope_post_peak_ri,\n",
    "            'Slope_Err_Post_Peak_ri': slope_err_post_peak_ri,\n",
    "        }\n",
    "\n",
    "        # Clear variables to free memory\n",
    "        del gp, flux, x, hyperparameters, sub, shead\n",
    "        gc.collect()  # Call garbage collector\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Processed SNID {snid} in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        return entry, None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing SNID {snid}: {e}\")\n",
    "        return None, (snid, str(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f010a415-1fd3-4c80-98d3-429d387a4c3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def process_file(hdf5_filename, object_type, intermediate_results_file, error_log_file, max_workers=64, task_timeout=10):\n",
    "    \"\"\"\n",
    "    Processes a single HDF5 file: reads SNIDs, processes them in batches using concurrent.futures with timeouts.\n",
    "\n",
    "    Parameters:\n",
    "        hdf5_filename (str): Path to the HDF5 file.\n",
    "        object_type (str): The object type (e.g., 'SNIa-SALT3').\n",
    "        intermediate_results_file (str): Path to the intermediate results CSV file.\n",
    "        error_log_file (str): Path to the error log CSV file.\n",
    "        max_workers (int): Maximum number of worker processes.\n",
    "        task_timeout (int): Timeout in seconds for each SNID processing task.\n",
    "    \"\"\"\n",
    "    print(f\"Processing file {hdf5_filename} for object type {object_type}\")\n",
    "    batch_size = 500  # Adjust as needed\n",
    "\n",
    "    try:\n",
    "        # Read the list of SNIDs from the HDF5 file\n",
    "        with pd.HDFStore(hdf5_filename, 'r') as store:\n",
    "            snid_series = store.select_column('header', 'SNID')\n",
    "            snids = snid_series.unique()\n",
    "        print(f\"Found {len(snids)} SNIDs in the file {hdf5_filename}.\")\n",
    "\n",
    "        num_snids = len(snids)\n",
    "        num_batches = (num_snids + batch_size - 1) // batch_size\n",
    "\n",
    "        for batch_num in range(num_batches):\n",
    "            batch_start = batch_num * batch_size\n",
    "            batch_end = min((batch_num + 1) * batch_size, num_snids)\n",
    "            batch_snids = snids[batch_start:batch_end]\n",
    "\n",
    "            print(f\"Processing batch {batch_num + 1}/{num_batches} with {len(batch_snids)} SNIDs.\")\n",
    "\n",
    "            # Prepare arguments for processing\n",
    "            args_list = [(snid, hdf5_filename, object_type) for snid in batch_snids]\n",
    "\n",
    "            # Initialize lists to collect results and errors\n",
    "            entries = []\n",
    "            errors = []\n",
    "\n",
    "            # Use ProcessPoolExecutor for concurrent processing with timeouts\n",
    "            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "                # Submit all tasks to the executor\n",
    "                future_to_snid = {executor.submit(process_snid, args): args[0] for args in args_list}\n",
    "\n",
    "                for future in concurrent.futures.as_completed(future_to_snid):\n",
    "                    snid = future_to_snid[future]\n",
    "                    try:\n",
    "                        # Attempt to get the result within the specified timeout\n",
    "                        entry, error = future.result(timeout=task_timeout)\n",
    "                        if entry is not None:\n",
    "                            entries.append(entry)\n",
    "                        if error is not None:\n",
    "                            errors.append(error)\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        print(f\"TimeoutError: Processing SNID {snid} exceeded {task_timeout} seconds.\")\n",
    "                        errors.append((snid, f\"Processing timed out after {task_timeout} seconds\"))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Exception: Processing SNID {snid} failed with error: {e}\")\n",
    "                        errors.append((snid, str(e)))\n",
    "\n",
    "            # Save intermediate results and errors after each batch\n",
    "            save_intermediate_results(entries, intermediate_results_file)\n",
    "            save_intermediate_errors(errors, error_log_file)\n",
    "\n",
    "            # Clear lists to free memory\n",
    "            del entries, errors, args_list, future_to_snid\n",
    "            gc.collect()  # Explicitly call garbage collector\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred with file {hdf5_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e8f27f1-a857-480b-9688-465fbf0264c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the base directory for the HDF5 files\n",
    "    hdf5_base_dir = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files'  \n",
    "\n",
    "    # Object types and model names\n",
    "    object_info = [\n",
    "     #   'AGN',\n",
    "     #   'TDE',\n",
    "     #   'SLSN-I+host',\n",
    "     #   'SLSN-I_no_host',\n",
    "     #   'SNIa-SALT3',\n",
    "     #   'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19',\n",
    "     #   'SNIIb+HostXT_V19','SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', \n",
    "     #   'KN_B19', 'KN_K17',\n",
    "    ]\n",
    "\n",
    "    all_hdf5_filenames = []\n",
    "    object_types = []\n",
    "    for object_type in object_info:\n",
    "        hdf5_filename_template = f\"{object_type}_{{index}}.h5\"\n",
    "        for i in range(33, 35):  # Adjust range based on the number of files\n",
    "            hdf5_filename = os.path.join(hdf5_base_dir, hdf5_filename_template.format(index=str(i).zfill(4)))\n",
    "\n",
    "            if os.path.exists(hdf5_filename):\n",
    "                all_hdf5_filenames.append(hdf5_filename)\n",
    "                object_types.append(object_type)\n",
    "            else:\n",
    "                print(f\"HDF5 file not found: {hdf5_filename}\")\n",
    "\n",
    "    # Define output directory for processed results and error logs\n",
    "    output_dir = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    intermediate_results_file = os.path.join(output_dir, 'SNIa-SALT3_30.35_intermediate.csv')\n",
    "    error_log_file = os.path.join(output_dir, 'SNIa-SALT3_log30.35_intermediate.csv')\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(total=len(all_hdf5_filenames), desc=\"Processing files\")\n",
    "\n",
    "    for hdf5_filename, object_type in zip(all_hdf5_filenames, object_types):\n",
    "        process_file(\n",
    "            hdf5_filename, \n",
    "            object_type, \n",
    "            intermediate_results_file, \n",
    "            error_log_file,\n",
    "            max_workers=64,       # Adjust based on your system's capabilities\n",
    "            task_timeout=3     # Timeout in seconds for each SNID\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # End the timer and print the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total time taken: {elapsed_time / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bb574-fcd3-4c6b-b598-d78296af4df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "582b31ee-4502-4c5a-9a78-756949191f15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = \"/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files/\"\n",
    "\n",
    "# Define the pattern to match all intermediate CSV files\n",
    "file_pattern = os.path.join(directory, \"*_intermediate.csv\")\n",
    "\n",
    "# Retrieve all CSV files matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Check if any files were found\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No files found matching pattern: {file_pattern}\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} files to process.\")\n",
    "\n",
    "# Initialize an empty list to store individual DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"Processing file: {file} with {len(df)} rows.\")\n",
    "        \n",
    "        # Check if 'SNID' column exists (case-insensitive)\n",
    "        snid_column = None\n",
    "        for col in df.columns:\n",
    "            if col.upper() == 'SNID':\n",
    "                snid_column = col\n",
    "                break\n",
    "        \n",
    "        if snid_column is None:\n",
    "            print(f\"Warning: 'SNID' column not found in {file}. Skipping this file.\")\n",
    "            continue\n",
    "        \n",
    "        # Remove duplicate rows based on 'SNID'\n",
    "        initial_count = len(df)\n",
    "        df_cleaned = df.drop_duplicates(subset=snid_column)\n",
    "        final_count = len(df_cleaned)\n",
    "        duplicates_removed = initial_count - final_count\n",
    "        print(f\"Removed {duplicates_removed} duplicate rows based on '{snid_column}'.\")\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list\n",
    "        dataframes.append(df_cleaned)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Concatenate all cleaned DataFrames into one\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"Combined DataFrame has {len(combined_df)} rows before final deduplication.\")\n",
    "else:\n",
    "    raise ValueError(\"No DataFrames to concatenate. Exiting the script.\")\n",
    "\n",
    "# Ensure 'SNID' column is identified correctly\n",
    "snid_column = None\n",
    "for col in combined_df.columns:\n",
    "    if col.upper() == 'SNID':\n",
    "        snid_column = col\n",
    "        break\n",
    "\n",
    "if snid_column is None:\n",
    "    raise ValueError(\"'SNID' column not found in the combined DataFrame.\")\n",
    "\n",
    "# Remove duplicates across all files based on 'SNID'\n",
    "initial_combined_count = len(combined_df)\n",
    "final_combined_df = combined_df.drop_duplicates(subset=snid_column)\n",
    "final_combined_count = len(final_combined_df)\n",
    "total_duplicates_removed = initial_combined_count - final_combined_count\n",
    "print(f\"Total duplicates removed across all files based on '{snid_column}': {total_duplicates_removed}\")\n",
    "\n",
    "# Define the output file path\n",
    "output_file = os.path.join(directory, \"ELAsTiCC2_Test.csv\")\n",
    "\n",
    "# Save the final DataFrame to CSV\n",
    "try:\n",
    "    final_combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Successfully saved the consolidated data to {output_file} with {final_combined_count} unique rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the consolidated CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5019fb56-b122-4657-8752-96804d792abc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def analyze_csv(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes the given CSV file to determine the total number of objects and the number of unique SNIDs.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Display basic information about the DataFrame\n",
    "        print(\"### CSV File Analysis ###\")\n",
    "        print(f\"File Path: {file_path}\\n\")\n",
    "        print(f\"Total Rows (Objects): {len(df)}\")\n",
    "        \n",
    "        # Check if 'SNID' column exists\n",
    "        if 'SNID' in df.columns:\n",
    "            unique_snids = df['SNID'].nunique(dropna=True)\n",
    "            print(f\"Unique SNIDs: {unique_snids}\")\n",
    "        else:\n",
    "            print(\"Warning: 'SNID' column not found in the CSV file.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path to your CSV file\n",
    "    csv_file_path = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files/ELAsTiCC2_Test.csv'\n",
    "    \n",
    "    # Analyze the CSV file\n",
    "    analyze_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cd93b-ef7c-4aaf-a76f-61ecc112a09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3a2cabc-1edb-4f1c-bc61-54d350d66677",
   "metadata": {},
   "source": [
    "### Fraction of observations per band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a2d3b-fc3e-42ab-8213-b4b8a20cf158",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Base path and template for file names\n",
    "base_path = \"../../../karpov/ELASTICC2/\"\n",
    "filename_template = \"ELASTICC2_FINAL_{object_type}/ELASTICC2_FINAL_NONIaMODEL0-{index}_HEAD.FITS.gz\"\n",
    "\n",
    "# Object types and model names\n",
    "object_info = [\n",
    "    'TDE',\n",
    "    'AGN',\n",
    "    'SLSN-I+host',\n",
    "    'SLSN-I_no_host',\n",
    "    'SNIa-SALT3',\n",
    "    'SNIa-91bg',\n",
    "    'SNIax',\n",
    "    'SNIcBL+HostXT_V19',\n",
    "    'SNIb+HostXT_V19',\n",
    "    'SNIIn-MOSFIT',\n",
    "    'SNII-NMF',\n",
    "    'SNII+HostXT_V19',\n",
    "    'SNIIb+HostXT_V19',\n",
    "    'KN_B19',\n",
    "    'KN_K17',\n",
    "]\n",
    "\n",
    "# Generate file paths for each object type\n",
    "all_filenames = []\n",
    "object_types = []\n",
    "for object_type in object_info:\n",
    "    filenames = [os.path.join(base_path, filename_template.format(object_type=object_type, index=str(i).zfill(4))) for i in range(1, 2)]\n",
    "    all_filenames.extend(filenames)\n",
    "    object_types.extend([object_type] * len(filenames))\n",
    "\n",
    "# Dictionary for counting data, organized by object type and band\n",
    "count_data = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Main loop to process each file\n",
    "for i, (filename, object_type) in tqdm(enumerate(zip(all_filenames, object_types)), total=len(all_filenames)):\n",
    "    try:\n",
    "        table, head = read_elasticc_file(filename)\n",
    "        snids, shead_list, sub_list = get_snid_head_sub(table, head)\n",
    "\n",
    "        # Process each SNID\n",
    "        for snid, shead, sub in zip(snids, shead_list, sub_list):\n",
    "            try:\n",
    "                if not isinstance(sub, Table):\n",
    "                    sub = Table(sub)\n",
    "\n",
    "                # Compute SNR of each data point\n",
    "                sub['SNR'] = sub['FLUXCAL'] / sub['FLUXCALERR']\n",
    "\n",
    "                # DETECTION CHECK: pass only if there are points with SNR > 5 and flux > 50 in any bands\n",
    "                snr_flux_mask = (sub['SNR'] > 5) & (sub['FLUXCAL'] > 100)\n",
    "\n",
    "                # Filter the data\n",
    "                filtered_data = sub[snr_flux_mask]\n",
    "\n",
    "                # Count data for each band\n",
    "                for band in np.unique(filtered_data['BAND']):\n",
    "                    count_data[object_type][band] += len(filtered_data['BAND'][filtered_data['BAND'] == band])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing SNID {snid}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue  # Skip this file and continue with the next one\n",
    "\n",
    "\n",
    "# End the timer and print the elapsed time\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbbb5d-1535-4ba1-9a89-7478e8dcbad6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the color coding for the bands\n",
    "band_colors = {'u': 'blue', 'g': 'green', 'r': 'red', 'i': 'purple', 'z': 'brown', 'Y': 'orange'}\n",
    "\n",
    "# Define the combined classes\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "# Combine counts based on combined classes\n",
    "combined_count_data = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for obj_class, subtypes in combined_classes.items():\n",
    "    for subtype in subtypes:\n",
    "        if subtype in count_data:\n",
    "            for band in bands:\n",
    "                combined_count_data[obj_class][band] += count_data[subtype][band]\n",
    "\n",
    "# Calculate total counts for normalization\n",
    "total_counts = {cls: sum(band_counts.values()) for cls, band_counts in combined_count_data.items()}\n",
    "\n",
    "# Create subplots for each combined class and one for cumulative efficiency\n",
    "num_combined_classes = len(combined_classes)\n",
    "fig, axs = plt.subplots(num_combined_classes + 1, 1, figsize=(8, 3 * (num_combined_classes + 1)), sharex=True)\n",
    "\n",
    "# Plot normalized counts (efficiency) for each combined class\n",
    "for idx, (combined_class, band_counts) in enumerate(combined_count_data.items()):\n",
    "    if total_counts[combined_class] > 0:\n",
    "        efficiencies = [band_counts[band] / total_counts[combined_class] for band in bands]\n",
    "        colors = [band_colors[band] for band in bands]\n",
    "        axs[idx].bar(bands, efficiencies, color=colors, alpha=0.75)\n",
    "        axs[idx].set_ylabel('Fraction')\n",
    "        axs[idx].set_title(f'{combined_class}')\n",
    "        axs[idx].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot cumulative efficiency across all combined classes\n",
    "cumulative_counts = defaultdict(int)\n",
    "for band_counts in combined_count_data.values():\n",
    "    for band in bands:\n",
    "        cumulative_counts[band] += band_counts[band]\n",
    "\n",
    "total_cumulative_count = sum(cumulative_counts.values())\n",
    "if total_cumulative_count > 0:\n",
    "    cumulative_efficiencies = [cumulative_counts[band] / total_cumulative_count for band in bands]\n",
    "    axs[-1].bar(bands, cumulative_efficiencies, color=[band_colors[band] for band in bands], alpha=0.75)\n",
    "    axs[-1].set_ylabel('Fraction')\n",
    "    axs[-1].set_title('All classes combined')\n",
    "    axs[-1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Set the xlabel only for the last subplot\n",
    "axs[-1].set_xlabel('Band')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30af2db-076d-418c-8a2c-2eac1dfdee09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the color coding for the bands\n",
    "band_colors = {'u': 'blue', 'g': 'green', 'r': 'red', 'i': 'purple', 'z': 'brown', 'Y': 'orange'}\n",
    "\n",
    "# Define the combined classes\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'CLAGN': ['CLAGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "# Combine counts based on combined classes\n",
    "combined_count_data = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for obj_class, subtypes in combined_classes.items():\n",
    "    for subtype in subtypes:\n",
    "        if subtype in count_data:\n",
    "            for band in bands:\n",
    "                combined_count_data[obj_class][band] += count_data[subtype][band]\n",
    "\n",
    "# Calculate total counts for normalization\n",
    "total_counts = {cls: sum(band_counts.values()) for cls, band_counts in combined_count_data.items()}\n",
    "\n",
    "# Create subplots for each combined class and one for cumulative efficiency\n",
    "num_combined_classes = len(combined_classes)\n",
    "fig, axs = plt.subplots(num_combined_classes + 1, 1, figsize=(8, 3 * (num_combined_classes + 1)), sharex=True)\n",
    "\n",
    "# Plot normalized counts (efficiency) for each combined class\n",
    "for idx, (combined_class, band_counts) in enumerate(combined_count_data.items()):\n",
    "    if total_counts[combined_class] > 0:\n",
    "        efficiencies = [band_counts[band] / total_counts[combined_class] for band in bands]\n",
    "        colors = [band_colors[band] for band in bands]\n",
    "        axs[idx].bar(bands, efficiencies, color=colors, alpha=0.75)\n",
    "        axs[idx].set_ylabel('Fraction')\n",
    "        axs[idx].set_title(f'{combined_class}')\n",
    "        axs[idx].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot cumulative efficiency across all combined classes\n",
    "cumulative_counts = defaultdict(int)\n",
    "for band_counts in combined_count_data.values():\n",
    "    for band in bands:\n",
    "        cumulative_counts[band] += band_counts[band]\n",
    "\n",
    "total_cumulative_count = sum(cumulative_counts.values())\n",
    "if total_cumulative_count > 0:\n",
    "    cumulative_efficiencies = [cumulative_counts[band] / total_cumulative_count for band in bands]\n",
    "    axs[-1].bar(bands, cumulative_efficiencies, color=[band_colors[band] for band in bands], alpha=0.75)\n",
    "    axs[-1].set_ylabel('Fraction')\n",
    "    axs[-1].set_title('All classes combined')\n",
    "    axs[-1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Set the xlabel only for the last subplot\n",
    "axs[-1].set_xlabel('Band')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e967d42-1267-4bad-87fa-355897086817",
   "metadata": {},
   "source": [
    "## Quality cut plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcef5b5-b275-4566-b53e-38bbdeaf2edc",
   "metadata": {},
   "source": [
    "### counts/efficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6b03a-8753-40c5-b8ea-467e62d36dbc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(message)s')\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the base directory for the ELASTICC2\n",
    "base_path = \"../../../karpov/ELASTICC2/\"\n",
    "\n",
    "# Combined classes and their respective object types\n",
    "combined_classes = {\n",
    "    'TDE': ['TDE'],\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host']\n",
    "}\n",
    "\n",
    "# Flatten the combined_classes dictionary to map individual types to combined classes\n",
    "object_type_to_class = {}\n",
    "for combined_class, obj_types in combined_classes.items():\n",
    "    for obj_type in obj_types:\n",
    "        object_type_to_class[obj_type] = combined_class\n",
    "\n",
    "# Object types and model names\n",
    "object_info = list(object_type_to_class.keys())\n",
    "\n",
    "# Generate file paths for each object type\n",
    "all_filenames = []\n",
    "object_types = []\n",
    "for object_type in object_info:\n",
    "    filename_template = \"ELASTICC2_FINAL_{object_type}/ELASTICC2_FINAL_NONIaMODEL0-{index}_HEAD.FITS.gz\"\n",
    "    for i in range(2, 3):  # Adjust range based on the number of files per object type\n",
    "        file_path = os.path.join(base_path, filename_template.format(object_type=object_type, index=str(i).zfill(4)))\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            all_filenames.append(file_path)\n",
    "            object_types.append(object_type)\n",
    "        else:\n",
    "            logging.warning(f\"File not found: {file_path}\")\n",
    "\n",
    "# **Step 1: Count Total Objects by Combined Class**\n",
    "\n",
    "def count_total_objects_by_combined_class(all_filenames, object_types, object_type_to_class):\n",
    "    \"\"\"\n",
    "    Counts the total number of objects per combined class across all HDF5 files.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_filenames (list): List of HEAD FITS file paths.\n",
    "    - object_types (list): Corresponding list of object types for each file.\n",
    "    - object_type_to_class (dict): Mapping from individual object types to combined classes.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.Series: Total counts per combined class.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store counts\n",
    "    total_counts = {combined_class: 0 for combined_class in combined_classes.keys()}\n",
    "    \n",
    "    for file_path, obj_type in zip(all_filenames, object_types):\n",
    "        combined_class = object_type_to_class.get(obj_type, 'Unknown')\n",
    "        if combined_class == 'Unknown':\n",
    "            logging.warning(f\"Object type '{obj_type}' not found in combined_classes mapping.\")\n",
    "            continue\n",
    "        try:\n",
    "            # Open the FITS file\n",
    "            with fits.open(file_path) as hdulist:\n",
    "                # Assuming the data is in the first extension\n",
    "                data = hdulist[1].data\n",
    "                num_objects = len(data)\n",
    "                total_counts[combined_class] += num_objects\n",
    "                logging.info(f\"Counted {num_objects} objects for class '{combined_class}' in file '{file_path}'\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to pandas Series\n",
    "    total_counts_series = pd.Series(total_counts)\n",
    "    \n",
    "    return total_counts_series\n",
    "\n",
    "# **Step 2: Count Passed-Cut Objects by Combined Class from FITS Files**\n",
    "\n",
    "def count_passing_objects_from_fits(all_filenames, object_types, object_type_to_class, selection_criteria):\n",
    "    \"\"\"\n",
    "    Counts the number of objects that pass the selection criteria per combined class directly from FITS files.\n",
    "    \n",
    "    Selection Criteria:\n",
    "    - At least 3 data points in any band with SNR > 5 and FLUXCAL > 100.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_filenames (list): List of HEAD FITS file paths.\n",
    "    - object_types (list): Corresponding list of object types for each file.\n",
    "    - object_type_to_class (dict): Mapping from individual object types to combined classes.\n",
    "    - selection_criteria (dict): Dictionary containing selection thresholds.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.Series: Passed-cut counts per combined class.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store counts\n",
    "    passed_counts = {combined_class: 0 for combined_class in combined_classes.keys()}\n",
    "    \n",
    "    for file_path, obj_type in zip(all_filenames, object_types):\n",
    "        combined_class = object_type_to_class.get(obj_type, 'Unknown')\n",
    "        if combined_class == 'Unknown':\n",
    "            logging.warning(f\"Object type '{obj_type}' not found in combined_classes mapping.\")\n",
    "            continue\n",
    "        try:\n",
    "            # Open the FITS file\n",
    "            with fits.open(file_path) as hdulist:\n",
    "                # Assuming the data is in the first extension\n",
    "                data = hdulist[1].data\n",
    "                # Check if necessary columns exist\n",
    "                required_columns = ['SNID', 'BAND', 'FLUXCAL', 'FLUXCALERR']\n",
    "                for col in required_columns:\n",
    "                    if col not in data.columns.names:\n",
    "                        logging.warning(f\"Column '{col}' not found in {file_path}. Skipping this file.\")\n",
    "                        raise ValueError(f\"Missing column '{col}'\")\n",
    "                \n",
    "                # Convert to pandas DataFrame for easier manipulation\n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                # Compute SNR\n",
    "                df['SNR'] = df['FLUXCAL'] / df['FLUXCALERR']\n",
    "                \n",
    "                # Group by SNID\n",
    "                grouped = df.groupby('SNID')\n",
    "                \n",
    "                # Iterate through each SNID\n",
    "                for snid, group in grouped:\n",
    "                    # Check if any band has at least 3 points with SNR > 5 and FLUXCAL > 100\n",
    "                    bands = group['BAND'].unique()\n",
    "                    pass_snid = False\n",
    "                    for band in bands:\n",
    "                        band_group = group[group['BAND'] == band]\n",
    "                        condition = (band_group['SNR'] > selection_criteria['SNR_threshold']) & (band_group['FLUXCAL'] > selection_criteria['FLUXCAL_threshold'])\n",
    "                        if condition.sum() >= selection_criteria['min_points']:\n",
    "                            pass_snid = True\n",
    "                            break\n",
    "                    if pass_snid:\n",
    "                        passed_counts[combined_class] += 1\n",
    "                        \n",
    "                logging.info(f\"File '{file_path}' processed. Passed-count for class '{combined_class}': {passed_counts[combined_class]}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to pandas Series\n",
    "    passed_counts_series = pd.Series(passed_counts)\n",
    "    \n",
    "    return passed_counts_series\n",
    "\n",
    "# **Step 3: Count Passed-Cut Objects by Combined Class from CSV File**\n",
    "\n",
    "def count_passed_cut_objects_combined(csv_file_path, object_type_to_class, combined_classes):\n",
    "    \"\"\"\n",
    "    Counts the number of objects per combined class that passed the selection cuts from the processed CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_file_path (str): Path to the processed CSV file.\n",
    "    - object_type_to_class (dict): Mapping from individual object types to combined classes.\n",
    "    - combined_classes (dict): Combined classes with their respective object types.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.Series: Passed-cut counts per combined class.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the processed CSV file\n",
    "        processed_df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        if 'Object_Type' not in processed_df.columns:\n",
    "            logging.error(f\"'Object_Type' column not found in {csv_file_path}.\")\n",
    "            return pd.Series({cls:0 for cls in combined_classes.keys()})\n",
    "        \n",
    "        # Map individual object types to combined classes\n",
    "        processed_df['Combined_Class'] = processed_df['Object_Type'].map(object_type_to_class)\n",
    "        \n",
    "        # Handle unmapped object types\n",
    "        unmapped = processed_df['Combined_Class'].isna().sum()\n",
    "        if unmapped > 0:\n",
    "            logging.warning(f\"{unmapped} objects have unmapped 'Object_Type'. They will be excluded from counts.\")\n",
    "            processed_df = processed_df.dropna(subset=['Combined_Class'])\n",
    "        \n",
    "        # Count occurrences of each combined class\n",
    "        passed_counts = processed_df['Combined_Class'].value_counts().sort_index()\n",
    "        \n",
    "        # Ensure all combined classes are represented\n",
    "        passed_counts = passed_counts.reindex(combined_classes.keys(), fill_value=0)\n",
    "        \n",
    "        logging.info(\"Passed-Cut object counts by combined class from CSV:\")\n",
    "        logging.info(passed_counts)\n",
    "        \n",
    "        return passed_counts\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {csv_file_path}: {e}\")\n",
    "        return pd.Series({cls:0 for cls in combined_classes.keys()})\n",
    "\n",
    "# **Step 4: Save Counts to CSV Files**\n",
    "\n",
    "def save_counts_to_csv(total_counts, passed_counts_fits, passed_counts_csv, output_dir='counts'):\n",
    "    \"\"\"\n",
    "    Saves the total and passed-cut counts from FITS and CSV to CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - total_counts (pd.Series): Total counts per combined class.\n",
    "    - passed_counts_fits (pd.Series): Passed-cut counts from FITS per combined class.\n",
    "    - passed_counts_csv (pd.Series): Passed-cut counts from CSV per combined class.\n",
    "    - output_dir (str): Directory to save the count CSV files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save total counts\n",
    "    total_counts.to_csv(os.path.join(output_dir, 'total_counts_combined_classes.csv'), header=True)\n",
    "    logging.info(f\"Total counts saved to '{os.path.join(output_dir, 'total_counts_combined_classes.csv')}'\")\n",
    "    \n",
    "    # Save passed counts from FITS\n",
    "    passed_counts_fits.to_csv(os.path.join(output_dir, 'passed_counts_from_fits_combined_classes.csv'), header=True)\n",
    "    logging.info(f\"Passed-cut counts from FITS saved to '{os.path.join(output_dir, 'passed_counts_from_fits_combined_classes.csv')}'\")\n",
    "    \n",
    "    # Save passed counts from CSV\n",
    "    passed_counts_csv.to_csv(os.path.join(output_dir, 'passed_counts_from_csv_combined_classes.csv'), header=True)\n",
    "    logging.info(f\"Passed-cut counts from CSV saved to '{os.path.join(output_dir, 'passed_counts_from_csv_combined_classes.csv')}'\")\n",
    "\n",
    "# **Step 5: Create Overlaid Histogram Comparing FITS and CSV Counts**\n",
    "\n",
    "def plot_combined_class_histogram_comparison(total_counts, passed_counts_fits, passed_counts_csv, output_path='object_type_histogram_comparison.png'):\n",
    "    \"\"\"\n",
    "    Plots a histogram comparing total objects, passed-cut objects from FITS, and passed-cut objects from CSV by combined class.\n",
    "    \n",
    "    Parameters:\n",
    "    - total_counts (pd.Series): Counts of total objects by combined class.\n",
    "    - passed_counts_fits (pd.Series): Counts of passed-cut objects from FITS by combined class.\n",
    "    - passed_counts_csv (pd.Series): Counts of passed-cut objects from CSV by combined class.\n",
    "    - output_path (str): Path to save the histogram image.\n",
    "    \"\"\"\n",
    "    # Combine the counts into a single DataFrame\n",
    "    combined_df = pd.DataFrame({\n",
    "        'Total': total_counts,\n",
    "        'Passed_Cut_FITS': passed_counts_fits,\n",
    "        'Passed_Cut_CSV': passed_counts_csv\n",
    "    }).fillna(0)  # Fill NaN with 0 for combined classes that didn't pass the cut\n",
    "    \n",
    "    # Calculate percentages for FITS and CSV\n",
    "    combined_df['Passed_Cut_FITS_Percent'] = (combined_df['Passed_Cut_FITS'] / combined_df['Total']) * 100\n",
    "    combined_df['Passed_Cut_CSV_Percent'] = (combined_df['Passed_Cut_CSV'] / combined_df['Total']) * 100\n",
    "    \n",
    "    # Handle cases where Total is 0 to avoid division by zero\n",
    "    combined_df['Passed_Cut_FITS_Percent'] = combined_df.apply(\n",
    "        lambda row: row['Passed_Cut_FITS_Percent'] if row['Total'] > 0 else 0, axis=1\n",
    "    )\n",
    "    combined_df['Passed_Cut_CSV_Percent'] = combined_df.apply(\n",
    "        lambda row: row['Passed_Cut_CSV_Percent'] if row['Total'] > 0 else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Sort the DataFrame by Total counts for better visualization\n",
    "    combined_df = combined_df.sort_values('Total', ascending=False)\n",
    "    \n",
    "    # Plotting\n",
    "    x = np.arange(len(combined_df))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    # Plot Total counts\n",
    "    bars1 = ax.bar(x - width, combined_df['Total'], width, label='Total Objects', color='skyblue')\n",
    "    \n",
    "    # Plot Passed-Cut counts from FITS\n",
    "    bars2 = ax.bar(x, combined_df['Passed_Cut_FITS'], width, label='Passed Cut (FITS)', color='salmon')\n",
    "    \n",
    "    # Plot Passed-Cut counts from CSV\n",
    "    bars3 = ax.bar(x + width, combined_df['Passed_Cut_CSV'], width, label='Passed Cut (CSV)', color='lightgreen')\n",
    "    \n",
    "    # Add labels, title, and custom x-axis tick labels\n",
    "    ax.set_xlabel('Combined Object Class', fontsize=16)\n",
    "    ax.set_ylabel('Number of Objects', fontsize=16)\n",
    "    ax.set_title('Comparison of Total Objects and Passed-Cut Objects by Combined Class', fontsize=18)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(combined_df.index, rotation=45, ha='right', fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "    # Function to attach labels to bars\n",
    "    def autolabel(bars, counts, percentages=None):\n",
    "        \"\"\"Attach a text label above each bar displaying its height and percentage.\"\"\"\n",
    "        for bar, count, percent in zip(bars, counts, percentages if percentages is not None else [None]*len(bars)):\n",
    "            height = bar.get_height()\n",
    "            if percent is not None and not np.isnan(percent):\n",
    "                label = f'{int(count)} ({percent:.1f}%)'\n",
    "            else:\n",
    "                label = f'{int(count)}'\n",
    "            ax.annotate(\n",
    "                label,\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=10\n",
    "            )\n",
    "    \n",
    "    # Attach labels to Total bars\n",
    "    autolabel(bars1, combined_df['Total'])\n",
    "    \n",
    "    # Attach labels to Passed-Cut FITS bars with percentages\n",
    "    autolabel(bars2, combined_df['Passed_Cut_FITS'], combined_df['Passed_Cut_FITS_Percent'])\n",
    "    \n",
    "    # Attach labels to Passed-Cut CSV bars with percentages\n",
    "    autolabel(bars3, combined_df['Passed_Cut_CSV'], combined_df['Passed_Cut_CSV_Percent'])\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.show()\n",
    "\n",
    "# **Step 6: Main Execution Flow**\n",
    "\n",
    "def main():\n",
    "    # **Counting Total Objects by Combined Class**\n",
    "    logging.info(\"Counting total objects by combined class from FITS files...\")\n",
    "    total_object_counts = count_total_objects_by_combined_class(all_filenames, object_types, object_type_to_class)\n",
    "    \n",
    "    # **Counting Passed-Cut Objects by Combined Class from FITS Files**\n",
    "    logging.info(\"Counting passed-cut objects by combined class directly from FITS files...\")\n",
    "    selection_criteria = {\n",
    "        'min_points': 3,\n",
    "        'SNR_threshold': 5,\n",
    "        'FLUXCAL_threshold': 100\n",
    "    }\n",
    "    passed_cut_counts_fits = count_passing_objects_from_fits(all_filenames, object_types, object_type_to_class, selection_criteria)\n",
    "    \n",
    "    # **Counting Passed-Cut Objects by Combined Class from CSV File**\n",
    "    logging.info(\"Counting passed-cut objects by combined class from CSV file...\")\n",
    "    processed_csv_path = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files/ELAsTiCC2_Test.csv'  # Update if necessary\n",
    "    passed_cut_counts_csv = count_passed_cut_objects_combined(processed_csv_path, object_type_to_class, combined_classes)\n",
    "    \n",
    "    # **Save Counts to CSV Files**\n",
    "    logging.info(\"Saving counts to CSV files...\")\n",
    "    save_counts_to_csv(total_object_counts, passed_cut_counts_fits, passed_cut_counts_csv, output_dir='counts')\n",
    "    \n",
    "    # **Plot the Overlaid Histogram Comparing FITS and CSV Counts**\n",
    "    logging.info(\"Plotting the comparison histogram...\")\n",
    "    plot_combined_class_histogram_comparison(total_object_counts, passed_cut_counts_fits, passed_cut_counts_csv, output_path='object_type_histogram_comparison.png')\n",
    "    \n",
    "    # End the timer and print elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info(f\"Script completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ae37a-c353-4c52-a4b3-bbea1f1f1510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c53a2-7924-4f37-88b5-32e47f71aaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f98d64-0dae-4903-9d8b-cad2b6e2e173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93472dfe-1eb8-479f-91b3-013d24c5de81",
   "metadata": {},
   "source": [
    "### population stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9bf5d8-1dc0-4676-a923-96f2a46097f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined classes and their respective object types\n",
    "combined_classes = {\n",
    "    'TDE': ['TDE'],\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'CLAGN': ['CLAGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host']\n",
    "  \n",
    "}\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "\n",
    "# Calculate counts for each combined class\n",
    "combined_class_counts = df0['Combined_Class'].value_counts()\n",
    "\n",
    "# Define custom colors\n",
    "custom_colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#c2c2f0','#ffb3e6']\n",
    "\n",
    "# Explode values to highlight TDE without hiding KN\n",
    "explode_values = [0.1 if combined_class == 'TDE' else 0 for combined_class in combined_class_counts.index]\n",
    "\n",
    "# Custom autopct function to show percentages and counts\n",
    "def autopct_with_count(pct, allvalues):\n",
    "    absolute = int(np.round(pct/100.*np.sum(allvalues)))\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "\n",
    "# Plot pie chart for combined classes with custom colors and shadow\n",
    "plt.figure(figsize=(10, 6))\n",
    "combined_class_counts.plot.pie(\n",
    "    autopct=lambda pct: autopct_with_count(pct, combined_class_counts.values),\n",
    "    colors=custom_colors,\n",
    "    startangle=140,\n",
    "    shadow=True,\n",
    "    explode=explode_values\n",
    ")\n",
    "plt.title('Distribution of Combined Classes', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "\n",
    "# Map combined classes to TDE vs Non-TDEs\n",
    "df0['TDE_vs_NonTDE'] = df0['Combined_Class'].apply(lambda x: 'TDE' if x == 'TDE' else 'Non-TDE')\n",
    "\n",
    "# Calculate counts for TDE vs Non-TDEs\n",
    "tde_vs_nontde_counts = df0['TDE_vs_NonTDE'].value_counts()\n",
    "\n",
    "# Plot pie chart for TDE vs Non-TDEs with custom colors and shadow\n",
    "plt.figure(figsize=(8, 6))\n",
    "tde_vs_nontde_counts.plot.pie(\n",
    "    autopct=lambda pct: autopct_with_count(pct, tde_vs_nontde_counts.values),\n",
    "    colors=['#66b3ff', '#ff9999'],\n",
    "    startangle=140,\n",
    "    shadow=True,\n",
    "    explode=[0.1 if combined_class == 'TDE' else 0 for combined_class in tde_vs_nontde_counts.index]\n",
    ")\n",
    "plt.title('Distribution of TDE vs Non-TDEs', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c4c5b-89d2-41f4-950e-e9ab13458bcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# true peak vs gp peak\n",
    "# Load the data\n",
    "file_path = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/test19-Copy1.1_snr5_intermediate.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the difference between 'TruePeakMJD' and 'peak_time_MJD'\n",
    "data['MJD_Difference'] = data['TruePeakMJD'] - data['peak_time_MJD']\n",
    "\n",
    "# Filter out rows with NaN values in the columns\n",
    "data = data.dropna(subset=['TruePeakMJD', 'peak_time_MJD'])\n",
    "\n",
    "# Plot 1: Histogram of the differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data['MJD_Difference'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of MJD Differences (TruePeakMJD - peak_time_MJD)')\n",
    "plt.xlabel('MJD Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: KDE plot (Density estimation)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data['MJD_Difference'], fill=True, color='lightcoral')\n",
    "plt.title('KDE of MJD Differences (TruePeakMJD - peak_time_MJD)')\n",
    "plt.xlabel('MJD Difference')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Scatter plot of TruePeakMJD vs. peak_time_MJD\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['TruePeakMJD'], data['peak_time_MJD'], alpha=0.5, color='purple')\n",
    "plt.title('Scatter Plot: TruePeakMJD vs peak_time_MJD')\n",
    "plt.xlabel('TruePeakMJD')\n",
    "plt.ylabel('peak_time_MJD')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Jointplot (scatter plot + KDE for comparison)\n",
    "sns.jointplot(x='TruePeakMJD', y='peak_time_MJD', data=data, kind=\"scatter\", color=\"blue\", marginal_kws=dict(bins=50, fill=True))\n",
    "plt.show()\n",
    "\n",
    "# Plot 5: Box plot of MJD differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data['MJD_Difference'], color='green')\n",
    "plt.title('Box Plot of MJD Differences')\n",
    "plt.xlabel('MJD Difference')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display some basic statistics\n",
    "print(data['MJD_Difference'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a1919-3e45-4733-9e2a-fd8d4dd0a497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560df0d-8d86-4ab7-9f39-b2150bd04a42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/test20.3_snr5_intermediate.csv'\n",
    "\n",
    "\n",
    "# Read the CSV file into an Astropy Table\n",
    "table1 = Table.read(file_path, format='csv')\n",
    "\n",
    "#table.colnames\n",
    "table1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc896e4-b6a1-4709-a999-36f220101f7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the file path\n",
    "file_path = '/home/bhardwaj/nonIa-TRUTH_OBJECTS.csv'\n",
    "\n",
    "# Read the CSV file into an Astropy Table\n",
    "truth_table = Table.read(file_path, format='csv')\n",
    "\n",
    "#table.colnames\n",
    "truth_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3519eb-917c-4d13-94ca-02cede00877e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from astropy.table import Table, join\n",
    "\n",
    "\n",
    "# Ensure that the 'SNID' column exists in both tables\n",
    "if 'SNID' not in truth_table.colnames or 'SNID' not in table1.colnames:\n",
    "    raise ValueError(\"'SNID' column not found in one or both tables.\")\n",
    "\n",
    "# Perform a join operation on the 'SNID' column to find matches\n",
    "matched_table = join(truth_table, table1, keys='SNID', join_type='inner')\n",
    "\n",
    "# Display the results of the matching\n",
    "print(\"Matched Rows:\")\n",
    "print(matched_table)\n",
    "\n",
    "# Ensure that the 'SNID' column exists in both tables\n",
    "if 'SNID' not in truth_table.colnames or 'SNID' not in table1.colnames:\n",
    "    raise ValueError(\"'SNID' column not found in one or both tables.\")\n",
    "\n",
    "# Perform a join operation on the 'SNID' column to find matches\n",
    "matched_table = join(truth_table, table1, keys='SNID', join_type='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3a07f-743b-4d2a-befa-24fdd9bd893b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure that the required columns exist in the matched table\n",
    "if 'PeakMag' not in matched_table.colnames or 'PEAKMAG_g' not in matched_table.colnames:\n",
    "    raise ValueError(\"'PeakMag' or 'PEAKMAG_g' column not found in the matched table.\")\n",
    "\n",
    "# Calculate the difference in PeakMag\n",
    "matched_table['MagDifference'] = matched_table['PeakMag'] - matched_table['PEAKMAG_g']\n",
    "\n",
    "# Plot the difference\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(matched_table['MagDifference'], bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Difference in PeakMag (PeakMag - PEAKMAG_g)')\n",
    "plt.ylabel('Number of SNID')\n",
    "plt.title('Difference in PeakMag for Matched SNID')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0082309-2095-4b35-8bf8-d569fff22b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a9f77-f2b8-4c5b-abb8-34750f6d56f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all variable (column) names in the DataFrame\n",
    "variable_names = df0.columns.tolist()\n",
    "\n",
    "# Display the list of variable names vertically\n",
    "for name in variable_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613ff52-8ec3-4950-b7d2-7644391058f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2433445-b790-4037-ba81-aed6e9b3901f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define which Object_Types are considered TDE\n",
    "tde_types = ['TDE']  # Replace 'TDE' with the actual values that represent TDE in your data\n",
    "\n",
    "# Create a new column in the DataFrame to categorize as 'TDE' or 'non-TDE'\n",
    "df0['Category'] = df0['Object_Type'].apply(lambda x: 'TDE' if x in tde_types else 'non-TDE')\n",
    "\n",
    "# Dictionary mapping old column names to new column names\n",
    "rename_mapping = {\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time',\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)'\n",
    "}\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df0.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# List of variables you want to include in the plot\n",
    "selected_variables = [\n",
    "    'LengthScale_Wavelength', \n",
    "    'LengthScale_Time', \n",
    "    'Amplitude',\n",
    "    'Rise time', \n",
    "    'Fade time',\n",
    "    'Mean Color Pre Peak (g-r)',\n",
    "    'Mean Color Pre Peak (r-i)',\n",
    "    'Mean Color Post Peak (r-i)',\n",
    "    'Mean Color Post Peak (g-r)',\n",
    "    'Slope Pre Peak (g-r)',\n",
    "    'Slope Pre Peak (r-i)',\n",
    "    'Slope Post Peak (r-i)',\n",
    "    'Slope Post Peak (g-r)'\n",
    "]\n",
    "\n",
    "# Filter out variables that do not exist in the DataFrame (in case some were renamed incorrectly)\n",
    "selected_variables = [var for var in selected_variables if var in df0.columns]\n",
    "\n",
    "non_missing_percentage_by_category = {}\n",
    "\n",
    "for category in ['TDE', 'non-TDE']:\n",
    "    df_subset = df0[df0['Category'] == category]\n",
    "    \n",
    "    # Calculate the non-missing percentage for selected variables\n",
    "    non_missing_percentage = df_subset[selected_variables].notnull().mean() * 100\n",
    "    \n",
    "    # Calculate the percentage of rows with all selected variables present\n",
    "    percentage_all_present = df_subset[selected_variables].dropna().shape[0] / df_subset.shape[0] * 100\n",
    "    \n",
    "    # Add this percentage as a new entry to the non_missing_percentage series\n",
    "    non_missing_percentage['All features extracted'] = percentage_all_present\n",
    "    \n",
    "    non_missing_percentage_by_category[category] = non_missing_percentage\n",
    "\n",
    "# Plotting\n",
    "# Create subplots with adjusted figure size and layout\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), sharex=True, constrained_layout=True)\n",
    "\n",
    "for ax, (category, non_missing_percentage) in zip(axes, non_missing_percentage_by_category.items()):\n",
    "    # Sort the percentages for better visualization\n",
    "    non_missing_percentage_sorted = non_missing_percentage.sort_values()\n",
    "    \n",
    "    # Create a horizontal bar plot\n",
    "    non_missing_percentage_sorted.plot(kind='barh', color='skyblue', ax=ax)\n",
    "    \n",
    "    # Set the title and labels\n",
    "    ax.set_title(f'Proportion of Extracted Features: {category}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Percentage', fontsize=12)\n",
    "    \n",
    "    # Customize tick parameters for better readability\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    \n",
    "    # Add grid lines for the x-axis\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Optionally, annotate the bars with percentage values\n",
    "    for i, (value, name) in enumerate(zip(non_missing_percentage_sorted, non_missing_percentage_sorted.index)):\n",
    "        ax.text(value + 1, i, f'{value:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# Save the figure once after all subplots are created\n",
    "save_path = '/home/bhardwaj/notebooksLSST/tdes-fzu/home/bhardwaj/notebooksLSST/tdes-fzu/notebooksLSST/ELAsTiCC2_processed/results-images/successrate' \n",
    "plt.savefig(save_path, dpi=100)\n",
    "print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df85b58-a33e-4ab7-b2cf-dc39e91483f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998229fd-4a09-4a14-bdcd-47c3713bdec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350978a4-453f-4859-95b3-ca358a56e680",
   "metadata": {},
   "source": [
    "## Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c48b3-2323-413d-a9c7-8df685a0b0f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Combined classes and their respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'yellow',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Initialize lists to store rise times, decay times, and amplitudes for different combined classes\n",
    "rise_times = {cls: [] for cls in combined_classes}\n",
    "decay_times = {cls: [] for cls in combined_classes}\n",
    "amplitudes = {cls: [] for cls in combined_classes}\n",
    "\n",
    "# Collect rise times, decay times, and amplitudes from the processed data\n",
    "for _, entry in df0.iterrows():\n",
    "    for combined_class, original_classes in combined_classes.items():\n",
    "        if entry['Object_Type'] in original_classes:\n",
    "            rise_times[combined_class].append(entry['LengthScale_Wavelength'])\n",
    "            decay_times[combined_class].append(entry['LengthScale_Time'])\n",
    "            amplitudes[combined_class].append(entry['Amplitude'])\n",
    "            break\n",
    "\n",
    "# Create 3D scatter plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add each combined class to the 3D plot\n",
    "for combined_class, color in colors.items():\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=rise_times[combined_class],\n",
    "        y=decay_times[combined_class],\n",
    "        z=amplitudes[combined_class],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=color,  # Set color for each class\n",
    "            opacity=0.5\n",
    "        ),\n",
    "        name=combined_class\n",
    "    ))\n",
    "\n",
    "# Set the layout for the 3D plot\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"LengthScale_Wavelength\",\n",
    "        yaxis_title=\"LengthScale_Time\",\n",
    "        zaxis_title=\"Amplitude\"\n",
    "    ),\n",
    "    title=\"3D Scatter Plot of LengthScale_Wavelength, LengthScale_Time, and Amplitude\",\n",
    "    margin=dict(l=0, r=0, b=0, t=50)\n",
    ")\n",
    "\n",
    "# Display the interactive 3D plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b953c17-7fa7-4a27-9226-be23be356330",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combined classes and their respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3','SNIa-91bg','SNIax', 'SNIcBL+HostXT_V19','SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT','SNII-NMF','SNII+HostXT_V19','SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "  #  'KN': ['KN_K17','KN_B19'],\n",
    "  #  'SLSN': ['SLSN-I+host','SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'orange',\n",
    "   # 'KN': 'purple',\n",
    "   # 'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "# Initialize lists to store rise and decay times for different combined classes\n",
    "rise_times = {cls: [] for cls in combined_classes}\n",
    "decay_times = {cls: [] for cls in combined_classes}\n",
    "\n",
    "# Collect rise and decay times from the processed data\n",
    "for _, entry in df0.iterrows():\n",
    "    for combined_class, original_classes in combined_classes.items():\n",
    "        if entry['Object_Type'] in original_classes:\n",
    "            rise_times[combined_class].append(entry['LengthScale_Wavelength'])\n",
    "            decay_times[combined_class].append(entry['LengthScale_Time'])\n",
    "            break\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot each combined class\n",
    "for combined_class, color in colors.items():\n",
    "    plt.scatter(rise_times[combined_class], decay_times[combined_class], color=color, label=combined_class, marker='.', alpha = 0.4)\n",
    "\n",
    "# Add labels, title, legend\n",
    "plt.xlabel(\"LengthScale_Wavelength\")\n",
    "plt.ylabel(\"LengthScale_Time\")\n",
    "plt.xlim(0, 50) \n",
    "plt.ylim(0, 20)    \n",
    "#plt.title(\"Measured by peakmag+1 either side of peak, using GP on g band\")\n",
    "plt.legend()\n",
    "plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/LWvsLT.png', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6aeb0-5c1e-4d94-abcf-b47dbcfa36ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d495cfa-9a71-4b7d-beda-f5fb598f7261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec34500-8d15-45eb-9489-0664cc54ef87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d651fd6a-2b7b-43b9-8848-ea29598bbad1",
   "metadata": {},
   "source": [
    "### Rate of color change v/s mean color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd80e23-7e98-4b74-9da6-34c171b8ec16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined classes and their respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'CLAGN': ['CLAGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'CLAGN': 'yellow',\n",
    "    'KN': 'purple',\n",
    "    'SLSN': 'black',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "\n",
    "# Plotting\n",
    "fig, ((ax8, ax9), (ax10, ax11)) = plt.subplots(2, 2, figsize=(17, 6))\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df0['Combined_Class'] == combined_class\n",
    "    \n",
    "    # Apply a mask to filter out NaN values for each plot\n",
    "    for ax, mean_col, slope_col, err_col in [\n",
    "        (ax8, 'Mean_Color_Pre_Peak', 'Slope_Pre_Peak', 'Slope_Err_Pre_Peak'),\n",
    "        (ax9, 'Mean_Color_Post_Peak', 'Slope_Post_Peak', 'Slope_Err_Post_Peak'),\n",
    "        (ax10, 'Mean_Color_Pre_Peak_GR', 'Slope_Pre_Peak_GR', 'Slope_Err_Pre_Peak_GR'),\n",
    "        (ax11, 'Mean_Color_Post_Peak_GR', 'Slope_Post_Peak_GR', 'Slope_Err_Post_Peak_GR')]:\n",
    "\n",
    "        valid_mask = mask & df0[mean_col].notna() & df0[slope_col].notna() & df0[err_col].notna()\n",
    "        \n",
    "        # Plot if there are any valid data points\n",
    "        if valid_mask.sum() > 0:\n",
    "            ax.errorbar(df0.loc[valid_mask, mean_col], df0.loc[valid_mask, slope_col], \n",
    "                        yerr=df0.loc[valid_mask, err_col].fillna(0), fmt='.', label=combined_class, alpha=0.4, color=color)\n",
    "        else:\n",
    "            print(f\"No valid data points for {combined_class} in {ax.get_title()}\")\n",
    "\n",
    "ax8.set_xlabel(\"Mean Pre-peak g-r Color\")\n",
    "ax8.set_ylabel(\"Rate of Color Change (1/day)\")\n",
    "ax9.set_xlabel(\"Mean Post-peak g-r Color\")\n",
    "ax9.set_ylabel(\"Rate of Color Change (1/day)\")\n",
    "ax10.set_xlabel(\"Mean Pre-peak r-i Color\")\n",
    "ax10.set_ylabel(\"Rate of Color Change (1/day)\")\n",
    "ax11.set_xlabel(\"Mean Post-peak r-i Color\")\n",
    "ax11.set_ylabel(\"Rate of Color Change (1/day)\")\n",
    "\n",
    "# Setting axis limits and adding legends\n",
    "for ax in [ax8, ax9, ax10, ax11]:\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(-3, 2.5)\n",
    "    ax.set_ylim(-0.07, 0.07)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48a043-fdb9-40ef-98cb-a34197bb1567",
   "metadata": {},
   "source": [
    "### Color-color diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfe901-a9fc-4d6f-bbbe-ee5ed030d5c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combined classes and their respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3','SNIa-91bg','SNIax', 'SNIcBL+HostXT_V19','SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT','SNII-NMF','SNII+HostXT_V19','SNIIb+HostXT_V19'],\n",
    "    'AGN': ['CLAGN'],\n",
    "  #  'KN': ['KN_K17','KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host','SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'yellow',\n",
    " #   'KN': 'purple',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "\n",
    "# Function to plot with marginal distributions\n",
    "def plot_with_marginals(joint, x_data, y_data, x_error, y_error, label, color):\n",
    "    joint.ax_joint.errorbar(\n",
    "        x_data, y_data, \n",
    "        xerr=x_error, yerr=y_error, \n",
    "        fmt='.', alpha=0.3, color=color, label=label\n",
    "    )\n",
    "    sns.histplot(\n",
    "        x=x_data, bins=100, kde=True, \n",
    "        color=color, ax=joint.ax_marg_x, alpha=0.5\n",
    "    )\n",
    "    sns.histplot(\n",
    "        y=y_data, bins=100, kde=True, \n",
    "        color=color, ax=joint.ax_marg_y, alpha=0.5, orientation='horizontal'\n",
    "    )\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Pre-peak Color-Color Diagram\n",
    "pre_joint = sns.JointGrid(height=6, ratio=4)\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df0['Combined_Class'] == combined_class\n",
    "    df_class = df0.loc[mask]\n",
    "    \n",
    "    # Check if there are enough data points to sample\n",
    "    if len(df_class) == 0:\n",
    "        continue  # Skip if no data points for this class\n",
    "    \n",
    "    # Sample 10% of the data for the current class\n",
    "    df_sampled = df_class.sample(frac=0.3, random_state=RANDOM_SEED)\n",
    "    \n",
    "    x_data = df_sampled['Mean Color Pre Peak (g-r)']\n",
    "    y_data = df_sampled['Mean Color Pre Peak (r-i)']\n",
    "    x_error = df_sampled['Pre_Peak_Color_err_gr']\n",
    "    y_error = df_sampled['Pre_Peak_Color_err_ri']\n",
    "    \n",
    "    plot_with_marginals(pre_joint, x_data, y_data, x_error, y_error, combined_class, color)\n",
    "\n",
    "pre_joint.set_axis_labels(\"Mean Pre-peak g-r Color\", \"Mean Pre-peak r-i Color\")\n",
    "pre_joint.ax_joint.legend(title='Combined Class')\n",
    "pre_joint.ax_joint.set_xlim(-1.5, 3)\n",
    "pre_joint.ax_joint.set_ylim(-1.5, 1.5)\n",
    "pre_joint.fig.suptitle('Pre-peak Color-Color Diagram', y=1.02)\n",
    "\n",
    "# Post-peak Color-Color Diagram\n",
    "post_joint = sns.JointGrid(height=6, ratio=4)\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df0['Combined_Class'] == combined_class\n",
    "    df_class = df0.loc[mask]\n",
    "    \n",
    "    # Check if there are enough data points to sample\n",
    "    if len(df_class) == 0:\n",
    "        continue  # Skip if no data points for this class\n",
    "    \n",
    "    # Sample 20% of the data for the current class\n",
    "    df_sampled = df_class.sample(frac=0.1, random_state=RANDOM_SEED)\n",
    "    \n",
    "    x_data = df_sampled['Mean Color Post Peak (g-r)']\n",
    "    y_data = df_sampled['Mean Color Post Peak (r-i)']\n",
    "    x_error = df_sampled['Post_Peak_Color_err_gr']\n",
    "    y_error = df_sampled['Post_Peak_Color_err_ri']\n",
    "    \n",
    "    plot_with_marginals(post_joint, x_data, y_data, x_error, y_error, combined_class, color)\n",
    "\n",
    "post_joint.set_axis_labels(\"Mean post-peak g-r\", \"Mean post-peak r-i\")\n",
    "post_joint.ax_joint.legend(loc='lower right')  # Move legend to bottom right\n",
    "post_joint.ax_joint.set_xlim(-0.8, 2)\n",
    "post_joint.ax_joint.set_ylim(-0.9, 0.9)\n",
    "#post_joint.fig.suptitle('Post-peak Color-Color Diagram', y=1.02)\n",
    "\n",
    "# Adjust layout to prevent clipping\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust rect to make space for suptitle\n",
    "\n",
    "# Save the Post-peak plot separately with proper settings\n",
    "save_path_post = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/results-images/colordiag_post_peak.png'\n",
    "post_joint.fig.savefig(save_path_post, dpi=150, bbox_inches='tight')\n",
    "print(f\"Post-peak Color-Color Diagram saved to {save_path_post}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda26f54-f3aa-4d1a-91af-1a51587d587f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Combined classes and their respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['CLAGN'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'orange',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Function to plot contours\n",
    "def plot_contours(joint, x_data, y_data, label, color):\n",
    "    # Plot KDE contours with seaborn\n",
    "    sns.kdeplot(\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        ax=joint.ax_joint,\n",
    "        levels=5,  # Number of contour levels\n",
    "        color=color,\n",
    "        linewidths=1,\n",
    "        alpha=0.6,  # Set transparency for contour lines\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "# Pre-peak Color-Color Diagram\n",
    "pre_joint = sns.JointGrid(height=6, ratio=4)\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df0['Combined_Class'] == combined_class\n",
    "    df_class = df0.loc[mask]\n",
    "\n",
    "    # Check if there are enough data points to sample\n",
    "    if len(df_class) == 0:\n",
    "        continue  # Skip if no data points for this class\n",
    "\n",
    "    # Sample 30% of the data for the current class\n",
    "    df_sampled = df_class.sample(frac=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "    x_data = df_sampled['Mean Color Pre Peak (g-r)']\n",
    "    y_data = df_sampled['Mean Color Pre Peak (r-i)']\n",
    "\n",
    "    plot_contours(pre_joint, x_data, y_data, combined_class, color)\n",
    "\n",
    "pre_joint.set_axis_labels(\"Mean Pre-peak g-r Color\", \"Mean Pre-peak r-i Color\")\n",
    "pre_joint.ax_joint.legend(title='Combined Class')\n",
    "pre_joint.ax_joint.set_xlim(-1.5, 3)\n",
    "pre_joint.ax_joint.set_ylim(-1.5, 1.5)\n",
    "pre_joint.fig.suptitle('Pre-peak Color-Color Diagram', y=1.02)\n",
    "\n",
    "# Post-peak Color-Color Diagram\n",
    "post_joint = sns.JointGrid(height=6, ratio=4)\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df0['Combined_Class'] == combined_class\n",
    "    df_class = df0.loc[mask]\n",
    "\n",
    "    # Check if there are enough data points to sample\n",
    "    if len(df_class) == 0:\n",
    "        continue  # Skip if no data points for this class\n",
    "\n",
    "    # Sample 10% of the data for the current class\n",
    "    df_sampled = df_class.sample(frac=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "    x_data = df_sampled['Mean Color Post Peak (g-r)']\n",
    "    y_data = df_sampled['Mean Color Post Peak (r-i)']\n",
    "\n",
    "    plot_contours(post_joint, x_data, y_data, combined_class, color)\n",
    "\n",
    "post_joint.set_axis_labels(\"Mean post-peak g-r\", \"Mean post-peak r-i\")\n",
    "post_joint.ax_joint.legend(loc='lower right')  # Move legend to bottom right\n",
    "post_joint.ax_joint.set_xlim(-0.8, 2)\n",
    "post_joint.ax_joint.set_ylim(-0.9, 0.9)\n",
    "\n",
    "# Adjust layout to prevent clipping\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust rect to make space for suptitle\n",
    "\n",
    "# Save the Post-peak plot separately with proper settings\n",
    "save_path_post = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/results-images/colordiag_post_peak.png'\n",
    "post_joint.fig.savefig(save_path_post, dpi=150, bbox_inches='tight')\n",
    "print(f\"Post-peak Color-Color Diagram saved to {save_path_post}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8236ed-4b20-4e51-9e1c-09cc194c225b",
   "metadata": {},
   "source": [
    "### rise vs decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a30c4-2019-4d3d-971d-905252c2083b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Assuming df0 is already loaded\n",
    "# Combined classes and their respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'yellow',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Initialize lists to store rise and decay times for different combined classes\n",
    "rise_times = {cls: [] for cls in combined_classes}\n",
    "decay_times = {cls: [] for cls in combined_classes}\n",
    "\n",
    "# Collect rise and decay times from the processed data\n",
    "for _, entry in df0.iterrows():\n",
    "    for combined_class, original_classes in combined_classes.items():\n",
    "        if entry['Object_Type'] in original_classes:\n",
    "            rise_times[combined_class].append(entry['Rise time'])\n",
    "            decay_times[combined_class].append(entry['Fade time'])\n",
    "            break\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each combined class with contours using Seaborn\n",
    "for combined_class, color in colors.items():\n",
    "    x = np.array(rise_times[combined_class])\n",
    "    y = np.array(decay_times[combined_class])\n",
    "\n",
    "    # KDE plot to create contours\n",
    "    sns.kdeplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        levels=5,  # Adjust the number of levels to match the contour style\n",
    "        color=color,\n",
    "        fill=False,  # Only draw contour lines\n",
    "        linewidths=1,\n",
    "        alpha=0.6  # Transparency for contours\n",
    "    )\n",
    "\n",
    "    # Calculate density using gaussian_kde\n",
    "    xy = np.vstack([x, y])\n",
    "    kde = gaussian_kde(xy)\n",
    "    z = kde(xy)\n",
    "\n",
    "    # Define a threshold for the density - only show points outside of dense regions\n",
    "    threshold = np.percentile(z, 90)  # Only show the lowest 10% of the points\n",
    "    scatter_x = x[z < threshold]\n",
    "    scatter_y = y[z < threshold]\n",
    "\n",
    "    # Scatter plot for points outside the contour regions\n",
    "    plt.scatter(\n",
    "        scatter_x, \n",
    "        scatter_y, \n",
    "        color=color, \n",
    "        label=combined_class, \n",
    "        marker='.', \n",
    "        alpha=0.4, \n",
    "        s=1  # Adjust size of points for better visualization\n",
    "    )\n",
    "\n",
    "# Add labels, title, legend\n",
    "plt.xlabel(\"Rise (days)\")\n",
    "plt.ylabel(\"Fade (days)\")\n",
    "plt.ylim(-5, 200)\n",
    "plt.xlim(-5, 200)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb4707-2512-44a7-a063-60a9310b05da",
   "metadata": {},
   "source": [
    "### rise/fade vs color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4792fdc-b259-4207-9e57-6746bf03c00e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Assuming df0 is already loaded with necessary columns including errors\n",
    "# Sample only 50% of each object type\n",
    "df_sampled = df0.groupby('Combined_Class').apply(lambda x: x.sample(frac=0.5)).reset_index(drop=True)\n",
    "\n",
    "# Define combined classes and respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'yellow',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Initialize color map for the different combined classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df_sampled['Combined_Class'] == combined_class\n",
    "    \n",
    "    # Plot Rise Time vs. Pre-Peak Mean Color with error bars\n",
    "    ax1.errorbar(df_sampled.loc[mask, 'Mean Color Post Peak (r-i)'], \n",
    "                 df_sampled.loc[mask, 'Slope Post Peak (r-i)'], \n",
    "                 yerr=df_sampled.loc[mask, 'Slope_Err_Post_Peak_gr'],  # Error for y\n",
    "                 xerr=df_sampled.loc[mask, 'Post_Peak_Color_err_gr'],  # Error for x\n",
    "                 fmt='.', label=combined_class, alpha=0.4, color=color)\n",
    "    \n",
    "    ax1.set_ylabel(\"Rate of color change (r-i) per day\")\n",
    "    ax1.set_xlabel(\"Mean Color Post Peak r-i (mag)\")\n",
    "    ax1.set_xlim(-0.7, 1)\n",
    "    ax1.set_ylim(-0.02, 0.05)\n",
    "\n",
    "    # Plot Fade Time vs. Post-Peak Mean Color with error bars\n",
    "    ax2.errorbar(df_sampled.loc[mask, 'Mean Color Post Peak (g-r)'], \n",
    "                 df_sampled.loc[mask, 'Slope Post Peak (g-r)'], \n",
    "                 yerr=df_sampled.loc[mask, 'Slope_Err_Post_Peak_gr'],  # Error for y\n",
    "                 xerr=df_sampled.loc[mask, 'Post_Peak_Color_err_gr'],  # Error for x\n",
    "                 fmt='.', label=combined_class, alpha=0.3, color=color)\n",
    "    \n",
    "    ax2.set_ylabel(\"Rate of color change (g-r) per day\")\n",
    "    ax2.set_xlabel(\"Mean Color Post Peak g-r (mag)\")\n",
    "    ax2.set_xlim(-0.7, 1)\n",
    "    ax2.set_ylim(-0.02, 0.05)\n",
    "\n",
    "# Create custom legend handles with higher alpha\n",
    "legend_handles = [Line2D([0], [0], marker='.', color='w', label=class_type,\n",
    "                          markerfacecolor=color, markersize=10, alpha=1) for class_type, color in colors.items()]\n",
    "\n",
    "# Add legend to both axes\n",
    "ax1.legend(handles=legend_handles, title=\"Combined Class\", loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "ax2.legend(handles=legend_handles, title=\"Combined Class\", loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/colorvslope.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed64c3-eb2d-4c7e-ab76-d0c7376d5e8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Assuming df0 is already loaded with necessary columns including errors\n",
    "# Sample only 50% of each object type\n",
    "\n",
    "# Define combined classes and respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    " #   'AGN': ['AGN'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "  #  'AGN': 'yellow',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "\n",
    "df_sampled = df0.groupby('Combined_Class').apply(lambda x: x.sample(frac=0.5)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Initialize color map for the different combined classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df_sampled['Combined_Class'] == combined_class\n",
    "    \n",
    "    # Plot Post vs. Pre-Peak Mean Color with error bars\n",
    "    ax1.errorbar(df_sampled.loc[mask, 'Mean Color Post Peak (r-i)'], \n",
    "                 df_sampled.loc[mask, 'Mean Color Pre Peak (r-i)'], \n",
    "                 xerr=df_sampled.loc[mask, 'Post_Peak_Color_err_ri'],  # Error for x\n",
    "                 yerr=df_sampled.loc[mask, 'Pre_Peak_Color_err_ri'],  # Error for y\n",
    "                 fmt='.', label=combined_class, alpha=0.3, color=color)\n",
    "    \n",
    "    ax1.set_ylabel(\"Mean Color Pre Peak r-i (mag)\")\n",
    "    ax1.set_xlabel(\"Mean Color Post Peak r-i (mag)\")\n",
    "    ax1.set_xlim(-1, 1)\n",
    "    ax1.set_ylim(-1, 1)\n",
    "\n",
    "    # Plot Fade Time vs. Post-Peak Mean Color with error bars\n",
    "    ax2.errorbar(df_sampled.loc[mask, 'Mean Color Post Peak (g-r)'], \n",
    "                 df_sampled.loc[mask, 'Mean Color Pre Peak (r-i)'], \n",
    "                 xerr=df_sampled.loc[mask, 'Post_Peak_Color_err_gr'],  # Error for x\n",
    "                 yerr=df_sampled.loc[mask, 'Pre_Peak_Color_err_ri'],  # Error for y\n",
    "                 fmt='.', label=combined_class, alpha=0.3, color=color)\n",
    "    \n",
    "    ax2.set_ylabel(\"Mean Color Pre Peak r-i (mag)\")\n",
    "    ax2.set_xlabel(\"Mean Color Post Peak g-r (mag)\")\n",
    "    ax2.set_xlim(-1, 1)\n",
    "    ax2.set_ylim(-1, 1)\n",
    "\n",
    "# Create custom legend handles with higher alpha\n",
    "legend_handles = [Line2D([0], [0], marker='.', color='w', label=class_type,\n",
    "                          markerfacecolor=color, markersize=10, alpha=1) for class_type, color in colors.items()]\n",
    "\n",
    "# Add legend to both axes\n",
    "ax1.legend(handles=legend_handles, title=\"Combined Class\", loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "ax2.legend(handles=legend_handles, title=\"Combined Class\", loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/colorvscolor.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f64556-17c6-4646-8318-ebea1feef526",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Assuming df0 is already loaded with necessary columns including errors\n",
    "\n",
    "# Define combined classes and respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'yellow',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Remove Extreme Outliers\n",
    "# ----------------------------\n",
    "\n",
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.1)\n",
    "    Q3 = df[column].quantile(0.9)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    before_count = df.shape[0]\n",
    "    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    after_count = df_filtered.shape[0]\n",
    "    print(f\"Removed {before_count - after_count} outliers from '{column}'.\")\n",
    "    return df_filtered\n",
    "\n",
    "# Columns to check for outliers\n",
    "outlier_columns = ['Mean Color Pre Peak (r-i)', 'Mean Color Post Peak (r-i)', 'LengthScale_Wavelength']\n",
    "\n",
    "for col in outlier_columns:\n",
    "    if col in df0.columns:\n",
    "        df0 = remove_outliers_iqr(df0, col)\n",
    "    else:\n",
    "        print(f\"Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Remove Points with Mean Color Pre/Post Peak == 0.0\n",
    "# ----------------------------\n",
    "\n",
    "# Define conditions for removal\n",
    "conditions = (df0['Mean Color Pre Peak (r-i)'] != 0.0) & (df0['Mean Color Post Peak (r-i)'] != 0.0)\n",
    "\n",
    "# Number of points before removal\n",
    "before_zero_removal = df0.shape[0]\n",
    "\n",
    "# Apply conditions\n",
    "df0 = df0[conditions]\n",
    "\n",
    "# Number of points after removal\n",
    "after_zero_removal = df0.shape[0]\n",
    "\n",
    "print(f\"Removed {before_zero_removal - after_zero_removal} points with 'Mean Color Pre/Post Peak (r-i)' == 0.0.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Remove Sampling Step (Use Entire Dataset)\n",
    "# ----------------------------\n",
    "\n",
    "# Commented out the sampling step\n",
    "# df_sampled = df0.groupby('Combined_Class').apply(lambda x: x.sample(frac=0.5)).reset_index(drop=True)\n",
    "\n",
    "# Use the entire cleaned dataset\n",
    "df_cleaned = df0.copy()\n",
    "print(f\"Total number of rows after cleaning: {df_cleaned.shape[0]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Plotting\n",
    "# ----------------------------\n",
    "\n",
    "# Initialize color map for the different combined classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for combined_class, color in colors.items():\n",
    "    mask = df_cleaned['Combined_Class'] == combined_class\n",
    "    \n",
    "    # Ensure there are data points for the class\n",
    "    if not mask.any():\n",
    "        continue\n",
    "    \n",
    "    # Plot Pre-Peak Mean Color vs LengthScale_Wavelength with error bars\n",
    "    ax1.errorbar(\n",
    "        df_cleaned.loc[mask, 'LengthScale_Wavelength'], \n",
    "        df_cleaned.loc[mask, 'Mean Color Pre Peak (r-i)'], \n",
    "        yerr=df_cleaned.loc[mask, 'Pre_Peak_Color_err_ri'],  # Error for y\n",
    "        fmt='.', label=combined_class, alpha=0.6, color=color, markersize=4, linestyle='none'\n",
    "    )\n",
    "    \n",
    "    # Plot Post-Peak Mean Color vs LengthScale_Wavelength with error bars\n",
    "    ax2.errorbar(\n",
    "        df_cleaned.loc[mask, 'LengthScale_Wavelength'], \n",
    "        df_cleaned.loc[mask, 'Mean Color Post Peak (r-i)'], \n",
    "        yerr=df_cleaned.loc[mask, 'Post_Peak_Color_err_ri'],  # Error for y\n",
    "        fmt='.', label=combined_class, alpha=0.6, color=color, markersize=4, linestyle='none'\n",
    "    )\n",
    "\n",
    "# Set labels and limits for ax1\n",
    "ax1.set_xlabel(\"LengthScale_Wavelength\")\n",
    "ax1.set_ylabel(\"Mean Color Pre Peak r-i (mag)\")\n",
    "ax1.set_xlim(10, 30)\n",
    "ax1.set_ylim(-2, 2)\n",
    "ax1.set_title(\"Pre-Peak Mean Color vs LengthScale_Wavelength\")\n",
    "\n",
    "# Set labels and limits for ax2\n",
    "ax2.set_xlabel(\"LengthScale_Wavelength\")\n",
    "ax2.set_ylabel(\"Mean Color Post Peak r-i (mag)\")\n",
    "ax2.set_xlim(10, 30)\n",
    "ax2.set_ylim(-2, 2)\n",
    "ax2.set_title(\"Post-Peak Mean Color vs LengthScale_Wavelength\")\n",
    "\n",
    "# Create custom legend handles with higher alpha\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], marker='o', color='w', label=class_type,\n",
    "           markerfacecolor=color, markersize=8, alpha=0.6)\n",
    "    for class_type, color in colors.items()\n",
    "]\n",
    "\n",
    "# Add legend to both axes\n",
    "ax1.legend(handles=legend_handles, title=\"Combined Class\", loc='upper right', frameon=True, facecolor='white', edgecolor='black')\n",
    "ax2.legend(handles=legend_handles, title=\"Combined Class\", loc='upper right', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/LWvscolor_cleaned.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb622825-25cd-409e-b3d1-369be437120c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined classes and their respective colors\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'CLAGN': ['CLAGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'CLAGN': 'yellow',\n",
    "    'KN': 'purple',\n",
    "    'SLSN': 'black',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    mask = df0['Combined_Class'] == combined_class\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "    \n",
    "    # Plot Rise Time vs. Post-Peak Mean Color\n",
    "    ax1.scatter(df0.loc[mask, 'REDSHIFT_FINAL'], df0.loc[mask, 'Mean_Color_Pre_Peak_GR'],  label=combined_class, alpha=0.5, color=color, marker = '.')\n",
    "    ax1.set_xlabel(\"Redshift\")\n",
    "    ax1.set_ylabel(\"Pre-Peak Mean Color\")\n",
    "    #ax1.set_title(\"Pre-Peak Mean Color g-r\")\n",
    "    ax1.set_ylim(-1,1)\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot Fade Time vs. Post-Peak Mean Color\n",
    "    ax2.scatter(df0.loc[mask, 'REDSHIFT_FINAL'], df0.loc[mask, 'Mean_Color_Post_Peak_GR'], label=combined_class, alpha=0.5, color=color, marker = '.')\n",
    "    ax2.set_xlabel(\"Redshift\")\n",
    "    ax2.set_ylabel(\"Post-Peak Mean Color\")\n",
    "    #ax2.set_title(\"Fade Time vs. Post-Peak Mean Color g-r\")\n",
    "    ax2.set_ylim(-1,1)\n",
    "    #ax2.set_yscale('log')\n",
    "    ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0195a1c-7421-4a73-97dd-e980a803512b",
   "metadata": {},
   "source": [
    "### Redshift vs peak-mag/flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952b70d-cbd9-4d51-91a6-8fed537936bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize color map for the different object types\n",
    "colors = {'TDE': 'blue', 'SNIa-SALT3': 'red', 'CLAGN': 'green', 'SLSN-I+host': 'yellow', 'SNIIn-MOSFIT': 'purple', 'SNIb+HostXT_V19': 'grey', 'SNIcBL+HostXT_V19': 'pink', 'SNIax': 'black' }\n",
    "\n",
    "# Create a figure with subplots\n",
    "#fig, (ax1, ax2) = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "for object_type, color in colors.items():\n",
    "    mask = df0['Object_Type'] == object_type\n",
    "    \n",
    "    # Plot Rise Time vs. Post-Peak Mean Color\n",
    "    plt.scatter(df0.loc[mask, 'REDSHIFT_FINAL'], df0.loc[mask, 'PeakMag'], label=object_type, alpha=0.5, color=color, marker = '.')\n",
    "    plt.xlabel(\"Redshift\")\n",
    "    plt.ylabel(\"PeakFlux_GP\")\n",
    "    #ax1.set_title(\"Pre-Peak Mean Color g-r\")\n",
    "    #ax1.set_ylim(-1,1)\n",
    "    #ax1.set_yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01f886-9dde-4b3e-935d-bebc14d11f9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize color map for the different object types\n",
    "colors = {'TDE': 'blue', 'SNIa': 'red', 'CLAGN': 'green', 'SLSN-I': 'yellow', 'SNII': 'purple'}\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for object_type, color in colors.items():\n",
    "    mask = df0['Object_Type'] == object_type\n",
    "    \n",
    "    # Plot Rise Time vs. Post-Peak Mean Color\n",
    "    ax1.scatter(df0.loc[mask, 'REDSHIFT_FINAL'], df0.loc[mask, 'PeakMag'], label=object_type, alpha=0.5, color=color, marker = '.')\n",
    "    ax1.set_xlabel(\"Redshift\")\n",
    "    ax1.set_ylabel(\"PeakMag\")\n",
    "    #ax1.set_title(\"Pre-Peak Mean Color g-r\")\n",
    "    #ax1.set_ylim(-1,1)\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot Fade Time vs. Post-Peak Mean Color\n",
    "    ax2.scatter(df0.loc[mask, 'REDSHIFT_FINAL'], df0.loc[mask, 'Mean_Color_Post_Peak_GR'], label=object_type, alpha=0.5, color=color, marker = '.')\n",
    "    ax2.set_xlabel(\"Redshift\")\n",
    "    ax2.set_ylabel(\"Post-Peak Mean Color\")\n",
    "    #ax2.set_title(\"Fade Time vs. Post-Peak Mean Color g-r\")\n",
    "    ax2.set_ylim(-1,1)\n",
    "    #ax2.set_yscale('log')\n",
    "    ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9d23f-d632-4ee3-94d1-95ec4012f165",
   "metadata": {},
   "source": [
    "### variable distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8778cd5-aa80-4bb8-b7b1-e1ffda1b227f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Assuming df0 is your DataFrame with all object types\n",
    "colors = {\n",
    "    'SNI': 'red',\n",
    "    'SNII': 'green',\n",
    "    'AGN': 'orange',\n",
    " #   'KN': 'purple',\n",
    "    'SLSN': 'purple',\n",
    "    'TDE': 'blue'\n",
    "}\n",
    "\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "#    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "rename_map = {\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time'\n",
    "    \n",
    "}\n",
    "\n",
    "df0.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Filtered columns based on your criteria\n",
    "filtered_columns = [\n",
    " #   'Amplitude',\n",
    " #   'LengthScale_Time',\n",
    " #   'LengthScale_Wavelength',\n",
    " #   'Mean Color Pre Peak (g-r)',\n",
    " #   'Mean Color Post Peak (g-r)',\n",
    " #   'Mean Color Pre Peak (r-i)',\n",
    " #   'Mean Color Post Peak (r-i)',\n",
    "#     'Slope Pre Peak (g-r)',\n",
    "     'Slope Post Peak (g-r)',\n",
    "   #  'Slope Pre Peak (r-i)',\n",
    "     'Slope Post Peak (r-i)',\n",
    "     'Rise time',\n",
    "     'Fade time'\n",
    "    \n",
    "]\n",
    "\n",
    "# Custom axis limits\n",
    "custom_limits = {\n",
    "    'Amplitude': (5, 15),\n",
    "    'LengthScale_Time': (4, 14),\n",
    "    'LengthScale_Wavelength': (12, 24),\n",
    "    'Mean Color Pre Peak (g-r)': (-0.8, 0.8),\n",
    "    'Mean Color Pre Peak (r-i)': (-0.8, 0.8),\n",
    "    'Mean Color Post Peak (g-r)': (-0.8, 0.8),\n",
    "    'Mean Color Post Peak (r-i)': (-0.8, 0.8),\n",
    "    'Slope Pre Peak (g-r)': (-0.04, 0.04),\n",
    "    'Slope Pre Peak (r-i)': (-0.04, 0.04),\n",
    "    'Slope Post Peak (g-r)': (-0.02, 0.04),\n",
    "    'Slope Post Peak (r-i)': (-0.02, 0.04),\n",
    "    'Rise time': (0, 100),\n",
    "    'Fade time': (0, 200)\n",
    "}\n",
    "\n",
    "\n",
    "# Calculate the number of rows needed for the subplots\n",
    "num_rows = int(np.ceil(len(filtered_columns) / 2))\n",
    "\n",
    "# Set up the matplotlib figure with the right number of subplots\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(20, 4 * num_rows))\n",
    "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "# Loop through the filtered columns and create the plots for each object type\n",
    "for i, col in enumerate(filtered_columns):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Determine quantile range for binning (e.g., 5th to 95th percentile)\n",
    "    lower_quantile = df0[col].quantile(0.0005)\n",
    "    upper_quantile = df0[col].quantile(0.9995)\n",
    "    \n",
    "    # Adjust the binning strategy to focus on the specified quantile range\n",
    "    bins = np.linspace(lower_quantile, upper_quantile, 1000)  # Reduced number of bins to smooth the distribution\n",
    "\n",
    "    # Plot histograms for each object type\n",
    "    for class_type, obj_types in combined_classes.items():\n",
    "        data = df0[df0['Object_Type'].isin(obj_types)][col].clip(lower_quantile, upper_quantile)\n",
    "        alpha = 0.6 if class_type == 'TDE' else 0.4  # Emphasize TDE by setting alpha to 0.6\n",
    "        linewidth = 1.2 if class_type == 'TDE' else 1  # Emphasize TDE by setting linewidth to 1.5\n",
    "        \n",
    "        # Adjust KDE using bw_adjust to better fit the data\n",
    "        sns.histplot(\n",
    "            data, bins=bins, color=colors[class_type], label=class_type, stat=\"probability\", linewidth=linewidth,\n",
    "            ax=ax, alpha=alpha, kde=False, kde_kws={'bw_adjust': 0.1}  # Lower bandwidth for tighter KDE fit\n",
    "        )\n",
    "\n",
    "    # Set custom axis limits if specified\n",
    "    if col in custom_limits:\n",
    "        ax.set_xlim(custom_limits[col])\n",
    "\n",
    "    ax.set_xlabel(col, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Probability', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right')  # Add legend to each subplot\n",
    "\n",
    "# Check if there are any unused axes and remove them\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/variable_dist1.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41804a6b-262f-4fb5-9730-955f402e9340",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'TDE': 'blue',\n",
    "    'Non-TDE': 'white'\n",
    "}\n",
    "\n",
    "combined_classes = {\n",
    "    'TDE': ['TDE'],\n",
    "    'Non-TDE': [\n",
    "        'SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19', \n",
    "        'SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19', \n",
    "        'CLAGN', 'KN_K17', 'KN_B19', 'SLSN-I+host', 'SLSN-I_no_host'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Filtered columns based on your criteria\n",
    "filtered_columns = [\n",
    "    'Amplitude',\n",
    "    'LengthScale_Time',\n",
    "    'LengthScale_Wavelength',\n",
    "    'Mean_Color_Pre_Peak',\n",
    "    'Mean_Color_Post_Peak',\n",
    "    'Slope_Pre_Peak',\n",
    "    'Slope_Post_Peak',\n",
    "    'Rise_Time',\n",
    "    'Decay_Time'\n",
    "]\n",
    "\n",
    "# Calculate the number of rows needed for the subplots\n",
    "num_rows = int(np.ceil(len(filtered_columns) / 2))\n",
    "\n",
    "# Set up the matplotlib figure with the right number of subplots\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(20, 4 * num_rows))\n",
    "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "# Loop through the filtered columns and create the plots for each object type 'Mean_Color_Pre_Peak_GR',\n",
    "\n",
    "for i, col in enumerate(filtered_columns):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Determine quantile range for binning (e.g., 5th to 95th percentile)\n",
    "    lower_quantile = df0[col].quantile(0.0005)\n",
    "    upper_quantile = df0[col].quantile(0.9995)\n",
    "    \n",
    "    # Adjust the binning strategy to focus on the specified quantile range\n",
    "    bins = np.linspace(lower_quantile, upper_quantile, 100)  \n",
    "\n",
    "    # Plot histograms for TDE and Others\n",
    "    for class_type, obj_types in combined_classes.items():\n",
    "        data = df0[df0['Object_Type'].isin(obj_types)][col].clip(lower_quantile, upper_quantile)\n",
    "        alpha = 1.0 if class_type == 'TDE' else 0.5  # Emphasize TDE by setting alpha to 1.0\n",
    "        linewidth = 2 if class_type == 'TDE' else 1  # Emphasize TDE by setting linewidth to 2\n",
    "        sns.histplot(data, bins=bins, color=colors[class_type], label=class_type if i == 0 else \"\", stat=\"probability\", linewidth=linewidth, ax=ax, alpha=alpha, kde=True, )\n",
    "\n",
    "    ax.set_xlabel(col, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Probability', fontsize=12, fontweight='bold')\n",
    "    if i == 0:  # Add legend only to the first subplot for clarity\n",
    "        ax.legend()\n",
    "\n",
    "# Check if there are any unused axes and remove them\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e911b8-b858-4b96-b034-1fa2c1d2d53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a158e42-afb3-4147-96ff-ec4ea83e24ed",
   "metadata": {},
   "source": [
    "## Correlation, PCA, ML classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e193b-2c1b-4416-97ba-3684fff04cc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "excluded_columns = [\n",
    "    'SNID',\n",
    " #'Amplitude', \n",
    " #'LengthScale_Time',\n",
    " #'LengthScale_Wavelength',\n",
    " 'TruePeakMJD', \n",
    "'peak_time_MJD',\n",
    "# 'Mean Color Pre Peak (g-r)',\n",
    " 'Pre_Peak_Color_err_gr', \n",
    "# 'Mean Color Post Peak (g-r)', \n",
    " 'Post_Peak_Color_err_gr', \n",
    "# 'Slope Pre Peak (g-r)',\n",
    " 'Slope_Err_Pre_Peak_gr',\n",
    "# 'Slope Post Peak (g-r)', \n",
    " 'Slope_Err_Post_Peak_gr', \n",
    "# 'Mean Color Pre Peak (r-i)', \n",
    " 'Pre_Peak_Color_err_ri',\n",
    "# 'Mean Color Post Peak (r-i)',\n",
    " 'Post_Peak_Color_err_ri', \n",
    "# 'Slope Pre Peak (r-i)', \n",
    " 'Slope_Err_Pre_Peak_ri',\n",
    "# 'Slope Post Peak (r-i)', \n",
    " 'Slope_Err_Post_Peak_ri',\n",
    "# 'Rise time', \n",
    "# 'Fade time',\n",
    "     'REDSHIFT_FINAL',\n",
    "    'REDSHIFT_FINAL_ERR',\n",
    "    'Error'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Function to filter numeric columns based on exclusions\n",
    "def filter_columns(df, excluded_cols):\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    filtered_columns = [col for col in numeric_columns if col not in excluded_cols]\n",
    "    return filtered_columns\n",
    "\n",
    "# Filter the DataFrame to include only TDE objects\n",
    "df_tde = df0[df0['Object_Type'] == 'TDE'].copy()\n",
    "\n",
    "# Apply the filtering based on the excluded_columns list\n",
    "filtered_columns = filter_columns(df_tde, excluded_columns)\n",
    "\n",
    "# Ensure \"Amplitude\" is included and check the filtered columns\n",
    "print(\"Filtered Columns: \", filtered_columns)\n",
    "\n",
    "# Plotting Correlation Heatmap for TDE with the lower triangle and diagonal\n",
    "def plot_correlation_heatmap(data):\n",
    "    plt.figure(figsize=(12, 10)) \n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = data.corr(method='kendall')\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=True, fmt=\".2f\", square=True, cbar_kws={\"shrink\": .75}, linewidths=.5)\n",
    "\n",
    "    # Enhanced title to reflect the analysis context\n",
    "    plt.title('Correlation Matrix of Selected TDE Features', fontsize=18)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/corrTDE_ELAsTiCC2.png', dpi=150)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the heatmap\n",
    "data_numeric = df_tde[filtered_columns]\n",
    "plot_correlation_heatmap(data_numeric)\n",
    "\n",
    "# Function to perform PCA and plot the results\n",
    "def perform_pca(data, axes):\n",
    "    imputer = SimpleImputer(strategy='median')  # Handle NaN values by imputation\n",
    "    data_imputed = imputer.fit_transform(data)\n",
    "    scaler = StandardScaler()  # Standardize the data\n",
    "    data_scaled = scaler.fit_transform(data_imputed)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca_data = pca.fit_transform(data_scaled)\n",
    "\n",
    "    # Scree plot of the explained variance\n",
    "    ax_scree = axes[0]  # First subplot for scree plot\n",
    "    ax_loadings = axes[1]  # Second subplot for loadings plot\n",
    "\n",
    "    ax_scree.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, color='blue', alpha=0.7)\n",
    "    ax_scree.set_ylabel('Explained Variance Ratio', fontsize=14)\n",
    "    ax_scree.set_xlabel('Principal Component', fontsize=14)\n",
    "    ax_scree.set_title('PCA Scree Plot for TDE Features', fontsize=16)\n",
    "    \n",
    "    # PCA Component Loadings Plot with annotations\n",
    "    components_df = pd.DataFrame(pca.components_, columns=data.columns, index=[f'PC{i+1}' for i in range(len(pca.components_))])\n",
    "    sns.heatmap(components_df, annot=True, cmap='coolwarm', ax=ax_loadings, fmt=\".2f\", cbar_kws={\"shrink\": .75})\n",
    "    ax_loadings.set_title('PCA Component Loadings for TDE Features', fontsize=16)\n",
    "    ax_loadings.set_xticklabels(ax_loadings.get_xticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "    ax_loadings.set_yticklabels(ax_loadings.get_yticklabels(), fontsize=12)\n",
    "\n",
    "# Preparing for PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "data_numeric = df_tde[filtered_columns]\n",
    "if data_numeric.shape[1] > 0:  # Proceed with PCA if there are columns left\n",
    "    perform_pca(data_numeric, axes)\n",
    "else:\n",
    "    print(\"No columns left for PCA after preprocessing for TDE. Check your data and preprocessing steps.\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3fc4c-2f71-44d6-86a6-234719ee8e5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "x = df_tde['Slope Post Peak (r-i)']\n",
    "y = df_tde['Mean Color Post Peak (r-i)']\n",
    "\n",
    "\n",
    "\n",
    "# Combine x and y into a DataFrame\n",
    "data = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "# Drop rows where either x or y is NaN\n",
    "data_clean = data.dropna(subset=['x', 'y'])\n",
    "\n",
    "# Extract cleaned x and y\n",
    "x_clean = data_clean['x']\n",
    "y_clean = data_clean['y']\n",
    "pearson_corr, _ = pearsonr(x_clean, y_clean)\n",
    "spearman_corr, _ = spearmanr(x_clean, y_clean)\n",
    "print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc02954-cfb6-4535-a8b3-16b234d7228e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=x_clean, y=y_clean)\n",
    "plt.title('Slope Post Peak (r-i) vs Mean Color Post Peak (r-i)')\n",
    "plt.xlabel('Slope Post Peak (r-i)')\n",
    "plt.ylabel('Mean Color Post Peak (r-i)')\n",
    "plt.show()\n",
    "sns.lmplot(x='x', y='y', data=data_clean)\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.xlabel('Slope Post Peak (r-i)')\n",
    "plt.ylabel('Mean Color Post Peak (r-i)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13274166-c18a-4fbd-8ba0-da9ef6c07c17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        # Calculate Q1 and Q3\n",
    "        Q1 = df_clean[col].quantile(0.1)\n",
    "        Q3 = df_clean[col].quantile(0.9)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define bounds\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        # Filter data within bounds\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define columns to exclude\n",
    "excluded_columns = [\n",
    "    'SNID',\n",
    "    'TruePeakMJD', \n",
    "    'peak_time_MJD',\n",
    "    'Pre_Peak_Color_err_gr', \n",
    "    'Post_Peak_Color_err_gr', \n",
    "    'Slope_Err_Pre_Peak_gr',\n",
    "    'Slope_Err_Post_Peak_gr', \n",
    "    'Pre_Peak_Color_err_ri',\n",
    "    'Post_Peak_Color_err_ri', \n",
    "    'Slope_Err_Pre_Peak_ri',\n",
    "    'Slope_Err_Post_Peak_ri',\n",
    "    'REDSHIFT_FINAL',\n",
    "    'REDSHIFT_FINAL_ERR',\n",
    "    'Error',\n",
    "    'Object_Type'  # Exclude if it's non-numeric\n",
    "]\n",
    "\n",
    "# Function to filter numeric columns based on exclusions\n",
    "def filter_columns(df, excluded_cols):\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    filtered_columns = [col for col in numeric_columns if col not in excluded_cols]\n",
    "    return filtered_columns\n",
    "\n",
    "# Assuming df0 is your original DataFrame\n",
    "# Filter the DataFrame to include only TDE objects\n",
    "df_tde = df0[df0['Object_Type'] == 'TDE'].copy()\n",
    "\n",
    "# Get the filtered columns\n",
    "filtered_columns = filter_columns(df_tde, excluded_columns)\n",
    "\n",
    "# Display the columns being used\n",
    "print(\"Filtered Columns: \", filtered_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09f24f-0467-49ec-b827-0bea814c4030",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "columns_with_outliers = [\n",
    "    'Slope Post Peak (r-i)', \n",
    "    'Mean Color Post Peak (r-i)', \n",
    "    'Slope Post Peak (r-i)', \n",
    "    'Mean Color Post Peak (r-i)',\n",
    "    'Fade time',\n",
    "    'Rise time'\n",
    "]\n",
    "\n",
    "df_tde_no_outliers = remove_outliers_iqr(df_tde, filtered_columns)\n",
    "\n",
    "# Check the number of observations before and after outlier removal\n",
    "print(f\"Number of observations before outlier removal: {len(df_tde)}\")\n",
    "print(f\"Number of observations after outlier removal: {len(df_tde_no_outliers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd484446-5130-4a45-b69f-94d923391bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the filtered columns\n",
    "df_tde_no_outliers = df_tde_no_outliers.dropna(subset=filtered_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c92d0-c51a-4f62-b58a-29c02b0195ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numeric, coercing errors to NaN (which have already been dropped)\n",
    "df_tde_no_outliers[filtered_columns] = df_tde_no_outliers[filtered_columns].apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d6673-0366-4ce9-960e-0ca62d82a2b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_heatmap(data, columns):\n",
    "    plt.figure(figsize=(12, 10)) \n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = data[columns].corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "    # Draw the heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        cmap='coolwarm',\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        square=True,\n",
    "        cbar_kws={\"shrink\": .75},\n",
    "        linewidths=.5\n",
    "    )\n",
    "\n",
    "    # Title and layout adjustments\n",
    "    plt.title('Correlation Matrix of Selected TDE Features (Outliers Removed)', fontsize=18)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/corrTDE_no_outliers.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plot_correlation_heatmap(df_tde_no_outliers, filtered_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7ac1b-843b-41b5-ad0b-9080d12570a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in filtered_columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Before outlier removal\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=df_tde[col])\n",
    "    plt.title(f'Before Outlier Removal - {col}')\n",
    "\n",
    "    # After outlier removal\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df_tde_no_outliers[col])\n",
    "    plt.title(f'After Outlier Removal - {col}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plt.savefig('/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/after_outliers.png', dpi=150)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530b409-d182-4423-b68f-76d5d18e390d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860cb2b7-bad9-40d7-8216-55f6f530e3e6",
   "metadata": {},
   "source": [
    "### ML Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d31bc-8d13-4d32-94a3-3b90a7878b10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files/ELAsTiCC2_Test.csv'\n",
    "# Read the CSV file into a DataFrame\n",
    "df0 = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to ensure it was loaded correctly\n",
    "#print(df0.head())\n",
    "df0\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66c5f6ea-c635-46e5-89ad-a84380d0e9e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Define the path to your consolidated CSV file\n",
    "csv_file_path = \"/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files/ELAsTiCC2_Test101.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Successfully loaded '{csv_file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    exit(1)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Error: The file '{csv_file_path}' is empty.\")\n",
    "    exit(1)\n",
    "except pd.errors.ParserError:\n",
    "    print(f\"Error: The file '{csv_file_path}' could not be parsed.\")\n",
    "    exit(1)\n",
    "\n",
    "# Check if 'Object_Type' column exists\n",
    "if 'Object_Type' not in df.columns:\n",
    "    print(\"Error: 'Object_Type' column not found in the CSV file.\")\n",
    "    exit(1)\n",
    "\n",
    "# Display the count of each unique Object_Type, including NaN values\n",
    "print(\"\\nCounts of each 'Object_Type':\")\n",
    "object_type_counts = df['Object_Type'].value_counts(dropna=False)\n",
    "print(object_type_counts)\n",
    "\n",
    "# Remove rows where 'Object_Type' is NaN\n",
    "initial_row_count = len(df)\n",
    "df_cleaned = df.dropna(subset=['Object_Type'])\n",
    "final_row_count = len(df_cleaned)\n",
    "rows_removed = initial_row_count - final_row_count\n",
    "\n",
    "print(f\"\\nRemoved {rows_removed} rows with NaN in 'Object_Type'.\")\n",
    "print(f\"DataFrame now contains {final_row_count} rows.\")\n",
    "\n",
    "# (Optional) Save the cleaned DataFrame to a new CSV file\n",
    "save_cleaned = True  # Set to False if you do not wish to save the cleaned DataFrame\n",
    "if save_cleaned:\n",
    "    cleaned_csv_path = \"/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files/ELAsTiCC2_Test.csv\"\n",
    "    try:\n",
    "        df_cleaned.to_csv(cleaned_csv_path, index=False)\n",
    "        print(f\"Cleaned data saved to '{cleaned_csv_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cleaned CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01545ad7-8b0b-40f9-8006-3ce4e41a5a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3af1c6b-88d0-4440-8e82-4cf53641f11d",
   "metadata": {},
   "source": [
    "### Multi-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fa0c7-658d-4071-a3ee-8d1f31230b7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select relevant columns for the model and exclude 'SNID', any truth info \n",
    "\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakFlux_GP', 'REDSHIFT_FINAL'] \n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower()] #remove error estimates\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "X = df0[feature_columns].fillna(-999)  # Handling missing values by filling with -999\n",
    "y = df0['Object_Type']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) #Standardize features by removing the mean and scaling to unit variance.\n",
    "                                   #The standard score of a sample x is calculated as:  z = (x - u) / s \n",
    "                                   #where u is the mean of the training samples or zero if with_mean=False, \n",
    "                                   #and s is the standard deviation of the training samples or one if with_std=False.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "\n",
    "def plot_normalized_conf_matrix(y_true, y_pred, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', ax=ax, \n",
    "                xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "    ax.set_title(title)\n",
    "\n",
    "strategies = {\n",
    "    'Original': None,  # No sampling for the original distribution\n",
    "    'Undersampling': RandomUnderSampler(random_state=42),\n",
    "    'Oversampling (SMOTE)': SMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "# Adjusting the figure size and orientation of the labels for clarity\n",
    "fig, axs = plt.subplots(len(strategies), 2, figsize=(24, len(strategies) * 8), gridspec_kw={'width_ratios': [2, 3]})\n",
    "\n",
    "for i, (strategy_name, sampler) in enumerate(strategies.items()):\n",
    "    if strategy_name == 'Original':\n",
    "        X_train_mod, y_train_mod = X_train, y_train\n",
    "    else:\n",
    "        X_train_mod, y_train_mod = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Visualize class distribution with improved label orientation\n",
    "    sns.countplot(y=y_train_mod, ax=axs[i, 0], palette='Set2', order=y_train_mod.value_counts().index)\n",
    "    axs[i, 0].set_title(f'Class Distribution : {strategy_name}')\n",
    "    axs[i, 0].set_xlabel('Counts')\n",
    "    axs[i, 0].set_ylabel('Class')\n",
    "    axs[i, 0].tick_params(axis='y', rotation=45)  # Rotate labels to avoid overlap\n",
    "    \n",
    "    # Train Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf_classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "    # Train the Gradient Boosting classifier\n",
    "    bdt_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "    bdt_classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "    # Predictiction \n",
    "    y_pred_rf = rf_classifier.predict(X_test)\n",
    "    y_pred_bdt = bdt_classifier.predict(X_test)\n",
    "\n",
    "    #plot normalized confusion matrix\n",
    "    plot_normalized_conf_matrix(y_test, y_pred_rf, axs[i, 1], f'RF: {strategy_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def plot_precision_recall_curve_multiclass(y_true, y_probas, class_labels):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        # Binarize the output for the current class\n",
    "        y_true_bin = (y_true == class_label).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_probas[:, i])\n",
    "        auc_score = auc(recall, precision) \n",
    "        plt.plot(recall, precision, lw=2, label=f'Class {class_label} (AUC = {auc_score:0.2f})')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Precision-Recall Curve for Multiple Classes\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Get the probabilities for each class\n",
    "y_probs_rf = rf_classifier.predict_proba(X_test)\n",
    "y_probs_bdt = bdt_classifier.predict_proba(X_test)\n",
    "\n",
    "# Class labels for the multi-class setupKey Adjustments:\n",
    "class_labels = rf_classifier.classes_\n",
    "\n",
    "# Plot the precision-recall curve for Random Forest\n",
    "plot_precision_recall_curve_multiclass(y_test, y_probs_rf, class_labels)\n",
    "\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nGradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bdt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00fa11-19aa-4733-af14-5263cfc21116",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the combined classes\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "# Map the original classes to combined classes\n",
    "class_mapping = {original: combined for combined, originals in combined_classes.items() for original in originals}\n",
    "df0['Combined_Type'] = df0['Object_Type'].map(class_mapping)\n",
    "\n",
    "# Select relevant columns for the model and exclude 'SNID', any truth info\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakFlux_GP', 'REDSHIFT_FINAL', 'Num_Peaks_GP']\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower()]  # remove error estimates\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "X = df0[feature_columns].fillna(-999)  # Handling missing values by filling with -999\n",
    "y = df0['Combined_Type']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardize features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "\n",
    "def plot_normalized_conf_matrix(y_true, y_pred, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', ax=ax, \n",
    "                xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "    ax.set_title(title)\n",
    "\n",
    "strategies = {\n",
    "    'Original': None,  # No sampling for the original distribution\n",
    "    'Undersampling': RandomUnderSampler(random_state=42),\n",
    "    'Oversampling (SMOTE)': SMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "# Adjusting the figure size and orientation of the labels for clarity\n",
    "fig, axs = plt.subplots(len(strategies), 2, figsize=(14, len(strategies) * 4), gridspec_kw={'width_ratios': [2, 3]})\n",
    "\n",
    "for i, (strategy_name, sampler) in enumerate(strategies.items()):\n",
    "    if strategy_name == 'Original':\n",
    "        X_train_mod, y_train_mod = X_train, y_train\n",
    "    else:\n",
    "        X_train_mod, y_train_mod = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Visualize class distribution with improved label orientation\n",
    "    sns.countplot(y=y_train_mod, ax=axs[i, 0], palette='Set2', order=y_train_mod.value_counts().index)\n",
    "    axs[i, 0].set_title(f'Class Distribution: {strategy_name}')\n",
    "    axs[i, 0].set_xlabel('Counts')\n",
    "    axs[i, 0].set_ylabel('Class')\n",
    "    axs[i, 0].tick_params(axis='y', rotation=45)  # Rotate labels to avoid overlap\n",
    "    \n",
    "    # Train Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf_classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "    # Train the Gradient Boosting classifier\n",
    "    bdt_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "    bdt_classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "    # Prediction \n",
    "    y_pred_rf = rf_classifier.predict(X_test)\n",
    "    y_pred_bdt = bdt_classifier.predict(X_test)\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plot_normalized_conf_matrix(y_test, y_pred_rf, axs[i, 1], f'RF: {strategy_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def plot_precision_recall_curve_multiclass(y_true, y_probas, class_labels):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        # Binarize the output for the current class\n",
    "        y_true_bin = (y_true == class_label).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_probas[:, i])\n",
    "        auc_score = auc(recall, precision) \n",
    "        plt.plot(recall, precision, lw=2, label=f'Class {class_label} (AUC = {auc_score:0.2f})')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Precision-Recall Curve for Multiple Classes\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Get the probabilities for each class\n",
    "y_probs_rf = rf_classifier.predict_proba(X_test)\n",
    "y_probs_bdt = bdt_classifier.predict_proba(X_test)\n",
    "\n",
    "# Class labels for the multi-class setup\n",
    "class_labels = rf_classifier.classes_\n",
    "\n",
    "# Plot the precision-recall curve for Random Forest\n",
    "plot_precision_recall_curve_multiclass(y_test, y_probs_rf, class_labels)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nGradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bdt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b34ef7-3078-411f-b999-e2c3153cf4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91307b4-dae4-401a-96b1-ac131ba9a0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25aef70-5066-4951-af50-53970aecaf53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_recall_curve, classification_report, roc_auc_score,\n",
    "    make_scorer, precision_score, recall_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.stats import uniform, randint\n",
    "import os\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# ---------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def find_threshold_multiclass(y_true, y_probs, target_class, target_metric='precision', target_value=0.95):\n",
    "    \"\"\"\n",
    "    Find the threshold for desired precision or recall for a specific class.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels.\n",
    "    - y_probs: Predicted probabilities for each class.\n",
    "    - target_class: The class index for which to find the threshold.\n",
    "    - target_metric: 'precision' or 'recall'.\n",
    "    - target_value: Desired value for the target metric.\n",
    "    \n",
    "    Returns:\n",
    "    - Threshold value.\n",
    "    \"\"\"\n",
    "    y_true_bin = (y_true == target_class).astype(int)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true_bin, y_probs[:, target_class])\n",
    "    \n",
    "    if target_metric == 'precision':\n",
    "        # Find indices where precision >= target_value\n",
    "        idx = np.where(precision >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% precision for class '{class_labels[target_class]}'. Returning threshold=1.0\")\n",
    "            return 1.0  # Maximum threshold\n",
    "        # Choose the threshold corresponding to the first occurrence\n",
    "        return thresholds[idx[0]]\n",
    "    elif target_metric == 'recall':\n",
    "        # Find indices where recall >= target_value\n",
    "        idx = np.where(recall >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% recall for class '{class_labels[target_class]}'. Returning threshold=0.0\")\n",
    "            return 0.0  # Minimum threshold\n",
    "        # Choose the threshold corresponding to the last occurrence\n",
    "        return thresholds[idx[-1]]\n",
    "    else:\n",
    "        raise ValueError(\"target_metric must be 'precision' or 'recall'.\")\n",
    "\n",
    "def plot_confusion_matrix_multiclass(cm, classes, ax, title, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - cm: Confusion matrix.\n",
    "    - classes: List of class names.\n",
    "    - ax: Matplotlib Axes object.\n",
    "    - title: Title for the plot.\n",
    "    - normalize: Whether to normalize the confusion matrix.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        cm_normalized = cm\n",
    "\n",
    "    sns.heatmap(cm_normalized, annot=False, fmt='.2f', cmap='Blues', ax=ax,\n",
    "                xticklabels=classes, yticklabels=classes, cbar=False)\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if normalize:\n",
    "                percentage = cm_normalized[i, j] * 100\n",
    "                count = cm[i, j]\n",
    "                text = f'{percentage:.1f}%\\n({count})'\n",
    "            else:\n",
    "                text = f'{cm[i, j]}'\n",
    "            # Adjust text color based on background for better visibility\n",
    "            color = 'white' if cm_normalized[i, j] > 0.5 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, text,\n",
    "                    ha='center', va='center',\n",
    "                    color=color, fontsize=8,\n",
    "                    bbox=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    ax.set_xlabel('Predicted Class', fontsize=10)\n",
    "    ax.set_ylabel('True Class', fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "def plot_precision_recall_curve_multiclass(y_true, y_probas, class_labels, save_path, figsize=(6, 4)):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for each class in a multi-class setting.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels.\n",
    "    - y_probas: Predicted probabilities for each class.\n",
    "    - class_labels: List of class names.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Figure size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        # Binarize the output for the current class\n",
    "        y_true_bin = (y_true == i).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_probas[:, i])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, lw=1.5, label=f'{class_label} (AUC = {pr_auc:.2f})')\n",
    "    \n",
    "    plt.xlabel('Recall', fontsize=10)\n",
    "    plt.ylabel('Precision', fontsize=10)\n",
    "    plt.title('Precision-Recall Curve for Multi-class', fontsize=12)\n",
    "    plt.legend(fontsize=8, loc='lower left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance_multiclass(model, feature_names, save_path, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot and save the feature importance for a multi-class XGBoost model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained XGBoost model.\n",
    "    - feature_names: List of feature names.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Figure size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.gca()\n",
    "    xgb.plot_importance(\n",
    "        model,\n",
    "        max_num_features=20,\n",
    "        importance_type='gain',\n",
    "        show_values=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title('Feature Importance', fontsize=12)\n",
    "    ax.set_xlabel('Importance (Gain)', fontsize=10)\n",
    "    ax.set_ylabel('Features', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_class_distribution(y, classes, ax, title):\n",
    "    \"\"\"\n",
    "    Plot the class distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Array-like of labels.\n",
    "    - classes: List of class names.\n",
    "    - ax: Matplotlib Axes object.\n",
    "    - title: Title for the plot.\n",
    "    \"\"\"\n",
    "    sns.countplot(y=y, ax=ax, palette='Set2', order=np.sort(np.unique(y)))\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel('Counts', fontsize=10)\n",
    "    ax.set_ylabel('Class', fontsize=10)\n",
    "    \n",
    "    # Dynamically adjust x-ticks to prevent overlapping\n",
    "    max_count = y.value_counts().max()\n",
    "    step = max_count // 5 if max_count > 5 else 1\n",
    "    ticks = list(range(0, int(max_count) + step, step))\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(ticks, rotation=0, fontsize=8)\n",
    "    \n",
    "    ax.set_yticklabels(classes[np.sort(np.unique(y))], rotation=45, fontsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data Preparation\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the combined classes, including the 'KN' class\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "   # 'KN': ['KN_K17', 'KN_B19'],  # Commented out as per original code\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "# Map the original classes to combined classes\n",
    "class_mapping = {original: combined for combined, originals in combined_classes.items() for original in originals}\n",
    "df0['Combined_Type'] = df0['Object_Type'].map(class_mapping)\n",
    "\n",
    "# Display initial counts\n",
    "initial_counts = df0['Combined_Type'].value_counts()\n",
    "print(\"Initial Class Distribution\")\n",
    "print(initial_counts)\n",
    "\n",
    "# Select relevant columns for the model and exclude 'SNID', any truth info\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakFlux_GP', 'REDSHIFT_FINAL', 'Num_Peaks_GP']\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower()]  # Remove error estimates\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "rename_map = {\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time'\n",
    "}\n",
    "df0.rename(columns=rename_map, inplace=True)\n",
    "feature_columns = [rename_map.get(col, col) for col in feature_columns]\n",
    "\n",
    "# Define features and target\n",
    "X = df0[feature_columns].fillna(-999)  # Handling missing values by filling with -999\n",
    "y = df0['Combined_Type']\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardize features\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=feature_columns)  # Retain feature names\n",
    "\n",
    "# Split the data with a 1:1 train-test ratio and stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.5, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Display counts after train-test split\n",
    "print(\"\\nTraining Set Class Distribution\")\n",
    "print(pd.Series(y_train).value_counts().sort_index().rename(index=lambda x: class_labels[x]))\n",
    "\n",
    "print(\"\\nTest Set Class Distribution\")\n",
    "print(pd.Series(y_test).value_counts().sort_index().rename(index=lambda x: class_labels[x]))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'learning_rate': uniform(0.01, 0.29),  # 0.01 to 0.3\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.5, 0.5),  # 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # 0.5 to 1.0\n",
    "    'gamma': uniform(0, 5),\n",
    "    'scale_pos_weight': uniform(1, 10)  # Useful for imbalanced classes\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier for multi-class\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(class_labels),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define a custom scorer that emphasizes macro recall at a certain precision threshold\n",
    "def macro_recall_at_precision_threshold(y_true, y_probas, precision_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the macro recall across all classes at a specified precision threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels.\n",
    "    - y_probas: Predicted probabilities for each class.\n",
    "    - precision_threshold: Desired precision threshold.\n",
    "    \n",
    "    Returns:\n",
    "    - Macro recall score.\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    for class_idx in range(len(class_labels)):\n",
    "        threshold = find_threshold_multiclass(y_true, y_probas, target_class=class_idx, target_metric='precision', target_value=precision_threshold)\n",
    "        y_pred = (y_probas[:, class_idx] >= threshold).astype(int)\n",
    "        recall = recall_score((y_true == class_idx).astype(int), y_pred, zero_division=0)\n",
    "        recalls.append(recall)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "# Create a scorer for RandomizedSearchCV\n",
    "def custom_scorer_multiclass(y_true, y_pred_probas):\n",
    "    return macro_recall_at_precision_threshold(y_true, y_pred_probas, precision_threshold=0.95)\n",
    "\n",
    "scorer = make_scorer(custom_scorer_multiclass, needs_proba=True)\n",
    "\n",
    "# Initialize RandomizedSearchCV with StratifiedKFold\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    scoring=scorer,\n",
    "    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best estimator\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "print(f\"\\nBest parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best macro recall at 95% precision: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Threshold Determination for Each Class\n",
    "# ---------------------------------------\n",
    "\n",
    "# Predict probabilities with the best model\n",
    "y_probs = best_xgb.predict_proba(X_test)\n",
    "\n",
    "# Define thresholds based on desired precision for each class\n",
    "thresholds_dict = {}\n",
    "for class_idx, class_label in enumerate(class_labels):\n",
    "    # Threshold for 95% Precision\n",
    "    threshold_95p_precision = find_threshold_multiclass(\n",
    "        y_test, y_probs, target_class=class_idx, target_metric='precision', target_value=0.95\n",
    "    )\n",
    "    thresholds_dict[class_label] = {\n",
    "        '95% Precision': threshold_95p_precision\n",
    "    }\n",
    "\n",
    "# Function to get predictions based on thresholds (One-vs-Rest)\n",
    "def get_multiclass_predictions(y_probas, thresholds_dict):\n",
    "    \"\"\"\n",
    "    Generate multi-class predictions based on per-class thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_probas: Predicted probabilities for each class.\n",
    "    - thresholds_dict: Dictionary containing thresholds for each class.\n",
    "    \n",
    "    Returns:\n",
    "    - Array of predicted class labels.\n",
    "    \"\"\"\n",
    "    y_pred = np.full(y_probas.shape[0], -1)  # Initialize with -1 (invalid class)\n",
    "    for class_idx, class_label in enumerate(class_labels):\n",
    "        threshold = thresholds_dict[class_label]['95% Precision']\n",
    "        y_pred[y_probas[:, class_idx] >= threshold] = class_idx\n",
    "    \n",
    "    # Assign the class with the highest probability if no threshold is met\n",
    "    y_pred[y_pred == -1] = np.argmax(y_probas[y_pred == -1], axis=1)\n",
    "    return y_pred\n",
    "\n",
    "# Generate predictions using the 95% Precision thresholds\n",
    "y_pred_threshold = get_multiclass_predictions(y_probs, thresholds_dict)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Confusion Matrices\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/XGB_multiclass'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Define the combined classes, including the 'KN' class\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "   # 'KN': ['KN_K17', 'KN_B19'],  # Commented out as per original code\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "# Since 'KN' is commented out, ensure it's handled accordingly\n",
    "# No further action needed as per original code\n",
    "\n",
    "# Define the strategies\n",
    "strategies = {\n",
    "    'Original': None,  # No sampling for the original distribution\n",
    "    'Undersampling': RandomUnderSampler(random_state=42),\n",
    "    'Oversampling (SMOTE)': SMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "# Adjusting the figure size and orientation of the labels for clarity\n",
    "fig, axs = plt.subplots(len(strategies), 2, figsize=(12, len(strategies) * 4), gridspec_kw={'width_ratios': [5, 7]})\n",
    "\n",
    "for i, (strategy_name, sampler) in enumerate(strategies.items()):\n",
    "    if strategy_name == 'Original':\n",
    "        X_train_mod, y_train_mod = X_train, y_train\n",
    "    else:\n",
    "        X_train_mod, y_train_mod = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Display counts after undersampling or oversampling\n",
    "    sampled_counts = pd.Series(y_train_mod).value_counts()\n",
    "    # Uncomment the following lines if you want to print sampled counts\n",
    "    # print(f\"\\n{strategy_name} Training Set Class Distribution\")\n",
    "    # print(sampled_counts)\n",
    "    \n",
    "    # Visualize class distribution with improved label orientation\n",
    "    plot_class_distribution(\n",
    "        y_train_mod, class_labels,\n",
    "        ax=axs[i, 0],\n",
    "        title=f'Training Set Class Distribution: {strategy_name}'\n",
    "    )\n",
    "    \n",
    "    # Train XGBoost classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=best_xgb.get_params()['n_estimators'],\n",
    "        learning_rate=best_xgb.get_params()['learning_rate'],\n",
    "        max_depth=best_xgb.get_params()['max_depth'],\n",
    "        min_child_weight=best_xgb.get_params()['min_child_weight'],\n",
    "        subsample=best_xgb.get_params()['subsample'],\n",
    "        colsample_bytree=best_xgb.get_params()['colsample_bytree'],\n",
    "        gamma=best_xgb.get_params()['gamma'],\n",
    "        scale_pos_weight=best_xgb.get_params()['scale_pos_weight'],\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(class_labels),\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_classifier.fit(X_train_mod, y_train_mod)\n",
    "    \n",
    "    # Prediction probabilities\n",
    "    y_probs_xgb = xgb_classifier.predict_proba(X_test)\n",
    "    \n",
    "    # Generate predictions using the 95% Precision thresholds\n",
    "    y_pred_xgb = get_multiclass_predictions(y_probs_xgb, thresholds_dict)\n",
    "    \n",
    "    # Plot normalized confusion matrix (Normalized by truth)\n",
    "    cm = confusion_matrix(y_test, y_pred_xgb, labels=np.arange(len(class_labels)))\n",
    "    plot_confusion_matrix_multiclass(cm, classes=class_labels, ax=axs[i, 1], title=f'XGB Confusion Matrix ({strategy_name})', normalize=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "confusion_matrix_path = os.path.join(save_directory, 'XGB_multiclass_confusion_matrices.png')\n",
    "plt.savefig(confusion_matrix_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()  # Close the figure to free memory\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Precision-Recall Curves\n",
    "# ---------------------------------------\n",
    "\n",
    "# Since we have already trained the best model, we can use it for plotting\n",
    "# However, for each strategy, the model might differ\n",
    "# To keep things consistent, we'll plot Precision-Recall curves for the 'Original' strategy\n",
    "\n",
    "# Re-train the best model on the 'Original' strategy for consistent plotting\n",
    "best_xgb_original = best_xgb\n",
    "best_xgb_original.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities with the best model\n",
    "y_probs_best = best_xgb_original.predict_proba(X_test)\n",
    "\n",
    "# Generate predictions using the 95% Precision thresholds\n",
    "y_pred_best = get_multiclass_predictions(y_probs_best, thresholds_dict)\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "pr_save_path = os.path.join(save_directory, 'XGB_multiclass_precision_recall.png')\n",
    "plot_precision_recall_curve_multiclass(y_test, y_probs_best, class_labels, pr_save_path, figsize=(8, 6))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Feature Importance\n",
    "# ---------------------------------------\n",
    "\n",
    "# Plot and save Feature Importance\n",
    "fi_save_path = os.path.join(save_directory, 'XGB_multiclass_feature_importance.png')\n",
    "plot_feature_importance_multiclass(best_xgb_original, feature_columns, fi_save_path, figsize=(10, 8))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Class Distribution\n",
    "# ---------------------------------------\n",
    "\n",
    "# Create subplots for class distributions\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training set class distribution for 'Original' strategy\n",
    "plot_class_distribution(\n",
    "    y_train, class_labels,\n",
    "    ax=axs[0],\n",
    "    title='Training Set Class Distribution'\n",
    ")\n",
    "\n",
    "# Test set class distribution\n",
    "plot_class_distribution(\n",
    "    y_test, class_labels,\n",
    "    ax=axs[1],\n",
    "    title='Test Set Class Distribution'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "class_distribution_path = os.path.join(save_directory, 'XGB_multiclass_class_distribution.png')\n",
    "plt.savefig(class_distribution_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Evaluation Metrics\n",
    "# ---------------------------------------\n",
    "\n",
    "print(\"XGBoost Multi-class Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=class_labels))\n",
    "\n",
    "# Compute ROC AUC for multi-class\n",
    "roc_auc = roc_auc_score(y_test, y_probs_best, multi_class='ovr')\n",
    "print(f\"ROC AUC (One-vs-Rest): {roc_auc:.4f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ---------------------------------------\n",
    "\n",
    "# Calculate permutation feature importance\n",
    "result = permutation_importance(\n",
    "    best_xgb_original, X_test, y_test, n_repeats=10, random_state=42, scoring='roc_auc_ovr'\n",
    ")\n",
    "\n",
    "# Create a Series for easy plotting\n",
    "perm_importance = pd.Series(result.importances_mean, index=feature_columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 permutation feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=perm_importance.values[:20], y=perm_importance.index[:20], palette='viridis')\n",
    "plt.title('Permutation Feature Importance (Top 20)', fontsize=12)\n",
    "plt.xlabel('Mean Decrease in ROC AUC', fontsize=10)\n",
    "plt.ylabel('Features', fontsize=10)\n",
    "plt.tight_layout()\n",
    "permutation_importance_path = os.path.join(save_directory, 'XGB_multiclass_permutation_importance.png')\n",
    "plt.savefig(permutation_importance_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a380d3-6588-4104-87be-4c90256d4d63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_recall_curve, classification_report, roc_auc_score,\n",
    "    make_scorer, precision_score, recall_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.stats import uniform, randint\n",
    "import os\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# ---------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def find_threshold_multiclass(y_true, y_probs, target_class, target_metric='precision', target_value=0.95):\n",
    "    \"\"\"\n",
    "    Find the threshold for desired precision or recall for a specific class.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels.\n",
    "    - y_probs: Predicted probabilities for each class.\n",
    "    - target_class: The class index for which to find the threshold.\n",
    "    - target_metric: 'precision' or 'recall'.\n",
    "    - target_value: Desired value for the target metric.\n",
    "    \n",
    "    Returns:\n",
    "    - Threshold value.\n",
    "    \"\"\"\n",
    "    y_true_bin = (y_true == target_class).astype(int)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true_bin, y_probs[:, target_class])\n",
    "    \n",
    "    if target_metric == 'precision':\n",
    "        idx = np.where(precision >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% precision for class {target_class}. Returning threshold=1.0\")\n",
    "            return 1.0\n",
    "        return thresholds[idx[0]]\n",
    "    elif target_metric == 'recall':\n",
    "        idx = np.where(recall >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% recall for class {target_class}. Returning threshold=0.0\")\n",
    "            return 0.0\n",
    "        return thresholds[idx[-1]]\n",
    "    else:\n",
    "        raise ValueError(\"target_metric must be 'precision' or 'recall'.\")\n",
    "\n",
    "def plot_confusion_matrix_multiclass(cm, classes, ax, title, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - cm: Confusion matrix.\n",
    "    - classes: List of class names.\n",
    "    - ax: Matplotlib Axes object.\n",
    "    - title: Title for the plot.\n",
    "    - normalize: Whether to normalize the confusion matrix.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        cm_normalized = cm\n",
    "\n",
    "    sns.heatmap(cm_normalized, annot=False, fmt='.2f', cmap='Blues', ax=ax,\n",
    "                xticklabels=classes, yticklabels=classes, cbar=False)\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if normalize:\n",
    "                percentage = cm_normalized[i, j] * 100\n",
    "                count = cm[i, j]\n",
    "                text = f'{percentage:.1f}%\\n({count})'\n",
    "            else:\n",
    "                text = f'{cm[i, j]}'\n",
    "            ax.text(j + 0.5, i + 0.5, text,\n",
    "                    ha='center', va='center',\n",
    "                    color='black', fontsize=8,\n",
    "                    bbox=dict(facecolor='white', edgecolor='white'))\n",
    "\n",
    "    ax.set_xlabel('Predicted Class', fontsize=10)\n",
    "    ax.set_ylabel('True Class', fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "def plot_precision_recall_curve_multiclass(y_true, y_probs, class_labels, save_path, figsize=(6, 4)):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for each class in a multi-class setting.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels.\n",
    "    - y_probs: Predicted probabilities for each class.\n",
    "    - class_labels: List of class names.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Figure size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        y_true_bin = (y_true == i).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_probs[:, i])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, lw=1.5, label=f'{class_label} (AUC = {pr_auc:.2f})')\n",
    "    \n",
    "    plt.xlabel('Recall', fontsize=10)\n",
    "    plt.ylabel('Precision', fontsize=10)\n",
    "    plt.title('Precision-Recall Curve for Multi-class', fontsize=12)\n",
    "    plt.legend(fontsize=8, loc='lower left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance_multiclass(model, feature_names, save_path, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot and save the feature importance for a multi-class XGBoost model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained XGBoost model.\n",
    "    - feature_names: List of feature names.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Figure size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.gca()\n",
    "    xgb.plot_importance(\n",
    "        model,\n",
    "        max_num_features=20,\n",
    "        importance_type='weight',\n",
    "        show_values=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title('Feature Importance', fontsize=12)\n",
    "    ax.set_xlabel('Importance (Weight)', fontsize=10)\n",
    "    ax.set_ylabel('Features', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_class_distribution(y, classes, ax, title):\n",
    "    \"\"\"\n",
    "    Plot the class distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Array-like of labels.\n",
    "    - classes: List of class names.\n",
    "    - ax: Matplotlib Axes object.\n",
    "    - title: Title for the plot.\n",
    "    \"\"\"\n",
    "    sns.countplot(y=y, ax=ax, palette='Set2', order=np.sort(np.unique(y)))\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel('Counts', fontsize=10)\n",
    "    ax.set_ylabel('Class', fontsize=10)\n",
    "    ax.set_yticklabels(classes[np.sort(np.unique(y))], rotation=45, fontsize=9)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data Preparation\n",
    "# ---------------------------------------\n",
    "\n",
    "# Ensure df0 is loaded\n",
    "if 'df0' not in globals():\n",
    "    raise ValueError(\"DataFrame 'df0' is not loaded. Please load your data into 'df0' before running the script.\")\n",
    "\n",
    "# Define combined classes\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'AGN': ['AGN'],\n",
    "    # 'KN': ['KN_K17', 'KN_B19'],  # Commented out as per original code\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "# Map the original classes to combined classes\n",
    "class_mapping = {original: combined for combined, originals in combined_classes.items() for original in originals}\n",
    "df0['Combined_Type'] = df0['Object_Type'].map(class_mapping)\n",
    "\n",
    "# Display initial counts\n",
    "initial_counts = df0['Combined_Type'].value_counts()\n",
    "print(\"Initial Class Distribution:\")\n",
    "print(initial_counts)\n",
    "\n",
    "# Select relevant columns for the model and exclude 'SNID', any truth info\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakFlux_GP', 'REDSHIFT_FINAL', 'Num_Peaks_GP']\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower()]  # Remove error estimates\n",
    "\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "rename_map = {\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time'\n",
    "}\n",
    "df0.rename(columns=rename_map, inplace=True)\n",
    "feature_columns = [rename_map.get(col, col) for col in feature_columns]\n",
    "\n",
    "# Define features and target\n",
    "X = df0[feature_columns].fillna(-999)  # Handling missing values by filling with -999\n",
    "y = df0['Combined_Type']\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardize features\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=feature_columns)  # Retain feature names\n",
    "\n",
    "# Split the data with a 1:1 train-test ratio and stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.5, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Display counts after train-test split\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "test_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "print(\"\\nTraining Set Class Distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index().rename(index=lambda x: class_labels[x]))\n",
    "\n",
    "print(\"\\nTest Set Class Distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index().rename(index=lambda x: class_labels[x]))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'learning_rate': uniform(0.01, 0.29),  # 0.01 to 0.3\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.5, 0.5),  # 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # 0.5 to 1.0\n",
    "    'gamma': uniform(0, 5),\n",
    "    'scale_pos_weight': uniform(1, 10)  # Useful for imbalanced classes\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier for multi-class\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(class_labels),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define a custom scorer that emphasizes macro recall at a certain precision threshold\n",
    "def macro_recall_at_precision_threshold(y_true, y_probas, precision_threshold=0.80):\n",
    "    \"\"\"\n",
    "    Calculate the macro recall at a specified precision threshold across all classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels.\n",
    "    - y_probas: Predicted probabilities for each class.\n",
    "    - precision_threshold: Desired precision threshold.\n",
    "    \n",
    "    Returns:\n",
    "    - Macro recall score.\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    for class_idx in range(len(class_labels)):\n",
    "        threshold = find_threshold_multiclass(y_true, y_probas, target_class=class_idx, target_metric='precision', target_value=precision_threshold)\n",
    "        y_pred = (y_probas[:, class_idx] >= threshold).astype(int)\n",
    "        recall = recall_score((y_true == class_idx).astype(int), y_pred, zero_division=0)\n",
    "        recalls.append(recall)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "# Create a scorer for RandomizedSearchCV\n",
    "def custom_scorer_multiclass(y_true, y_pred_probas):\n",
    "    return macro_recall_at_precision_threshold(y_true, y_pred_probas, precision_threshold=0.80)\n",
    "\n",
    "scorer = make_scorer(custom_scorer_multiclass, needs_proba=True)\n",
    "\n",
    "# Initialize RandomizedSearchCV with StratifiedKFold\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    scoring=scorer,\n",
    "    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best estimator\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "print(f\"\\nBest parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best macro recall at 80% precision: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Threshold Determination for Each Class\n",
    "# ---------------------------------------\n",
    "\n",
    "# Predict probabilities with the best model\n",
    "y_probs = best_xgb.predict_proba(X_test)\n",
    "\n",
    "# Define thresholds based on desired precision and recall for each class\n",
    "thresholds_dict = {}\n",
    "for class_idx, class_label in enumerate(class_labels):\n",
    "    # Threshold for 80% Precision\n",
    "    threshold_80p_precision = find_threshold_multiclass(\n",
    "        y_test, y_probs, target_class=class_idx, target_metric='precision', target_value=0.80\n",
    "    )\n",
    "    # Threshold for 95% Precision\n",
    "    threshold_95p_precision = find_threshold_multiclass(\n",
    "        y_test, y_probs, target_class=class_idx, target_metric='precision', target_value=0.95\n",
    "    )\n",
    "    # Threshold for 95% Recall\n",
    "    threshold_95p_recall = find_threshold_multiclass(\n",
    "        y_test, y_probs, target_class=class_idx, target_metric='recall', target_value=0.95\n",
    "    )\n",
    "    \n",
    "    thresholds_dict[class_label] = {\n",
    "        '80% Precision': threshold_80p_precision,\n",
    "        '95% Precision': threshold_95p_precision,\n",
    "        '95% Recall': threshold_95p_recall\n",
    "    }\n",
    "\n",
    "# Function to get predictions based on thresholds (One-vs-Rest)\n",
    "def get_multiclass_predictions(y_probs, thresholds_dict):\n",
    "    \"\"\"\n",
    "    Generate multi-class predictions based on per-class thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_probs: Predicted probabilities for each class.\n",
    "    - thresholds_dict: Dictionary containing thresholds for each class.\n",
    "    \n",
    "    Returns:\n",
    "    - Array of predicted class labels.\n",
    "    \"\"\"\n",
    "    y_pred = np.full(y_probs.shape[0], -1)  # Initialize with -1 (invalid class)\n",
    "    for idx, class_label in enumerate(class_labels):\n",
    "        thresholds = thresholds_dict[class_label]\n",
    "        # For simplicity, use one threshold (e.g., 80% Precision) or define rules to combine thresholds\n",
    "        # Here, we'll use 80% Precision thresholds for prediction\n",
    "        threshold = thresholds['80% Precision']\n",
    "        y_pred[y_probs[:, idx] >= threshold] = idx\n",
    "    \n",
    "    # Assign the class with the highest probability if no threshold is met\n",
    "    y_pred[y_pred == -1] = np.argmax(y_probs[y_pred == -1], axis=1)\n",
    "    return y_pred\n",
    "\n",
    "# Generate predictions using the 80% Precision thresholds\n",
    "y_pred_threshold = get_multiclass_predictions(y_probs, thresholds_dict)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Confusion Matrices\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/XGB_multiclass'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_threshold, labels=np.arange(len(class_labels)))\n",
    "\n",
    "# Create a single plot for confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix_multiclass(cm, classes=class_labels, ax=plt.gca(), title='XGBoost Confusion Matrix', normalize=True)\n",
    "plt.tight_layout()\n",
    "confusion_matrix_path = os.path.join(save_directory, 'XGB_multiclass_confusion_matrix.png')\n",
    "plt.savefig(confusion_matrix_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Precision-Recall Curves\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define save path for Precision-Recall curve\n",
    "save_path_pr = os.path.join(save_directory, 'XGB_multiclass_precision_recall.png')\n",
    "\n",
    "# Plot and save Precision-Recall curves\n",
    "plot_precision_recall_curve_multiclass(y_test, y_probs, class_labels, save_path_pr, figsize=(8, 6))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Feature Importance\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define save path for Feature Importance plot\n",
    "save_path_fi = os.path.join(save_directory, 'XGB_multiclass_feature_importance.png')\n",
    "\n",
    "# Plot and save Feature Importance\n",
    "plot_feature_importance_multiclass(best_xgb, feature_columns, save_path_fi, figsize=(10, 8))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Class Distribution\n",
    "# ---------------------------------------\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training set class distribution\n",
    "plot_class_distribution(\n",
    "    y_train, class_labels,\n",
    "    ax=axs[0],\n",
    "    title='Training Set Class Distribution'\n",
    ")\n",
    "\n",
    "# Test set class distribution\n",
    "plot_class_distribution(\n",
    "    y_test, class_labels,\n",
    "    ax=axs[1],\n",
    "    title='Test Set Class Distribution'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "class_distribution_path = os.path.join(save_directory, 'XGB_multiclass_class_distribution.png')\n",
    "plt.savefig(class_distribution_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Evaluation Metrics\n",
    "# ---------------------------------------\n",
    "\n",
    "print(\"XGBoost Multi-class Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_threshold, target_names=class_labels))\n",
    "\n",
    "# Compute ROC AUC for multi-class\n",
    "roc_auc = roc_auc_score(y_test, y_probs, multi_class='ovr')\n",
    "print(f\"ROC AUC (One-vs-Rest): {roc_auc:.4f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Permutation Feature Importance\n",
    "# ---------------------------------------\n",
    "\n",
    "result = permutation_importance(\n",
    "    best_xgb, X_test, y_test, n_repeats=10, random_state=42, scoring='roc_auc_ovr'\n",
    ")\n",
    "\n",
    "perm_importance = pd.Series(result.importances_mean, index=feature_columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=perm_importance.values[:20], y=perm_importance.index[:20], palette='viridis')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Mean Decrease in ROC AUC')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "permutation_importance_path = os.path.join(save_directory, 'XGB_multiclass_permutation_importance.png')\n",
    "plt.savefig(permutation_importance_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7993390-c953-4250-8752-9228eb44e38f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gain-based importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(best_xgb, importance_type='gain', max_num_features=20, show_values=False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cover-based importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(best_xgb, importance_type='cover', max_num_features=20, show_values=False)\n",
    "plt.title('Feature Importance (Cover)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a78de18-9c28-4679-ac12-868401038834",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the classes to exclude\n",
    "excluded_classes = ['KN_K17', 'KN_B19', 'SLSN-I_no_host', 'SNIIn-MOSFIT']\n",
    "\n",
    "# Filter out the excluded classes\n",
    "df0_filtered = df0[~df0['Object_Type'].isin(excluded_classes)]\n",
    "\n",
    "# Select relevant columns for the model and exclude 'SNID', any truth info\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakFlux_GP', 'REDSHIFT_FINAL']\n",
    "excluded_columns += [col for col in df0_filtered.columns if 'err' in col.lower()]  # remove error estimates\n",
    "feature_columns = [col for col in df0_filtered.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "X = df0_filtered[feature_columns].fillna(-999)  # Handling missing values by filling with -999\n",
    "y = df0_filtered['Object_Type']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardize features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "\n",
    "def plot_normalized_conf_matrix(y_true, y_pred, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', ax=ax, \n",
    "                xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "    ax.set_title(title)\n",
    "\n",
    "strategies = {\n",
    "    'Original': None,  # No sampling for the original distribution\n",
    "    'Undersampling': RandomUnderSampler(random_state=42),\n",
    "    'Oversampling (SMOTE)': SMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "# Adjusting the figure size and orientation of the labels for clarity\n",
    "fig, axs = plt.subplots(len(strategies), 2, figsize=(24, len(strategies) * 8), gridspec_kw={'width_ratios': [2, 3]})\n",
    "\n",
    "for i, (strategy_name, sampler) in enumerate(strategies.items()):\n",
    "    if strategy_name == 'Original':\n",
    "        X_train_mod, y_train_mod = X_train, y_train\n",
    "    else:\n",
    "        X_train_mod, y_train_mod = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Visualize class distribution with improved label orientation\n",
    "    sns.countplot(y=y_train_mod, ax=axs[i, 0], palette='Set2', order=y_train_mod.value_counts().index)\n",
    "    axs[i, 0].set_title(f'Class Distribution: {strategy_name}')\n",
    "    axs[i, 0].set_xlabel('Counts')\n",
    "    axs[i, 0].set_ylabel('Class')\n",
    "    axs[i, 0].tick_params(axis='y', rotation=45)  # Rotate labels to avoid overlap\n",
    "    \n",
    "    # Train Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf_classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "    # Train the Gradient Boosting classifier\n",
    "    bdt_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "    bdt_classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "    # Prediction \n",
    "    y_pred_rf = rf_classifier.predict(X_test)\n",
    "    y_pred_bdt = bdt_classifier.predict(X_test)\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plot_normalized_conf_matrix(y_test, y_pred_rf, axs[i, 1], f'RF: {strategy_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def plot_precision_recall_curve_multiclass(y_true, y_probas, class_labels):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        # Binarize the output for the current class\n",
    "        y_true_bin = (y_true == class_label).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_probas[:, i])\n",
    "        auc_score = auc(recall, precision) \n",
    "        plt.plot(recall, precision, lw=2, label=f'Class {class_label} (AUC = {auc_score:0.2f})')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Precision-Recall Curve for Multiple Classes\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Get the probabilities for each class\n",
    "y_probs_rf = rf_classifier.predict_proba(X_test)\n",
    "y_probs_bdt = bdt_classifier.predict_proba(X_test)\n",
    "\n",
    "# Class labels for the multi-class setup\n",
    "class_labels = rf_classifier.classes_\n",
    "\n",
    "# Plot the precision-recall curve for Random Forest\n",
    "plot_precision_recall_curve_multiclass(y_test, y_probs_rf, class_labels)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nGradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bdt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d1c69-fb71-4937-ae6d-b94a3ad1aefe",
   "metadata": {},
   "source": [
    "### precision recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3963d-eac3-40cf-8020-780bff28a9e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#FROM  tdescore (Stein), \n",
    "\n",
    "cls = 'TDE'\n",
    "probs_cls = y_probs_rf[:, class_labels.tolist().index(cls)]\n",
    "pr, recall, thresholds = precision_recall_curve(y_test == cls, probs_cls)\n",
    "\n",
    "index = np.arange(len(thresholds))\n",
    "\n",
    "mask = recall[1:] >= 0.96\n",
    "loose_index = max(index[mask])\n",
    "threshold_loose = thresholds[loose_index]\n",
    "print(f\"Loose threshold {threshold_loose:.2f}, Purity={100.*pr[loose_index]:.1f}%, Efficiency={100.*recall[loose_index]:.1f}%\")\n",
    "\n",
    "mask = pr[:-1] >= .95\n",
    "strict_index = min(index[mask])\n",
    "threshold_strict = thresholds[strict_index]\n",
    "print(f\"Strict Threshold {threshold_strict:.2f}, Purity={100.*pr[strict_index]:.1f}%, Efficiency={100.*recall[strict_index]:.1f}%\")\n",
    "\n",
    "mask = pr[:-1] >= 0.90\n",
    "balanced_index = min(index[mask])\n",
    "threshold_balanced = thresholds[balanced_index]\n",
    "print(f\"Balanced Threshold {threshold_balanced:.2f}, Purity={100.*pr[balanced_index]:.1f}%, Efficiency={100.*recall[balanced_index]:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f2d58-9513-4632-9e9f-01b9a96c93fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls = 'TDE'\n",
    "probs_cls = y_probs_rf[:, class_labels.tolist().index(cls)]\n",
    "\n",
    "# Calculate precision, recall, and thresholds for TDE\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test == cls, probs_cls)\n",
    "\n",
    "# Plot precision and recall as functions of the threshold\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(thresholds_pr, precision[:-1], label='Precision', color='blue')\n",
    "plt.plot(thresholds_pr, recall[:-1], label='Recall', color='orange')\n",
    "\n",
    "# Adding vertical lines for specific thresholds\n",
    "threshold_95_precision = thresholds_pr[np.where(precision[:-1] >= 0.95)[0][0]]\n",
    "#threshold_90_recall = thresholds_pr[np.where(recall[:-1] >= 0.90)[0][0]]\n",
    "#threshold_balanced = thresholds_pr[np.argmax(precision[:-1] >= 0.80)]\n",
    "\n",
    "plt.axvline(x=threshold_95_precision, color='black', linestyle='--', label='95% Precision Threshold')\n",
    "#plt.axvline(x=threshold_90_recall, color='black', linestyle='-', label='90% Recall Threshold')\n",
    "#plt.axvline(x=threshold_balanced, color='black', linestyle='-', label='Balanced Threshold (80% Precision)')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall as a function of threshold')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45d8c8-8dfb-4157-80d5-88bc65f9aaef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine all SN into one class and leave SLSN, TDE, AGN, and KN as separate classes\n",
    "combined_classes = {\n",
    "    'SNI': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19'],\n",
    "    'SNII': ['SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19'],\n",
    "    'CLAGN': ['CLAGN'],\n",
    "    'KN': ['KN_K17', 'KN_B19'],\n",
    "    'SLSN': ['SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "\n",
    "# Create a combined class column in df0\n",
    "df0['Combined_Class'] = df0['Object_Type']\n",
    "for combined_class, original_classes in combined_classes.items():\n",
    "    df0.loc[df0['Object_Type'].isin(original_classes), 'Combined_Class'] = combined_class\n",
    "\n",
    "# Select relevant columns for the model and exclude 'SNID', any truth info\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakFlux_GP', 'REDSHIFT_FINAL']\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower()]\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "X = df0[feature_columns].fillna(-999)\n",
    "y = df0['Combined_Class']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.7, random_state=42)\n",
    "\n",
    "# Balance the training set using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train Random Forest classifier on the balanced dataset\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_classifier.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the probabilities for each class (classifier scores)\n",
    "y_probs_rf = rf_classifier.predict_proba(X_test)\n",
    "class_labels = rf_classifier.classes_\n",
    "\n",
    "# Define a color palette\n",
    "palette = sns.color_palette(\"tab10\", len(class_labels))\n",
    "\n",
    "# Plot precision and recall as functions of the threshold for each class\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for idx, cls in enumerate(class_labels):\n",
    "    probs_cls = y_probs_rf[:, class_labels.tolist().index(cls)]\n",
    "    precision, recall, thresholds_pr = precision_recall_curve(y_test == cls, probs_cls)\n",
    "    \n",
    "    plt.plot(thresholds_pr, precision[:-1], label=f'Precision {cls}', color=palette[idx])\n",
    "    plt.plot(thresholds_pr, recall[:-1], label=f'Recall {cls}', color=palette[idx], linestyle='--')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall as a function of threshold for each class')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deac101-d8c8-4be8-932d-0f0c89a74eca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the combined classes\n",
    "combined_classes = {\n",
    "    'Non-TDE': ['SNIa-SALT3', 'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19', \n",
    "                'SNIIn-MOSFIT', 'SNII-NMF', 'SNII+HostXT_V19', 'SNIIb+HostXT_V19', \n",
    "                'CLAGN', 'KN_K17', 'KN_B19', \n",
    "                'SLSN-I+host', 'SLSN-I_no_host'],\n",
    "    'TDE': ['TDE']\n",
    "}\n",
    "\n",
    "# Map the original classes to combined classes\n",
    "class_mapping = {original: combined for combined, originals in combined_classes.items() for original in originals}\n",
    "df0['Combined_Class'] = df0['Object_Type'].map(class_mapping)\n",
    "\n",
    "# Select relevant columns for the model and exclude 'SNID', any truth info\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakFlux_GP', 'REDSHIFT_FINAL', 'Num_Peaks_GP']\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower()]  # remove error estimates\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "X = df0[feature_columns].fillna(-999)  # Handling missing values by filling with -999\n",
    "y = df0['Combined_Class']\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardize features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.7, random_state=42)\n",
    "\n",
    "# Balance the training set using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train XGBoost classifier on the balanced dataset\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "xgb_classifier.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the probabilities for each class (classifier scores)\n",
    "y_probs_xgb = xgb_classifier.predict_proba(X_test)\n",
    "class_labels = xgb_classifier.classes_\n",
    "\n",
    "# Plot precision and recall as functions of the threshold for TDE vs Non-TDE\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for idx, cls in enumerate(class_labels):\n",
    "    probs_cls = y_probs_xgb[:, class_labels.tolist().index(cls)]\n",
    "    precision, recall, thresholds_pr = precision_recall_curve(y_test == cls, probs_cls)\n",
    "    \n",
    "    plt.plot(thresholds_pr, precision[:-1], label=f'Precision {label_encoder.inverse_transform([cls])[0]}', color=sns.color_palette(\"tab10\")[idx])\n",
    "    plt.plot(thresholds_pr, recall[:-1], label=f'Recall {label_encoder.inverse_transform([cls])[0]}', color=sns.color_palette(\"tab10\")[idx], linestyle='--')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall as a function of threshold for TDE vs Non-TDE')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "def plot_normalized_conf_matrix(y_true, y_pred, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', ax=ax, \n",
    "                xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "plot_normalized_conf_matrix(y_test, y_pred_xgb, ax, 'XGB: TDE vs Non-TDE')\n",
    "plt.show()\n",
    "\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e4a26-dca0-4342-be06-99d41f265fb2",
   "metadata": {},
   "source": [
    "### Binary Classification: TDE vs Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26abb672-669a-4d58-832d-07e874a08a38",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f98fe-8ef8-44f8-b294-a424787adc58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_recall_curve, classification_report, roc_auc_score,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import os\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# ---------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def find_threshold(y_true, y_probs, target='precision', target_value=0.95):\n",
    "    \"\"\"\n",
    "    Find the threshold for desired precision or recall.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels.\n",
    "    - y_probs: Predicted probabilities for the positive class.\n",
    "    - target: 'precision' or 'recall'.\n",
    "    - target_value: Desired value for the target metric.\n",
    "\n",
    "    Returns:\n",
    "    - Threshold value.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    \n",
    "    if target == 'precision':\n",
    "        # Find indices where precision >= target_value\n",
    "        idx = np.where(precision >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% precision. Returning threshold=1.0\")\n",
    "            return 1.0  # Maximum threshold\n",
    "        # Choose the threshold corresponding to the first occurrence\n",
    "        return thresholds[idx[0]]\n",
    "    \n",
    "    elif target == 'recall':\n",
    "        # Find indices where recall >= target_value\n",
    "        idx = np.where(recall >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% recall. Returning threshold=0.0\")\n",
    "            return 0.0  # Minimum threshold\n",
    "        # Choose the threshold corresponding to the last occurrence\n",
    "        return thresholds[idx[-1]]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Target must be 'precision' or 'recall'.\")\n",
    "\n",
    "def plot_confusion_matrix(cm, ax, title, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with percentages and counts.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: Confusion matrix.\n",
    "    - ax: Matplotlib Axes object.\n",
    "    - title: Title for the subplot.\n",
    "    - normalize: Whether to normalize the confusion matrix per true label.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        cm_normalized = cm\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm_normalized, annot=False, fmt='.2f', cmap='Blues', ax=ax,\n",
    "        xticklabels=['Non-TDE', 'TDE'], yticklabels=['Non-TDE', 'TDE'], cbar=False\n",
    "    )\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if normalize:\n",
    "                percentage = cm_normalized[i, j] * 100\n",
    "                count = cm[i, j]\n",
    "                text = f'{percentage:.1f}%\\n({count})'\n",
    "            else:\n",
    "                text = f'{cm[i, j]}'\n",
    "            ax.text(\n",
    "                j + 0.5, i + 0.5, text, ha='center', va='center',\n",
    "                color='black', fontsize=8, bbox=dict(facecolor='white', edgecolor='white')\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_probs, thresholds_to_mark, save_path, figsize=(6, 4)):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall as functions of the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels.\n",
    "    - y_probs: Predicted probabilities for the positive class.\n",
    "    - thresholds_to_mark: Dictionary with labels and threshold values to mark.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Tuple specifying the figure size.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(thresholds, precision[:-1], label='Precision', color='blue')\n",
    "    plt.plot(thresholds, recall[:-1], label='Recall', color='green')\n",
    "    \n",
    "    for label, thresh in thresholds_to_mark.items():\n",
    "        if '95% Precision' in label:\n",
    "            color = 'red'  # Change color for '95% Precision' threshold\n",
    "        else:\n",
    "            color = 'blue'  # Default color for other thresholds\n",
    "        plt.axvline(x=thresh, linestyle='--', color=color, label=f'{label} ({thresh:.2f})')\n",
    "    \n",
    "    plt.xlabel('Threshold', fontsize=10)\n",
    "    plt.ylabel('Score', fontsize=10)\n",
    "    plt.title('Precision and Recall vs. Threshold', fontsize=12)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, save_path, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot and save the feature importance from the Random Forest model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained Random Forest model.\n",
    "    - feature_names: List of feature names.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Tuple specifying the figure size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_n = 20\n",
    "    indices = indices[:top_n]\n",
    "    sorted_importances = importances[indices]\n",
    "    sorted_features = [feature_names[i] for i in indices]\n",
    "    \n",
    "    sns.barplot(x=sorted_importances, y=sorted_features, palette='viridis')\n",
    "    plt.title('Feature Importance', fontsize=12)\n",
    "    plt.xlabel('Importance', fontsize=10)\n",
    "    plt.ylabel('Features', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_permutation_importance_rf(model, X_test, y_test, feature_names, save_path, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot and save permutation feature importance from the Random Forest model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained Random Forest model.\n",
    "    - X_test: Test feature set.\n",
    "    - y_test: Test labels.\n",
    "    - feature_names: List of feature names.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Tuple specifying the figure size.\n",
    "    \"\"\"\n",
    "    result = permutation_importance(\n",
    "        model, X_test, y_test, n_repeats=10, random_state=42, scoring='roc_auc'\n",
    "    )\n",
    "    perm_importance = pd.Series(result.importances_mean, index=feature_names).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(x=perm_importance.values[:20], y=perm_importance.index[:20], palette='viridis')\n",
    "    plt.title('Permutation Feature Importance')\n",
    "    plt.xlabel('Mean Decrease in ROC AUC')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data Preparation\n",
    "# ---------------------------------------\n",
    "\n",
    "# Ensure df0 is loaded\n",
    "if 'df0' not in locals():\n",
    "    raise ValueError(\"DataFrame 'df0' is not loaded. Please load your data into 'df0' before running the script.\")\n",
    "\n",
    "# Create a simplified object type column\n",
    "df0['Simple_Object_Type'] = df0['Object_Type'].apply(lambda x: 'TDE' if x == 'TDE' else 'Other')\n",
    "\n",
    "# Select numeric columns for the model and exclude specified columns\n",
    "excluded_columns = [\n",
    "    'SNID', \n",
    "    'PeakMag',\n",
    "    'PeakMagErr',\n",
    "    'REDSHIFT_FINAL',\n",
    "    'REDSHIFT_FINAL_ERR',\n",
    "    # Add other excluded columns if needed\n",
    "]\n",
    "\n",
    "# Exclude columns containing 'err', 'flux', or 'mjd'\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower() or 'flux' in col.lower() or 'mjd' in col.lower()]\n",
    "\n",
    "# Select feature columns\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "rename_map = {\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time'\n",
    "}\n",
    "df0.rename(columns=rename_map, inplace=True)\n",
    "feature_columns = [rename_map.get(col, col) for col in feature_columns]\n",
    "\n",
    "# Define features and target\n",
    "X = df0[feature_columns].fillna(-999)  # Handle missing values\n",
    "y = df0['Simple_Object_Type'].apply(lambda x: 1 if x == 'TDE' else 0)  # Binary encoding\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert scaled features back to DataFrame with feature names\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=feature_columns)\n",
    "\n",
    "# Split the dataset with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.5, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# ---------------------------------------\n",
    "# Hyperparameter Tuning for Random Forest\n",
    "# ---------------------------------------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(5, 50),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Define a custom scorer that emphasizes recall at a precision threshold\n",
    "def recall_at_precision_threshold(y_true, y_probs, precision_threshold=0.80):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    # Find the threshold where precision >= precision_threshold\n",
    "    idx = np.where(precision >= precision_threshold)[0]\n",
    "    if len(idx) == 0:\n",
    "        return 0\n",
    "    # Return the maximum recall at or above the precision threshold\n",
    "    return recall[idx].max()\n",
    "\n",
    "# Create a scorer for RandomizedSearchCV\n",
    "def custom_scorer_rf(y_true, y_pred_proba):\n",
    "    return recall_at_precision_threshold(y_true, y_pred_proba, precision_threshold=0.80)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scorer_rf = make_scorer(custom_scorer_rf, needs_proba=True)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=100,\n",
    "    scoring=scorer_rf,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best estimator\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "\n",
    "print(f\"Best parameters found: {random_search_rf.best_params_}\")\n",
    "print(f\"Best recall at 80% precision: {random_search_rf.best_score_:.4f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Threshold Determination\n",
    "# ---------------------------------------\n",
    "\n",
    "# Predict probabilities with the best model\n",
    "y_probs_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Define thresholds based on desired precision and recall\n",
    "threshold_80p_precision_rf = find_threshold(y_test, y_probs_rf, target='precision', target_value=0.80)  # 80% Precision\n",
    "threshold_95p_precision_rf = find_threshold(y_test, y_probs_rf, target='precision', target_value=0.95)  # 95% Precision\n",
    "threshold_95p_recall_rf = find_threshold(y_test, y_probs_rf, target='recall', target_value=0.95)        # 95% Recall\n",
    "\n",
    "thresholds_dict_rf = {\n",
    "    '80% Precision': threshold_80p_precision_rf,\n",
    "    '95% Precision': threshold_95p_precision_rf,\n",
    "    '95% Recall': threshold_95p_recall_rf\n",
    "}\n",
    "\n",
    "# Function to get binary predictions based on a threshold\n",
    "def get_predictions_rf(y_probs, threshold):\n",
    "    return (y_probs >= threshold).astype(int)\n",
    "\n",
    "# Generate predictions for the desired thresholds\n",
    "predictions_rf = {\n",
    "    label: get_predictions_rf(y_probs_rf, thresh)\n",
    "    for label, thresh in thresholds_dict_rf.items()\n",
    "}\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Confusion Matrices\n",
    "# ---------------------------------------\n",
    "\n",
    "# Titles for classifiers and thresholds\n",
    "classifier_titles_rf = [\n",
    "    'Random Forest - 80% Precision',\n",
    "    'Random Forest - 95% Precision',\n",
    "    'Random Forest - 95% Recall'\n",
    "]\n",
    "\n",
    "# Combine predictions into a list maintaining the order\n",
    "all_predictions_rf = [\n",
    "    predictions_rf['80% Precision'],\n",
    "    predictions_rf['95% Precision'],\n",
    "    predictions_rf['95% Recall']\n",
    "]\n",
    "\n",
    "# Define the save directory\n",
    "save_directory_rf = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/RandomForest'\n",
    "os.makedirs(save_directory_rf, exist_ok=True)\n",
    "\n",
    "# Prepare subplots with wider horizontal layout\n",
    "fig_rf, axs_rf = plt.subplots(1, 3, figsize=(18, 6))  # 1 row, 3 columns with a total size of 18x6 inches\n",
    "\n",
    "# Plot confusion matrices\n",
    "for i in range(3):\n",
    "    cm_rf = confusion_matrix(y_test, all_predictions_rf[i], labels=[0, 1])\n",
    "    title_rf = classifier_titles_rf[i]\n",
    "    plot_confusion_matrix(cm_rf, axs_rf[i], title_rf, normalize=True)\n",
    "\n",
    "# Adjust layout, save, and display the confusion matrices plot\n",
    "plt.tight_layout()\n",
    "confusion_matrix_path_rf = os.path.join(save_directory_rf, 'RandomForest_confusion_matrices.png')\n",
    "plt.savefig(confusion_matrix_path_rf, dpi=300)\n",
    "plt.show()\n",
    "plt.close()  # Close the figure to free memory\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Precision and Recall Curves\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define thresholds to mark\n",
    "thresholds_to_mark_rf = {\n",
    "    '80% Precision': threshold_80p_precision_rf,\n",
    "    '95% Precision': threshold_95p_precision_rf\n",
    "}\n",
    "\n",
    "# Define save path for Precision-Recall curve\n",
    "save_path_pr_rf = os.path.join(save_directory_rf, 'RandomForest_precision_recall.png')\n",
    "\n",
    "# Define desired figure size for Precision-Recall curve\n",
    "pr_figsize_rf = (6, 4)  # Adjust as needed\n",
    "\n",
    "# Plot and save Precision-Recall curve\n",
    "plot_precision_recall_curve(y_test, y_probs_rf, thresholds_to_mark_rf, save_path_pr_rf, figsize=pr_figsize_rf)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Feature Importance\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define save path for Feature Importance plot\n",
    "save_path_fi_rf = os.path.join(save_directory_rf, 'RandomForest_feature_importance.png')\n",
    "\n",
    "# Define desired figure size for Feature Importance plot\n",
    "fi_figsize_rf = (10, 8)  # Adjust as needed\n",
    "\n",
    "# Plot and save Feature Importance\n",
    "plot_feature_importance(best_rf, feature_columns, save_path_fi_rf, figsize=fi_figsize_rf)\n",
    "\n",
    "# Optionally, plot Permutation Feature Importance\n",
    "save_path_perm_fi_rf = os.path.join(save_directory_rf, 'RandomForest_permutation_importance.png')\n",
    "plot_permutation_importance_rf(best_rf, X_test, y_test, feature_columns, save_path_perm_fi_rf, figsize=(10, 8))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Evaluation Metrics\n",
    "# ---------------------------------------\n",
    "\n",
    "def evaluate_model_rf(y_true, y_probs, predictions, classifier_name):\n",
    "    print(f\"\\n{classifier_name} Classification Report:\")\n",
    "    print(classification_report(y_true, predictions, target_names=['Non-TDE', 'TDE']))\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_true, y_probs):.4f}\")\n",
    "\n",
    "# Evaluate the Classifier at each threshold\n",
    "for label in thresholds_dict_rf.keys():\n",
    "    classifier_name_rf = f'Random Forest - {label}'\n",
    "    evaluate_model_rf(y_test, y_probs_rf, predictions_rf[label], classifier_name_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564f49d-7c10-48a7-b9ad-00da71b64ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare subplots with wider horizontal layout\n",
    "fig_rf, axs_rf = plt.subplots(1, 3, figsize=(8, 3))  # 1 row, 3 columns with a total size of 18x6 inches\n",
    "\n",
    "# Plot confusion matrices\n",
    "for i in range(3):\n",
    "    cm_rf = confusion_matrix(y_test, all_predictions_rf[i], labels=[0, 1])\n",
    "    title_rf = classifier_titles_rf[i]\n",
    "    plot_confusion_matrix(cm_rf, axs_rf[i], title_rf, normalize=True)\n",
    "\n",
    "# Adjust layout, save, and display the confusion matrices plot\n",
    "plt.tight_layout()\n",
    "confusion_matrix_path_rf = os.path.join(save_directory_rf, 'RandomForest_confusion_matrices.png')\n",
    "plt.savefig(confusion_matrix_path_rf, dpi=300)\n",
    "plt.show()\n",
    "plt.close()  # Close the figure to free memory\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Precision and Recall Curves\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define thresholds to mark\n",
    "thresholds_to_mark_rf = {\n",
    "    '80% Precision': threshold_80p_precision_rf,\n",
    "    '95% Precision': threshold_95p_precision_rf\n",
    "}\n",
    "\n",
    "# Define save path for Precision-Recall curve\n",
    "save_path_pr_rf = os.path.join(save_directory_rf, 'RandomForest_precision_recall.png')\n",
    "\n",
    "# Define desired figure size for Precision-Recall curve\n",
    "pr_figsize_rf = (6, 4)  # Adjust as needed\n",
    "\n",
    "# Plot and save Precision-Recall curve\n",
    "plot_precision_recall_curve(y_test, y_probs_rf, thresholds_to_mark_rf, save_path_pr_rf, figsize=pr_figsize_rf)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Feature Importance\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define save path for Feature Importance plot\n",
    "save_path_fi_rf = os.path.join(save_directory_rf, 'RandomForest_feature_importance.png')\n",
    "\n",
    "# Define desired figure size for Feature Importance plot\n",
    "fi_figsize_rf = (6, 3)  # Adjust as needed\n",
    "\n",
    "# Plot and save Feature Importance\n",
    "plot_feature_importance(best_rf, feature_columns, save_path_fi_rf, figsize=fi_figsize_rf)\n",
    "\n",
    "# Optionally, plot Permutation Feature Importance\n",
    "save_path_perm_fi_rf = os.path.join(save_directory_rf, 'RandomForest_permutation_importance.png')\n",
    "plot_permutation_importance_rf(best_rf, X_test, y_test, feature_columns, save_path_perm_fi_rf, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cad680-66d4-4adb-9f87-dae7af122f0a",
   "metadata": {},
   "source": [
    "#### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0299792-2665-4645-adc0-5f7b8b7255d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_recall_curve, classification_report, roc_auc_score,\n",
    "    make_scorer\n",
    ")\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# ---------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def find_threshold(y_true, y_probs, target='precision', target_value=0.95):\n",
    "    \"\"\"\n",
    "    Find the threshold for desired precision or recall.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels.\n",
    "    - y_probs: Predicted probabilities for the positive class.\n",
    "    - target: 'precision' or 'recall'.\n",
    "    - target_value: Desired value for the target metric.\n",
    "\n",
    "    Returns:\n",
    "    - Threshold value.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "\n",
    "    if target == 'precision':\n",
    "        # Find indices where precision >= target_value\n",
    "        idx = np.where(precision >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% precision. Returning threshold=1.0\")\n",
    "            return 1.0  # Maximum threshold\n",
    "        # Choose the threshold corresponding to the first occurrence\n",
    "        return thresholds[idx[0]]\n",
    "\n",
    "    elif target == 'recall':\n",
    "        # Find indices where recall >= target_value\n",
    "        idx = np.where(recall >= target_value)[0]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"No threshold found to achieve {target_value*100}% recall. Returning threshold=0.0\")\n",
    "            return 0.0  # Minimum threshold\n",
    "        # Choose the threshold corresponding to the last occurrence\n",
    "        return thresholds[idx[-1]]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Target must be 'precision' or 'recall'.\")\n",
    "\n",
    "def plot_confusion_matrix(cm, ax, title, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with percentages and counts.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: Confusion matrix.\n",
    "    - ax: Matplotlib Axes object.\n",
    "    - title: Title for the subplot.\n",
    "    - normalize: Whether to normalize the confusion matrix per true label.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        cm_normalized = cm\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm_normalized, annot=False, fmt='.2f', cmap='Blues', ax=ax,\n",
    "        xticklabels=['Non-TDE', 'TDE'], yticklabels=['Non-TDE', 'TDE'], cbar=False\n",
    "    )\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if normalize:\n",
    "                percentage = cm_normalized[i, j] * 100\n",
    "                count = cm[i, j]\n",
    "                text = f'{percentage:.1f}%\\n({count})'\n",
    "            else:\n",
    "                text = f'{cm[i, j]}'\n",
    "            ax.text(\n",
    "                j + 0.5, i + 0.5, text, ha='center', va='center',\n",
    "                color='black', fontsize=8, bbox=dict(facecolor='white', edgecolor='white')\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_probs, thresholds_to_mark, save_path, figsize=(6, 4)):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall as functions of the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels.\n",
    "    - y_probs: Predicted probabilities for the positive class.\n",
    "    - thresholds_to_mark: Dictionary with labels and threshold values to mark.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Tuple specifying the figure size.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(thresholds, precision[:-1], label='Precision', color='blue')\n",
    "    plt.plot(thresholds, recall[:-1], label='Recall', color='green')\n",
    "\n",
    "    for label, thresh in thresholds_to_mark.items():\n",
    "        if '95% Precision' in label:\n",
    "            color = 'red'  # Change color for '95% Precision' threshold\n",
    "        else:\n",
    "            color = 'blue'  # Default color for other thresholds\n",
    "        plt.axvline(x=thresh, linestyle='--', color=color, label=f'{label} ({thresh:.2f})')\n",
    "\n",
    "    plt.xlabel('Threshold', fontsize=10)\n",
    "    plt.ylabel('Score', fontsize=10)\n",
    "    plt.title('Precision and Recall vs. Threshold', fontsize=12)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, save_path, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot and save the feature importance from the XGBoost model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained XGBoost model.\n",
    "    - feature_names: List of feature names.\n",
    "    - save_path: Path to save the plot.\n",
    "    - figsize: Tuple specifying the figure size.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.gca()\n",
    "    xgb.plot_importance(\n",
    "        model,\n",
    "        max_num_features=20,\n",
    "        importance_type='gain',\n",
    "        show_values=False,\n",
    "        ax=ax\n",
    "    )\n",
    "  #  plt.title('Feature Importance', fontsize=12)\n",
    "    plt.xlabel('Importance (gain)', fontsize=10)\n",
    "    plt.ylabel('Features', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data Preparation\n",
    "# ---------------------------------------\n",
    "\n",
    "# Ensure df0 is loaded\n",
    "if 'df0' not in locals():\n",
    "    raise ValueError(\"DataFrame 'df0' is not loaded. Please load your data into 'df0' before running the script.\")\n",
    "\n",
    "# Create a simplified object type column\n",
    "df0['Simple_Object_Type'] = df0['Object_Type'].apply(lambda x: 'TDE' if x == 'TDE' else 'Other')\n",
    "\n",
    "# Select numeric columns for the model and exclude specified columns\n",
    "excluded_columns = [\n",
    "    'SNID', \n",
    "    'PeakMag',\n",
    "    'PeakMagErr',\n",
    "    'REDSHIFT_FINAL',\n",
    "    'REDSHIFT_FINAL_ERR',\n",
    "    # Add other excluded columns if needed\n",
    "]\n",
    "\n",
    "# Exclude columns containing 'err', 'flux', or 'mjd'\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower() or 'flux' in col.lower() or 'mjd' in col.lower()]\n",
    "\n",
    "# Select feature columns\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "# Renaming columns for clarity\n",
    "rename_map = {\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time'\n",
    "}\n",
    "df0.rename(columns=rename_map, inplace=True)\n",
    "feature_columns = [rename_map.get(col, col) for col in feature_columns]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature Selection\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the list of selected features\n",
    "# Modify this list based on your requirements\n",
    "# Example: Selecting GP hyperparameters, Rise time, Fade time, and Mean Color Post Peak\n",
    "selected_features = [\n",
    "    'Amplitude',  \n",
    "    'LengthScale_Time',\n",
    "    'LengthScale_Wavelength',\n",
    "    'Mean Color Post Peak (g-r)',\n",
    "    'Mean Color Post Peak (r-i)',\n",
    "    'Mean Color Pre Peak (g-r)',\n",
    "    'Mean Color Pre Peak (r-i)',\n",
    "    'Slope Pre Peak (g-r)',\n",
    "    'Slope Post Peak (g-r)',\n",
    "    'Slope Pre Peak (r-i)',\n",
    "    'Slope Post Peak (r-i)',\n",
    "    'Rise time',\n",
    "    'Fade time']\n",
    "\n",
    "# Validate selected features\n",
    "missing_features = set(selected_features) - set(feature_columns)\n",
    "if missing_features:\n",
    "    raise ValueError(f\"The following selected features are not present in the feature columns: {missing_features}\")\n",
    "\n",
    "# Update feature_columns to include only selected features\n",
    "feature_columns_selected = selected_features\n",
    "\n",
    "# Define features and target using selected features\n",
    "X = df0[feature_columns_selected].fillna(-999)  # Handle missing values\n",
    "y = df0['Simple_Object_Type'].apply(lambda x: 1 if x == 'TDE' else 0)  # Binary encoding\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert scaled features back to DataFrame with selected feature names\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=feature_columns_selected)\n",
    "\n",
    "# Split the dataset with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.5, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# ---------------------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 8),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'gamma': uniform(0, 5),\n",
    "    'scale_pos_weight': uniform(1, 10)  # Useful for imbalanced classes\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define a custom scorer that emphasizes recall at a specified precision threshold\n",
    "def recall_at_precision_threshold(y_true, y_pred_proba, precision_threshold=0.80):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    # Find indices where precision >= precision_threshold\n",
    "    idx = np.where(precision >= precision_threshold)[0]\n",
    "    if len(idx) == 0:\n",
    "        return 0\n",
    "    # Return the maximum recall at or above the precision threshold\n",
    "    return recall[idx].max()\n",
    "\n",
    "# Create a scorer for RandomizedSearchCV\n",
    "def custom_scorer(y_true, y_pred_proba):\n",
    "    return recall_at_precision_threshold(y_true, y_pred_proba, precision_threshold=0.80)\n",
    "\n",
    "# Register the custom scorer\n",
    "scorer = make_scorer(custom_scorer, needs_proba=True, greater_is_better=True)\n",
    "\n",
    "# Initialize StratifiedKFold for cross-validation\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    scoring=scorer,\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best estimator\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best recall at 80% precision: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Threshold Determination\n",
    "# ---------------------------------------\n",
    "\n",
    "# Predict probabilities with the best model\n",
    "y_probs = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Define thresholds based on desired precision and recall\n",
    "threshold_80p_precision = find_threshold(y_test, y_probs, target='precision', target_value=0.80)  # 80% Precision\n",
    "threshold_95p_precision = find_threshold(y_test, y_probs, target='precision', target_value=0.95)  # 95% Precision\n",
    "threshold_95p_recall = find_threshold(y_test, y_probs, target='recall', target_value=0.95)        # 95% Recall\n",
    "\n",
    "thresholds_dict = {\n",
    "    '80% Precision': threshold_80p_precision,\n",
    "    '95% Precision': threshold_95p_precision,\n",
    "    '95% Recall': threshold_95p_recall\n",
    "}\n",
    "\n",
    "# Function to get binary predictions based on a threshold\n",
    "def get_predictions(y_probs, threshold):\n",
    "    return (y_probs >= threshold).astype(int)\n",
    "\n",
    "# Generate predictions for the desired thresholds\n",
    "predictions = {\n",
    "    label: get_predictions(y_probs, thresh)\n",
    "    for label, thresh in thresholds_dict.items()\n",
    "}\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Confusion Matrices\n",
    "# ---------------------------------------\n",
    "\n",
    "# Titles for classifiers and thresholds\n",
    "classifier_titles = [\n",
    "    'XGB - 80% Precision',\n",
    "    'XGB - 95% Precision',\n",
    "    'XGB - 95% Recall'\n",
    "]\n",
    "\n",
    "# Combine predictions into a list maintaining the order\n",
    "all_predictions = [\n",
    "    predictions['80% Precision'],\n",
    "    predictions['95% Precision'],\n",
    "    predictions['95% Recall']\n",
    "]\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/XGB'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Prepare subplots with wider horizontal layout\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))  # 1 row, 3 columns with a total size of 18x6 inches\n",
    "\n",
    "# Plot confusion matrices\n",
    "for i in range(3):\n",
    "    cm = confusion_matrix(y_test, all_predictions[i], labels=[0, 1])\n",
    "    title = classifier_titles[i]\n",
    "    plot_confusion_matrix(cm, axs[i], title, normalize=True)\n",
    "\n",
    "# Adjust layout, save, and display the confusion matrices plot\n",
    "plt.tight_layout()\n",
    "confusion_matrix_path = os.path.join(save_directory, 'XGB_confusion_matrices.png')\n",
    "plt.savefig(confusion_matrix_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()  # Close the figure to free memory\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Precision and Recall Curves\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define thresholds to mark\n",
    "thresholds_to_mark = {\n",
    "    '80% Precision': threshold_80p_precision,\n",
    "    '95% Precision': threshold_95p_precision\n",
    "}\n",
    "\n",
    "# Define save path for Precision-Recall curve\n",
    "save_path_pr = os.path.join(save_directory, 'XGB_precision_recall.png')\n",
    "\n",
    "# Define desired figure size for Precision-Recall curve\n",
    "pr_figsize = (6, 4)  # Adjust as needed\n",
    "\n",
    "# Plot and save Precision-Recall curve\n",
    "plot_precision_recall_curve(y_test, y_probs, thresholds_to_mark, save_path_pr, figsize=pr_figsize)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plotting Feature Importance\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define save path for Feature Importance plot\n",
    "save_path_fi = os.path.join(save_directory, 'XGB_feature_importance.png')\n",
    "\n",
    "# Define desired figure size for Feature Importance plot\n",
    "fi_figsize = (6, 3)  # Adjust as needed\n",
    "\n",
    "# Plot and save Feature Importance\n",
    "plot_feature_importance(best_xgb, feature_columns_selected, save_path_fi, figsize=fi_figsize)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Evaluation Metrics\n",
    "# ---------------------------------------\n",
    "\n",
    "def evaluate_model(y_true, y_probs, predictions, classifier_name):\n",
    "    print(f\"\\n{classifier_name} Classification Report:\")\n",
    "    print(classification_report(y_true, predictions, target_names=['Non-TDE', 'TDE']))\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_true, y_probs):.4f}\")\n",
    "\n",
    "# Evaluate the Classifier at each threshold\n",
    "for label in thresholds_dict.keys():\n",
    "    classifier_name = f'XGB - {label}'\n",
    "    evaluate_model(y_test, y_probs, predictions[label], classifier_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd88e2-307c-41e1-83f7-8fe471305753",
   "metadata": {},
   "source": [
    "### Using actual Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f037e6-0eea-4413-9280-f2fa854ce6f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# File paths\n",
    "TRAIN_DATA_PATH = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/ELAsTiCC2_snr5.csv'\n",
    "TEST_DATA_PATH = '/home/bhardwaj/notebooksLSST/ELAsTiCC2_processed/hdf5_files/ELAsTiCC2_Test_cleaned.csv'\n",
    "\n",
    "# Load the training data\n",
    "print(\"Loading training data...\")\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "print(\"Training data loaded successfully.\\n\")\n",
    "\n",
    "# Load the test data\n",
    "print(\"Loading test data...\")\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "print(\"Test data loaded successfully.\\n\")\n",
    "\n",
    "# Function to display column names\n",
    "def display_columns(df, df_name):\n",
    "    print(f\"Columns in {df_name}:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Display columns before renaming\n",
    "display_columns(df_train, \"Training Data (Before Renaming)\")\n",
    "display_columns(df_test, \"Test Data (Before Renaming)\")\n",
    "\n",
    "# Define rename map to handle both training and test data\n",
    "rename_map = {\n",
    "    # Training data mappings\n",
    "    'Mean_Color_Pre_Peak': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak': 'Mean Color Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak': 'Slope Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_RI': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_RI': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_RI': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_RI': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time',\n",
    "    'Amplitude': 'Amplitude',\n",
    "    'LengthScale_Time': 'LengthScale Time',\n",
    "    'LengthScale_Wavelength': 'LengthScale Wavelength',\n",
    "    'TruePeakMJD': 'True Peak MJD',\n",
    "    'peak_time_MJD': 'Peak Time MJD',\n",
    "    \n",
    "    # Test data mappings\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "}\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(df, rename_map, feature_columns, excluded_columns, df_name=\"\"):\n",
    "    # Create 'Simple_Object_Type' label\n",
    "    if 'Object_Type' in df.columns:\n",
    "        df['Simple_Object_Type'] = df['Object_Type'].apply(lambda x: 'TDE' if x == 'TDE' else 'Other')\n",
    "    else:\n",
    "        print(f\"'Object_Type' column not found in {df_name}. Ensure the test data contains this column.\")\n",
    "        # If Object_Type is not present, handle accordingly\n",
    "        df['Simple_Object_Type'] = np.nan  # Placeholder\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Update feature columns based on renaming\n",
    "    renamed_feature_columns = [rename_map.get(col, col) for col in feature_columns]\n",
    "\n",
    "    # Identify which renamed_feature_columns are present in df\n",
    "    available_features = [col for col in renamed_feature_columns if col in df.columns]\n",
    "    missing_features = [col for col in renamed_feature_columns if col not in df.columns]\n",
    "\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following expected columns are missing in {df_name}:\")\n",
    "        for col in missing_features:\n",
    "            print(f\"- {col}\")\n",
    "        print(\"These columns will be excluded from the feature set.\\n\")\n",
    "\n",
    "    # Select features that are available\n",
    "    X = df[available_features].fillna(-999)  # Handle missing values\n",
    "    y = df['Simple_Object_Type'].apply(lambda x: 1 if x == 'TDE' else 0)  # Binary encoding\n",
    "\n",
    "    return X, y, available_features, missing_features\n",
    "\n",
    "# Define columns to exclude (ignore 'err', 'flux', 'mjd', 'redshift')\n",
    "excluded_columns = [\n",
    "    'SNID', \n",
    "    'REDSHIFT_FINAL',\n",
    "    'REDSHIFT_FINAL_ERR',\n",
    "    'Error'  # From Test Data\n",
    "]\n",
    "\n",
    "# Exclude columns containing 'err', 'flux', or 'mjd' from training and test data\n",
    "excluded_columns += [col for col in df_train.columns if 'err' in col.lower() or 'flux' in col.lower() or 'mjd' in col.lower()]\n",
    "excluded_columns += [col for col in df_test.columns if 'err' in col.lower() or 'flux' in col.lower() or 'mjd' in col.lower()]\n",
    "\n",
    "# Select numeric feature columns from training data (excluding specified columns)\n",
    "feature_columns = [col for col in df_train.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "# Preprocess training data\n",
    "print(\"Preprocessing training data...\")\n",
    "X_train, y_train, available_train_features, missing_train_features = preprocess_data(\n",
    "    df_train, rename_map, feature_columns, excluded_columns, df_name=\"Training Data\"\n",
    ")\n",
    "print(\"Training data preprocessed.\\n\")\n",
    "\n",
    "# Preprocess test data\n",
    "print(\"Preprocessing test data...\")\n",
    "X_test, y_test, available_test_features, missing_test_features = preprocess_data(\n",
    "    df_test, rename_map, feature_columns, excluded_columns, df_name=\"Test Data\"\n",
    ")\n",
    "print(\"Test data preprocessed.\\n\")\n",
    "\n",
    "# Display available features\n",
    "print(\"Available Features in Training Data:\", available_train_features)\n",
    "print(\"Available Features in Test Data:\", available_test_features)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ensure that training and test have the same feature set\n",
    "common_features = list(set(available_train_features).intersection(set(available_test_features)))\n",
    "print(f\"Number of common features between Training and Test data: {len(common_features)}\")\n",
    "print(\"Common Features:\")\n",
    "for feature in common_features:\n",
    "    print(f\"- {feature}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# If there are no common features, you cannot proceed further\n",
    "if not common_features:\n",
    "    raise ValueError(\"No common features found between Training and Test data after renaming. Please check the rename_map and feature selection.\")\n",
    "\n",
    "# Update X_train and X_test to include only common_features\n",
    "X_train = X_train[common_features]\n",
    "X_test = X_test[common_features]\n",
    "\n",
    "# Standardize the features\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Feature scaling completed.\\n\")\n",
    "\n",
    "# Display class distribution before SMOTE\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(\"Training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "print(\"\\nApplying SMOTE to the training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "print(\"SMOTE applied successfully.\\n\")\n",
    "\n",
    "# Display class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Function to plot confusion matrices with specified normalization\n",
    "def plot_confusion_matrix(y_true, y_pred, ax, title, normalize='pred'):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix with specified normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True labels\n",
    "    - y_pred: Predicted labels\n",
    "    - ax: Matplotlib Axes object\n",
    "    - title: Title for the plot\n",
    "    - normalize: 'pred' for prediction normalized, 'true' for truth normalized\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    \n",
    "    if normalize == 'pred':\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=0)\n",
    "    elif normalize == 'true':\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        cm_normalized = cm\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=False, fmt=\".2f\", cmap='Blues', ax=ax, \n",
    "                xticklabels=['Non-TDE', 'TDE'], yticklabels=['Non-TDE', 'TDE'])\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if normalize == 'pred':\n",
    "                percentage = cm_normalized[i, j] * 100 if cm.sum(axis=0)[j] != 0 else 0\n",
    "            elif normalize == 'true':\n",
    "                percentage = cm_normalized[i, j] * 100 if cm.sum(axis=1)[i] != 0 else 0\n",
    "            else:\n",
    "                percentage = cm_normalized[i, j]\n",
    "            text = f'{percentage:.1f}%\\n({cm[i, j]})'\n",
    "            ax.text(j + 0.5, i + 0.5, text, ha='center', va='center', color='black', fontsize=10, \n",
    "                    bbox=dict(facecolor='white', edgecolor='white'))\n",
    "    \n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Train XGBoost classifier on original training data\n",
    "print(\"Training XGBoost classifier on original training data...\")\n",
    "xgb_original = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.3,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = xgb_original.predict(X_test_scaled)\n",
    "y_probs_original = xgb_original.predict_proba(X_test_scaled)[:, 1]\n",
    "print(\"Original XGBoost classifier trained.\\n\")\n",
    "\n",
    "# Train XGBoost classifier on SMOTE-augmented training data\n",
    "print(\"Training XGBoost classifier on SMOTE-augmented training data...\")\n",
    "xgb_smote = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.3,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = xgb_smote.predict(X_test_scaled)\n",
    "y_probs_smote = xgb_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "print(\"SMOTE XGBoost classifier trained.\\n\")\n",
    "\n",
    "# Plot confusion matrices with smaller figures\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))  # Reduced figure size\n",
    "\n",
    "# Original Classifier - Prediction Normalized\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred_original, axs[0, 0], 'XGB Original - Prediction Normalized', normalize='pred'\n",
    ")\n",
    "\n",
    "# Original Classifier - Truth Normalized\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred_original, axs[0, 1], 'XGB Original - Truth Normalized', normalize='true'\n",
    ")\n",
    "\n",
    "# SMOTE Classifier - Prediction Normalized\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred_smote, axs[1, 0], 'XGB SMOTE - Prediction Normalized', normalize='pred'\n",
    ")\n",
    "\n",
    "# SMOTE Classifier - Truth Normalized\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred_smote, axs[1, 1], 'XGB SMOTE - Truth Normalized', normalize='true'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to plot feature importance\n",
    "def plot_feature_importance(model, ax, title, feature_names):\n",
    "    xgb.plot_importance(model, max_num_features=20, importance_type='weight', ax=ax)\n",
    "    # Ensure that the yticklabels correspond to feature names\n",
    "    ax.set_yticklabels([\n",
    "        rename_map.get(feature, feature) for feature in feature_names\n",
    "    ], fontsize=10)\n",
    "    ax.set_xlabel('Feature Importance', fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "# Plot feature importances for both classifiers with smaller figures\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # Reduced figure size\n",
    "\n",
    "# Original Classifier Feature Importance\n",
    "plot_feature_importance(\n",
    "    xgb_original, axs[0], 'XGBoost Feature Importance - Original', common_features\n",
    ")\n",
    "\n",
    "# SMOTE Classifier Feature Importance\n",
    "plot_feature_importance(\n",
    "    xgb_smote, axs[1], 'XGBoost Feature Importance - SMOTE', common_features\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional Evaluation Metrics\n",
    "print(\"Evaluation Metrics:\\n\")\n",
    "\n",
    "print(\"Original Classifier:\")\n",
    "print(classification_report(y_test, y_pred_original))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_probs_original):.4f}\\n\")\n",
    "\n",
    "print(\"SMOTE Classifier:\")\n",
    "print(classification_report(y_test, y_pred_smote))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_probs_smote):.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4ff02-94e8-431c-b655-8cb74a0092ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ddfc4-7b0e-40e9-992b-478cd3d927b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd65c8-2fcc-430d-a299-b00ba7bf12e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Target class label\n",
    "cls = 1  # 'TDE' class is encoded as 1\n",
    "\n",
    "# Get the probabilities for the target class\n",
    "probs_cls = y_probs_xgb_original[:, cls]\n",
    "pr, recall, thresholds = precision_recall_curve(y_test, probs_cls)\n",
    "\n",
    "index = np.arange(len(thresholds))\n",
    "\n",
    "# Loose threshold: recall >= 0.96\n",
    "mask = recall[1:] >= 0.90\n",
    "if np.any(mask):\n",
    "    loose_index = max(index[mask])\n",
    "    threshold_loose = thresholds[loose_index]\n",
    "    print(f\"Loose threshold {threshold_loose:.2f}, Purity={100.*pr[loose_index]:.1f}%, Efficiency={100.*recall[loose_index]:.1f}%\")\n",
    "\n",
    "# Strict threshold: precision >= 0.95\n",
    "mask = pr[:-1] >= 0.95\n",
    "if np.any(mask):\n",
    "    strict_index = min(index[mask])\n",
    "    threshold_strict = thresholds[strict_index]\n",
    "    print(f\"Strict Threshold {threshold_strict:.2f}, Purity={100.*pr[strict_index]:.1f}%, Efficiency={100.*recall[strict_index]:.1f}%\")\n",
    "\n",
    "# Balanced threshold: precision >= 0.90\n",
    "mask = pr[:-1] >= 0.90\n",
    "if np.any(mask):\n",
    "    balanced_index = min(index[mask])\n",
    "    threshold_balanced = thresholds[balanced_index]\n",
    "    print(f\"Balanced Threshold {threshold_balanced:.2f}, Purity={100.*pr[balanced_index]:.1f}%, Efficiency={100.*recall[balanced_index]:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d632422-65e3-4f8f-89ab-fe96d1b8b223",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to find the threshold where precision reaches a specific value\n",
    "def find_threshold_for_precision(precision, thresholds, target_precision=0.95):\n",
    "    for p, t in zip(precision, thresholds):\n",
    "        if p >= target_precision:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "# Function to plot precision and recall as a function of the threshold\n",
    "def plot_precision_recall_thresholds(y_true, y_probs, model_name):\n",
    "    # Get the probabilities for the 'TDE' class (assuming 'TDE' is encoded as 1 and 'Other' as 0)\n",
    "    probs_tde = y_probs[:, 1]\n",
    "    precision_tde, recall_tde, thresholds_tde = precision_recall_curve(y_true == 1, probs_tde)\n",
    "\n",
    "    # Get the probabilities for the 'Non-TDE' class (assuming 'Other' is encoded as 0)\n",
    "    probs_non_tde = y_probs[:, 0]\n",
    "    precision_non_tde, recall_non_tde, thresholds_non_tde = precision_recall_curve(y_true == 0, probs_non_tde)\n",
    "\n",
    "    # Find the threshold for 95% purity (precision) for TDE\n",
    "    threshold_95_tde = find_threshold_for_precision(precision_tde, thresholds_tde)\n",
    "\n",
    "    plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "    # Plot for TDE class\n",
    "    plt.plot(thresholds_tde, precision_tde[:-1], label='Purity TDE', color='blue')\n",
    "    plt.plot(thresholds_tde, recall_tde[:-1], label='Efficiency TDE', color='blue', linestyle='--')\n",
    "\n",
    "    # Plot for Non-TDE class\n",
    "    plt.plot(thresholds_non_tde, precision_non_tde[:-1], label='Purity Non-TDE', color='red')\n",
    "    plt.plot(thresholds_non_tde, recall_non_tde[:-1], label='Efficiency Non-TDE', color='red', linestyle='--')\n",
    "\n",
    "    # Add vertical dashed lines for 95% purity for TDE\n",
    "    if threshold_95_tde is not None:\n",
    "        plt.axvline(x=threshold_95_tde, color='gray', linestyle='--', label='95% Purity Threshold for TDE')\n",
    "\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.text(0.5, 0.5, model_name, fontsize=14, ha='center', transform=plt.gca().transAxes, bbox=dict(facecolor='white', edgecolor='white'))\n",
    "    plt.legend(loc='lower right', fontsize='small')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# For Random Forest\n",
    "#plot_precision_recall_thresholds(y_test, y_probs_rf_original, 'Random Forest')\n",
    "\n",
    "# For XGBoost\n",
    "plot_precision_recall_thresholds(y_test, y_probs_xgb_original, 'XGBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c8b27-2d80-4bae-a8a4-1754090aee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac151fe9-0113-45ce-9b0f-eccc65d2bd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9c0c6-7095-4047-9055-e5eddf13c12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9d522-7e42-4253-875a-71829a096ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffda2a4-de59-4f07-89e5-896b9b2c972d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52e13449-fde2-4189-a6b3-ea4bc66a550c",
   "metadata": {},
   "source": [
    "## Check fits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fae25-f397-4a65-8cf0-1ace5ead30ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_elasticc_file(filename):\n",
    "    if '_PHOT' in filename:\n",
    "        headname = filename.replace('_PHOT', '_HEAD')\n",
    "    else:\n",
    "        headname = filename\n",
    "        filename = filename.replace('_HEAD', '_PHOT')\n",
    "\n",
    "    # Debug prints to verify paths\n",
    "    #print(f\"reading phot file: {filename}\")\n",
    "    print(f\"reading head file: {headname}\")\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "    if not os.path.exists(headname):\n",
    "        raise FileNotFoundError(f\"File not found: {headname}\")\n",
    "\n",
    "    table = Table.read(filename)\n",
    "    head = Table.read(headname)\n",
    "\n",
    "    # Sanitize the data\n",
    "    for _ in table:\n",
    "        _['BAND'] = _['BAND'].strip()\n",
    "\n",
    "    head['SNID'] = np.int64(head['SNID'])\n",
    "    \n",
    "    # Sanity check \n",
    "    if np.sum(table['MJD'] < 0) != len(head):\n",
    "        print(filename, 'is broken:', np.sum(table['MJD'] < 0), '!=', len(head))\n",
    "        \n",
    "    # Measured mag and magerr - simulated one is in SIM_MAGOBS\n",
    "    table['mag'] = np.nan\n",
    "    table['magerr'] = np.nan\n",
    "    idx = table['FLUXCAL'] > 0\n",
    "    \n",
    "    table['mag'][idx] = 27.5 - 2.5 * np.log10(table['FLUXCAL'][idx])\n",
    "    table['magerr'][idx] = 2.5 / np.log(10) * table['FLUXCALERR'][idx] / table['FLUXCAL'][idx]\n",
    "    \n",
    "    # Augment table with SNID (light curve id) from head\n",
    "    table['SNID'] = 0\n",
    "    \n",
    "    idx = np.where(table['MJD'] < 0)[0]\n",
    "    idx = np.hstack((np.array([0]), idx))\n",
    "\n",
    "    for i in range(1, len(idx)):\n",
    "        i0, i1 = idx[i - 1], idx[i]\n",
    "        table['SNID'][i0:i1] = head['SNID'][i - 1]\n",
    "    \n",
    "    table = table[table['MJD'] > 0]\n",
    "\n",
    "    return table, head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f33ed-1119-48d4-996e-61cfab1c8ff4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def try_optimization(gp, neg_ln_like, grad_neg_ln_like, initial_guess, retries=3):\n",
    "    best_result = None\n",
    "    for attempt in range(retries):\n",
    "        result = minimize(neg_ln_like, initial_guess, jac=grad_neg_ln_like, method='L-BFGS-B')\n",
    "        if best_result is None or (result.success and result.fun < best_result.fun):\n",
    "            best_result = result\n",
    "        if result.success:\n",
    "            break\n",
    "        else:\n",
    "            # Slightly perturb the initial guess for the next attempt\n",
    "            initial_guess += np.random.normal(0, 1e-2, size=initial_guess.shape)\n",
    "    \n",
    "    if best_result is None or not best_result.success:\n",
    "        print(f\"All optimization attempts failed for SNID {shead['SNID']}.\")\n",
    "        gp.set_parameter_vector(initial_guess)  # Use the best guess available\n",
    "    else:\n",
    "        gp.set_parameter_vector(best_result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cf2d0-1aa9-4a8c-a726-238b011bc0ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_gp(sub, shead, verbose=False):\n",
    "    try:\n",
    "        # Convert inputs to numpy arrays with appropriate dtype\n",
    "        t = np.array(sub['MJD'], dtype=float)\n",
    "        flux = np.array(sub['FLUXCAL'], dtype=float)\n",
    "        fluxerr = np.array(sub['FLUXCALERR'], dtype=float)\n",
    "        band = np.array([lsst_bands.get(b) for b in sub['BAND']], dtype=float)\n",
    "        \n",
    "        # 2D positions of data points (time and wavelength)\n",
    "        x = np.vstack([t, band]).T\n",
    "        \n",
    "        # Clean the data: remove rows with NaNs, infs, and non-positive flux values\n",
    "        mask = np.isfinite(flux) & np.isfinite(fluxerr) & np.all(np.isfinite(x), axis=1) & (flux > 0)\n",
    "        x = x[mask]\n",
    "        flux = flux[mask]\n",
    "        fluxerr = fluxerr[mask]\n",
    "\n",
    "        if len(flux) < 5:  # Ensure there are enough data points\n",
    "            raise ValueError(\"Not enough data points to perform GP fitting.\")\n",
    "        \n",
    "        # Calculate signal-to-noise ratios\n",
    "        signal_to_noises = np.abs(flux) / np.sqrt(fluxerr ** 2 + (1e-2 * np.max(flux)) ** 2)\n",
    "        scale = np.abs(flux[np.argmax(signal_to_noises)])\n",
    "\n",
    "        # Define the kernel for GP\n",
    "        kernel = (0.5 * scale) ** 2 * george.kernels.Matern32Kernel([100 ** 2, 6000 ** 2], ndim=2)\n",
    "        \n",
    "        # Define the GP model with HODLR solver and white noise\n",
    "        gp = george.GP(kernel, white_noise=np.log(np.var(fluxerr)), fit_white_noise=True, solver=george.HODLRSolver)\n",
    "        \n",
    "        # Compute the GP\n",
    "        gp.compute(x, fluxerr)\n",
    "        \n",
    "        # Define the negative log likelihood and its gradient for optimization\n",
    "        def neg_ln_like(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.log_likelihood(flux)\n",
    "        \n",
    "        def grad_neg_ln_like(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.grad_log_likelihood(flux)\n",
    "        \n",
    "        # Attempt optimization with multiple initial guesses\n",
    "        initial_guess = gp.get_parameter_vector()\n",
    "        try_optimization(gp, neg_ln_like, grad_neg_ln_like, initial_guess)\n",
    "        \n",
    "        # Return the GP, flux, data points, and final parameters\n",
    "        return gp, flux, x, gp.get_parameter_vector()\n",
    "\n",
    "    except (ValueError, np.linalg.LinAlgError, Exception) as e:\n",
    "        print(f\"GP optimization failed for SNID {shead['SNID']}: {e}\")\n",
    "        return None, None, None, None  # Return None to indicate failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a02dc-cbdc-492a-9df0-a0bd95a31fb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def peak_and_risefade(gp, x, flux, peak_threshold=2.512):\n",
    "    \"\"\"\n",
    "    Find the peak using Gaussian Process predictions for both g and r bands.\n",
    "    Then calculate the rise and fade times relative to the peak.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define bands\n",
    "    bands = ['g', 'r']\n",
    "    t_min, t_max = x[:, 0].min(), x[:, 0].max()\n",
    "\n",
    "    peak_mjd_g, peak_flux_g = None, None\n",
    "    peak_mjd_r, peak_flux_r = None, None\n",
    "\n",
    "    # Use the provided x values directly and extend for GP predictions\n",
    "    mjd_for_pred = np.linspace(t_min - 10, t_max + 10, 1000)\n",
    "\n",
    "    for band in bands:\n",
    "        wavelength = lsst_bands[band]\n",
    "        x_pred = np.vstack([mjd_for_pred, wavelength * np.ones_like(mjd_for_pred)]).T\n",
    "\n",
    "        # Predict flux at these times using the GP model\n",
    "        mean_pred, _ = gp.predict(flux, x_pred, return_var=True)\n",
    "\n",
    "        # Find the time of peak flux\n",
    "        peak_flux_idx = np.argmax(mean_pred)\n",
    "        peak_mjd = mjd_for_pred[peak_flux_idx]\n",
    "        peak_flux = mean_pred[peak_flux_idx]\n",
    "\n",
    "        if band == 'g':\n",
    "            peak_mjd_g, peak_flux_g = peak_mjd, peak_flux\n",
    "        elif band == 'r':\n",
    "            peak_mjd_r, peak_flux_r = peak_mjd, peak_flux\n",
    "\n",
    "    # Choose peak time and flux\n",
    "    t_peak = peak_mjd_g if peak_mjd_g is not None else peak_mjd_r\n",
    "    fpeak = peak_flux_g if peak_flux_g is not None else peak_flux_r\n",
    "\n",
    "    # Find the threshold flux for defining rise and fade times\n",
    "    threshold_flux = fpeak / peak_threshold  # Threshold for rise and fade time calculation\n",
    "\n",
    "    # Calculate rise time relative to the peak\n",
    "    try:\n",
    "        first_detection_index = np.where(mean_pred[:peak_flux_idx] <= threshold_flux)[0]  # <= for fainter\n",
    "        if len(first_detection_index) > 0:\n",
    "            rise_time = t_peak - mjd_for_pred[first_detection_index[-1]]\n",
    "        else:\n",
    "            rise_time = None\n",
    "    except IndexError:\n",
    "        rise_time = None\n",
    "\n",
    "    # Calculate fade time relative to the peak\n",
    "    try:\n",
    "        last_detection_index = np.where(mean_pred[peak_flux_idx:] <= threshold_flux)[0]  # <= for fainter\n",
    "        if len(last_detection_index) > 0:\n",
    "            fade_time = mjd_for_pred[peak_flux_idx + last_detection_index[0]] - t_peak\n",
    "        else:\n",
    "            fade_time = None\n",
    "    except IndexError:\n",
    "        fade_time = None\n",
    "\n",
    "    # Fallback to default values only if necessary\n",
    "    if rise_time is None:\n",
    "        rise_time = 50  # Default rise time\n",
    "    if fade_time is None:\n",
    "        fade_time = 75  # Default fade time\n",
    "\n",
    "    return rise_time, fade_time, t_peak, fpeak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fdef8-4271-4c4b-ac05-e4aed8f5afb5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_mean_colors_and_slope(sub, shead, gp, band1, band2, object_type, snid, rise_time, fade_time, classification, t_peak):\n",
    "    \"\"\"\n",
    "    Calculate mean colors, slopes, and plot color evolution for a given SNID using rise and fade times.\n",
    "    Fallback to original logic if rise_time or fade_time is not available.\n",
    "    \"\"\"\n",
    "    if isinstance(snid, bytes):\n",
    "        snid = snid.decode(\"utf-8\")\n",
    "    elif isinstance(snid, (np.integer, int)):\n",
    "        snid = str(snid)\n",
    "\n",
    "    snid_numeric = re.sub(r'\\D', '', snid)  # Extract only numeric part of SNID\n",
    "        \n",
    "    if isinstance(rise_time, np.ndarray):\n",
    "        rise_time = rise_time[0]\n",
    "    if isinstance(fade_time, np.ndarray):\n",
    "        fade_time = fade_time[0]\n",
    "\n",
    "    t_min, t_max = x[:, 0].min(), x[:, 0].max()\n",
    "    x1 = np.linspace(t_min - 10, t_max + 75, 1000)\n",
    "\n",
    "    \n",
    "    indices_pre_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                       (sub['MJD'] >= t_peak - rise_time) & (sub['MJD'] <= t_peak)\n",
    "    mjd_pre_peak = sub['MJD'][indices_pre_peak]\n",
    "    time_pre_peak = mjd_pre_peak - t_peak  # Time since peak\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = calc_color(gp, flux, sub, mjd_pre_peak, band1, band2)\n",
    "\n",
    "\n",
    "    mask_pre = ~np.isnan(x_pre_peak) & ~np.isnan(color_pre_peak) & (color_err_pre_peak < 1)\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = time_pre_peak[mask_pre], color_pre_peak[mask_pre], color_err_pre_peak[mask_pre]\n",
    "\n",
    "    indices_post_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                        (sub['MJD'] > t_peak) & (sub['MJD'] <= t_peak + fade_time)\n",
    "    mjd_post_peak = sub['MJD'][indices_post_peak]\n",
    "    time_post_peak = mjd_post_peak - t_peak  # Time since peak\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = calc_color(gp, flux, sub, mjd_post_peak, band1, band2)\n",
    "\n",
    "\n",
    "    mask_post = ~np.isnan(x_post_peak) & ~np.isnan(color_post_peak) & (color_err_post_peak < 1)\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = time_post_peak[mask_post], color_post_peak[mask_post], color_err_post_peak[mask_post]  \n",
    "    \n",
    "    slope_pre_peak, intercept_pre_peak, slope_err_pre_peak = None, None, None\n",
    "    slope_post_peak, intercept_post_peak, slope_err_post_peak = None, None, None\n",
    "\n",
    "    try:\n",
    "        if len(x_pre_peak) >= 2:\n",
    "            weights_pre = 1 / color_err_pre_peak**2\n",
    "            p_pre_peak, cov_pre_peak = np.polyfit(x_pre_peak, color_pre_peak, 1, w=weights_pre, cov=True)\n",
    "            slope_pre_peak, intercept_pre_peak = p_pre_peak\n",
    "            slope_err_pre_peak = np.sqrt(cov_pre_peak[0, 0])\n",
    "\n",
    "        if len(x_post_peak) >= 2:\n",
    "            weights_post = 1 / color_err_post_peak**2\n",
    "            p_post_peak, cov_post_peak = np.polyfit(x_post_peak, color_post_peak, 1, w=weights_post, cov=True)\n",
    "            slope_post_peak, intercept_post_peak = p_post_peak\n",
    "            slope_err_post_peak = np.sqrt(cov_post_peak[0, 0])\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Linear fit failed for SNID {snid_numeric}: {e}\")\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8, 6), sharex=True)\n",
    "    \n",
    "    for band, color in [('u', 'blue'), ('g', 'green'), ('r', 'red'), ('i', 'orange'), ('z', 'purple'), ('Y', 'yellow')]:\n",
    "        wavelength = lsst_bands[band]\n",
    "        x_band = np.vstack([x1, wavelength * np.ones_like(x1)]).T\n",
    "        flux_band, fluxerr_band = gp.predict(flux, x_band, return_var=True)\n",
    "        flux_band = np.maximum(flux_band, 0)  # Ensure positive predictions\n",
    "        \n",
    "        ax[0].plot(x_band[:, 0] - t_peak, flux_band, color=color, lw=1.5, alpha=0.5) #, label=f'{band} GP')\n",
    "        ax[0].fill_between(x_band[:, 0] - t_peak, flux_band - np.sqrt(fluxerr_band), flux_band + np.sqrt(fluxerr_band), color=color, alpha=0.2)\n",
    "\n",
    "    band_colors = {'u': 'blue', 'g': 'green', 'r': 'red', 'i': 'orange', 'z': 'purple', 'Y': 'yellow'}    \n",
    "    for band in ['u', 'g', 'r', 'i', 'z', 'Y']:\n",
    "        idx = (sub['BAND'] == band)\n",
    "        ax[0].errorbar(sub['MJD'][idx] - t_peak, sub['FLUXCAL'][idx], sub['FLUXCALERR'][idx], fmt='.', color=band_colors[band], label=f'{band}')\n",
    "\n",
    "    ax[0].axvline(0, color='blue', linestyle='dashed', label='GP Peak')\n",
    " #   ax[0].axvline(shead['PEAKMJD'] - t_peak, color='orange', linestyle='dashed', label='True Peak')\n",
    "\n",
    "    # Plotting rise and fade times on the flux plot relative to peak\n",
    "    if rise_time is not None:\n",
    "        ax[0].axvline(-rise_time, color='green', linestyle='dashed', label='Rise Time')  # Negative because it's before the peak\n",
    "        print(f\"Plotting rise time relative to peak at: {-rise_time}\")\n",
    "    \n",
    "    if fade_time is not None:\n",
    "        ax[0].axvline(fade_time, color='red', linestyle='dashed', label='Fade Time')  # Positive because it's after the peak\n",
    "        print(f\"Plotting fade time relative to peak at: {fade_time}\")\n",
    "\n",
    "         \n",
    "    ax[0].grid(alpha=0.3)\n",
    "    ax[0].set_ylabel('Flux')\n",
    "    ax[0].set_title(f'{object_type} {snid_numeric} {classification}')\n",
    "    ax[0].legend()#loc='upper left', bbox_to_anchor=(1, 1), borderaxespad=0., fontsize='small')\n",
    "    \n",
    "    if len(x_pre_peak) == len(color_pre_peak) and len(color_pre_peak) == len(color_err_pre_peak):\n",
    "        ax[1].errorbar(x_pre_peak, color_pre_peak, yerr=color_err_pre_peak, fmt='.', label='Pre-Peak Color (g-r)')\n",
    "    else:\n",
    "        print(\"Mismatch in lengths for color pre-peak.\")\n",
    "\n",
    "    if len(x_post_peak) == len(color_post_peak) and len(color_post_peak) == len(color_err_post_peak):\n",
    "        ax[1].errorbar(x_post_peak, color_post_peak, yerr=color_err_post_peak, fmt='.', label='Post-Peak Color (g-r)')\n",
    "    else:\n",
    "        print(\"Mismatch in lengths for color post-peak.\")\n",
    "\n",
    "    if slope_pre_peak is not None and slope_err_pre_peak is not None:\n",
    "        line_pre_peak = slope_pre_peak * np.array(x_pre_peak) + intercept_pre_peak\n",
    "        ax[1].plot(x_pre_peak, line_pre_peak, 'r-', label='Pre-Peak Fit')\n",
    "\n",
    "    if slope_post_peak is not None and slope_err_post_peak is not None:\n",
    "        line_post_peak = slope_post_peak * np.array(x_post_peak) + intercept_post_peak\n",
    "        ax[1].plot(x_post_peak, line_post_peak, 'g-', label='Post-Peak Fit')\n",
    "\n",
    "    ax[1].set_xlabel('Time Since Peak (days)')\n",
    "    ax[1].set_ylabel('Color (g-r)')\n",
    "    ax[1].legend()#loc='upper left', bbox_to_anchor=(1, 1), borderaxespad=0., fontsize='small')\n",
    "    ax[1].set_xlim(-100, 200)\n",
    "    ax[1].set_ylim(-1, 1)\n",
    "    ax[1].grid(alpha=0.3)\n",
    "    ax[1].axvline(0, color='blue', linestyle='dashed', label='GP-g Peak')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6c579c3-7932-490a-adce-bf408d31b723",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Load the data\n",
    "#table, head = read_elasticc_file('/home/kunal/kun/ELAsTiCC_2/ELASTICC2_TRAIN_02_TDE/ELASTICC2_TRAIN_02_NONIaMODEL0-0001_HEAD.FITS.gz')  # Adjust the file path as needed\n",
    "\n",
    "# Find the data for the selected SNID\n",
    "snid_of_interest = 19875457  # Replace with your specific SNID\n",
    "sub = table[table['SNID'] == snid_of_interest]\n",
    "shead = head[head['SNID'] == snid_of_interest]\n",
    "\n",
    "# Ensure that the head data is a single row for the selected SNID\n",
    "if len(shead) == 0:\n",
    "    raise ValueError(f\"No data found for SNID {snid_of_interest}.\")\n",
    "elif len(shead) > 1:\n",
    "    raise ValueError(f\"Multiple header rows found for SNID {snid_of_interest}. Expected a unique single header row.\")\n",
    "else:\n",
    "    shead = shead[0]  # Extract the single row if it's an Astropy Table\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Define band colors for plotting\n",
    "band_colors = ['blue', 'green', 'red', 'purple', 'brown', 'yellow']  # Replace with your preferred colors\n",
    "\n",
    "# Iterate through the bands and plot\n",
    "for i, band in enumerate(['u', 'g', 'r', 'i', 'z', 'Y']):\n",
    "    idx = sub['BAND'] == band\n",
    "    sub_band = sub[idx]\n",
    "    \n",
    "    # Errorbar plot for observations\n",
    "    plt.errorbar(sub_band['MJD'] - shead['PEAKMJD'], sub_band['mag'], sub_band['magerr'], fmt='o', alpha=0.2, color=band_colors[i])\n",
    "    \n",
    "    # Point plot for observations\n",
    "    plt.plot(sub_band['MJD'] - shead['PEAKMJD'], sub_band['mag'], 'o', alpha=1, color=band_colors[i], label=band)\n",
    "\n",
    "# Invert the y-axis \n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylim(28, np.nanmin(sub['mag']) - 1)\n",
    "plt.xlim(-60, 100)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "plt.xlabel('Time since peak (days)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title(f'Light Curve for SNID {snid_of_interest}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c6173-9b81-4d30-89ed-607842b2b734",
   "metadata": {},
   "source": [
    "## FP and FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197f380-c7dc-4699-9a59-f1e71017f212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_positive_snids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab4048-f7e0-4155-84a4-37c8a2fa91d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calc_mean_colors_and_slope(sub, shead, gp, band1, band2, object_type, snid, rise_time, fade_time, classification, t_peak):\n",
    "    \"\"\"\n",
    "    Calculate mean colors, slopes, and plot color evolution for a given SNID using rise and fade times.\n",
    "    Fallback to original logic if rise_time or fade_time is not available.\n",
    "    \"\"\"\n",
    "    if isinstance(snid, bytes):\n",
    "        snid = snid.decode(\"utf-8\")\n",
    "    elif isinstance(snid, (np.integer, int)):\n",
    "        snid = str(snid)\n",
    "\n",
    "    snid_numeric = re.sub(r'\\D', '', snid)  # Extract only numeric part of SNID\n",
    "        \n",
    "    if isinstance(rise_time, np.ndarray):\n",
    "        rise_time = rise_time[0]\n",
    "    if isinstance(fade_time, np.ndarray):\n",
    "        fade_time = fade_time[0]\n",
    "\n",
    "    # Assuming 'x' and 'flux' are defined elsewhere in your code\n",
    "    t_min, t_max = x[:, 0].min(), x[:, 0].max()\n",
    "    x1 = np.linspace(t_min - 10, t_max + 75, 1000)\n",
    "\n",
    "    indices_pre_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                       (sub['MJD'] >= t_peak - rise_time) & (sub['MJD'] <= t_peak)\n",
    "    mjd_pre_peak = sub['MJD'][indices_pre_peak]\n",
    "    time_pre_peak = mjd_pre_peak - t_peak  # Time since peak\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = calc_color(gp, flux, sub, mjd_pre_peak, band1, band2)\n",
    "\n",
    "    mask_pre = ~np.isnan(x_pre_peak) & ~np.isnan(color_pre_peak) & (color_err_pre_peak < 1)\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = time_pre_peak[mask_pre], color_pre_peak[mask_pre], color_err_pre_peak[mask_pre]\n",
    "\n",
    "    indices_post_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                        (sub['MJD'] > t_peak) & (sub['MJD'] <= t_peak + fade_time)\n",
    "    mjd_post_peak = sub['MJD'][indices_post_peak]\n",
    "    time_post_peak = mjd_post_peak - t_peak  # Time since peak\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = calc_color(gp, flux, sub, mjd_post_peak, band1, band2)\n",
    "\n",
    "    mask_post = ~np.isnan(x_post_peak) & ~np.isnan(color_post_peak) & (color_err_post_peak < 1)\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = time_post_peak[mask_post], color_post_peak[mask_post], color_err_post_peak[mask_post]  \n",
    "    \n",
    "    slope_pre_peak, intercept_pre_peak, slope_err_pre_peak = None, None, None\n",
    "    slope_post_peak, intercept_post_peak, slope_err_post_peak = None, None, None\n",
    "\n",
    "    try:\n",
    "        if len(x_pre_peak) >= 2:\n",
    "            weights_pre = 1 / color_err_pre_peak**2\n",
    "            p_pre_peak, cov_pre_peak = np.polyfit(x_pre_peak, color_pre_peak, 1, w=weights_pre, cov=True)\n",
    "            slope_pre_peak, intercept_pre_peak = p_pre_peak\n",
    "            slope_err_pre_peak = np.sqrt(cov_pre_peak[0, 0])\n",
    "\n",
    "        if len(x_post_peak) >= 2:\n",
    "            weights_post = 1 / color_err_post_peak**2\n",
    "            p_post_peak, cov_post_peak = np.polyfit(x_post_peak, color_post_peak, 1, w=weights_post, cov=True)\n",
    "            slope_post_peak, intercept_post_peak = p_post_peak\n",
    "            slope_err_post_peak = np.sqrt(cov_post_peak[0, 0])\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Linear fit failed for SNID {snid_numeric}: {e}\")\n",
    "\n",
    "    # Create subplots with adjusted height ratios\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8, 5), sharex=True, \n",
    "                           gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    for band, color in [('u', 'blue'), ('g', 'green'), ('r', 'red'), \n",
    "                        ('i', 'orange'), ('z', 'purple'), ('Y', 'yellow')]:\n",
    "        wavelength = lsst_bands[band]\n",
    "        x_band = np.vstack([x1, wavelength * np.ones_like(x1)]).T\n",
    "        flux_band, fluxerr_band = gp.predict(flux, x_band, return_var=True)\n",
    "        flux_band = np.maximum(flux_band, 0)  # Ensure positive predictions\n",
    "        \n",
    "        ax[0].plot(x_band[:, 0] - t_peak, flux_band, color=color, lw=1.5, alpha=0.5)\n",
    "        ax[0].fill_between(x_band[:, 0] - t_peak, \n",
    "                           flux_band - np.sqrt(fluxerr_band), \n",
    "                           flux_band + np.sqrt(fluxerr_band), \n",
    "                           color=color, alpha=0.2)\n",
    "\n",
    "    band_colors = {'u': 'blue', 'g': 'green', 'r': 'red', \n",
    "                  'i': 'orange', 'z': 'purple', 'Y': 'yellow'}    \n",
    "    for band in ['u', 'g', 'r', 'i', 'z', 'Y']:\n",
    "        idx = (sub['BAND'] == band)\n",
    "        ax[0].errorbar(sub['MJD'][idx] - t_peak, \n",
    "                      sub['FLUXCAL'][idx], \n",
    "                      sub['FLUXCALERR'][idx], \n",
    "                      fmt='.', \n",
    "                      color=band_colors[band], \n",
    "                      label=f'{band}')\n",
    "\n",
    "    ax[0].axvline(0, color='blue', linestyle='dashed', label='GP Peak')\n",
    "    # ax[0].axvline(shead['PEAKMJD'] - t_peak, color='orange', linestyle='dashed', label='True Peak')\n",
    "\n",
    "    # Plotting rise and fade times on the flux plot relative to peak\n",
    "    if rise_time is not None:\n",
    "        ax[0].axvline(-rise_time, color='green', linestyle='dashed', label='Rise Time')  # Negative because it's before the peak\n",
    "        print(f\"Plotting rise time relative to peak at: {-rise_time}\")\n",
    "    \n",
    "    if fade_time is not None:\n",
    "        ax[0].axvline(fade_time, color='red', linestyle='dashed', label='Fade Time')  # Positive because it's after the peak\n",
    "        print(f\"Plotting fade time relative to peak at: {fade_time}\")\n",
    "\n",
    "    # Set Y-axis limit for the top plot\n",
    "    ax[0].set_ylim(bottom=-50)  # Prevent y-axis from going below -50\n",
    "\n",
    "    # Add text labels \"rise\" and \"fade\" near the respective vertical lines\n",
    "    y_max_flux = ax[0].get_ylim()[1]\n",
    "    text_y_position = y_max_flux * 0.95  # 95% of the y-axis maximum\n",
    "\n",
    "    if rise_time is not None:\n",
    "        ax[0].text(-rise_time, text_y_position, 'rise', \n",
    "                   color='green', fontsize=9, ha='right', va='top', \n",
    "                   bbox=dict(facecolor='white', edgecolor='green', boxstyle='round,pad=0.2'))\n",
    "    \n",
    "    if fade_time is not None:\n",
    "        ax[0].text(fade_time, text_y_position, 'fade', \n",
    "                   color='red', fontsize=9, ha='left', va='top', \n",
    "                   bbox=dict(facecolor='white', edgecolor='red', boxstyle='round,pad=0.2'))\n",
    "\n",
    "    ax[0].grid(alpha=0.3)\n",
    "    ax[0].set_ylabel('Flux')\n",
    "    ax[0].set_title(f'ELAsTiCC2 {object_type} {snid_numeric} {classification}')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot Pre-Peak Color\n",
    "    if len(x_pre_peak) == len(color_pre_peak) and len(color_pre_peak) == len(color_err_pre_peak):\n",
    "        ax[1].errorbar(x_pre_peak, color_pre_peak, yerr=color_err_pre_peak, fmt='.', \n",
    "                      label='Pre-Peak Color (g-r)')\n",
    "    else:\n",
    "        print(\"Mismatch in lengths for color pre-peak.\")\n",
    "\n",
    "    # Plot Post-Peak Color\n",
    "    if len(x_post_peak) == len(color_post_peak) and len(color_post_peak) == len(color_err_post_peak):\n",
    "        ax[1].errorbar(x_post_peak, color_post_peak, yerr=color_err_post_peak, fmt='.', \n",
    "                      label='Post-Peak Color (g-r)')\n",
    "    else:\n",
    "        print(\"Mismatch in lengths for color post-peak.\")\n",
    "\n",
    "    # Plot Pre-Peak Fit\n",
    "    if slope_pre_peak is not None and slope_err_pre_peak is not None:\n",
    "        line_pre_peak = slope_pre_peak * np.array(x_pre_peak) + intercept_pre_peak\n",
    "        ax[1].plot(x_pre_peak, line_pre_peak, 'r-', label='Pre-Peak Fit')\n",
    "\n",
    "    # Plot Post-Peak Fit\n",
    "    if slope_post_peak is not None and slope_err_post_peak is not None:\n",
    "        line_post_peak = slope_post_peak * np.array(x_post_peak) + intercept_post_peak\n",
    "        ax[1].plot(x_post_peak, line_post_peak, 'g-', label='Post-Peak Fit')\n",
    "\n",
    "    ax[1].set_xlabel('Time Since Peak (days)')\n",
    "    ax[1].set_ylabel('g-r (magnitude)')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlim(-100, 200)\n",
    "    ax[1].set_ylim(-0.5, 0.7)\n",
    "    ax[1].grid(alpha=0.3)\n",
    "    ax[1].axvline(0, color='blue', linestyle='dashed', label='GP-g Peak')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77eadb-3bc8-468c-968f-6d836cbf78b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#True positives\n",
    "\n",
    "# Base path and template for file names\n",
    "base_path = \"../../../karpov/ELASTICC2/\"\n",
    "filename_template = \"ELASTICC2_FINAL_{object_type}/ELASTICC2_FINAL_NONIaMODEL0-{index}_HEAD.FITS.gz\"\n",
    "\n",
    "# Object types and model names\n",
    "object_info = [\n",
    "    'TDE', #'AGN', 'SLSN-I+host', 'SLSN-I_no_host', 'SNIa-SALT3', 'SNIa-91bg',\n",
    "    #'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19', 'SNIIn-MOSFIT', 'SNII-NMF',\n",
    "    #'SNII+HostXT_V19', 'SNIIb+HostXT_V19', 'KN_B19', 'KN_K17'\n",
    "]\n",
    "\n",
    "# Generate all filenames and corresponding object types\n",
    "all_filenames = []\n",
    "object_types = []\n",
    "for object_type in object_info:\n",
    "    filenames = [os.path.join(base_path, filename_template.format(object_type=object_type, index=str(i).zfill(4))) for i in range(5, 20)]\n",
    "    all_filenames.extend(filenames)\n",
    "    object_types.extend([object_type] * len(filenames))\n",
    "\n",
    "# Process files\n",
    "for filename, object_type in zip(all_filenames, object_types):\n",
    "    table, head = read_elasticc_file(filename)\n",
    "    snids, shead_list, sub_list = get_snid_head_sub(table, head)\n",
    "\n",
    "    for snid, shead, sub in zip(snids, shead_list, sub_list):\n",
    "        snid = int(snid)  # Convert to standard integer\n",
    "\n",
    "        # Check if the SNID is in the TP TDE list\n",
    "        if snid not in true_positive_snids:\n",
    "            continue  # Skip if not in TP TDE list\n",
    "\n",
    "        # Compute GP for the data set of the SNID\n",
    "        gp, flux, x, params = compute_gp(sub, shead)\n",
    "        \n",
    "        if gp is None:\n",
    "            print(f\"Skipping SNID {shead['SNID']} due to GP optimization failure.\")\n",
    "            continue  # Skip processing this SNID if GP failed\n",
    "\n",
    "            \n",
    "        # Calculate peak, rise, and fade times using the combined function\n",
    "        rise_time, fade_time, t_peak, fpeak = peak_and_risefade(gp, x, flux)\n",
    "\n",
    "        # Plot using the calc_mean_colors_and_slope function\n",
    "        slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak = calc_mean_colors_and_slope(\n",
    "            sub, shead, gp, 'g', 'r', object_type, snid, rise_time, fade_time, \"TP\", t_peak)\n",
    "\n",
    "        plt.show()  # Display the plot immediately\n",
    "\n",
    "# End the timer and print the elapsed time\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc3582-0a62-4eb9-b3e1-c2670cb79abd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calc_mean_colors_and_slope(sub, shead, gp, band1, band2, object_type, snid, rise_time, fade_time, classification, t_peak, save_path=None, show_plot=True):\n",
    "    \"\"\"\n",
    "    Calculate mean colors, slopes, and plot color evolution for a given SNID using rise and fade times.\n",
    "    Optionally save the figure to a specified path and control plot display.\n",
    "    \n",
    "    Parameters:\n",
    "    - save_path (str or None): Path to save the figure. If None, the figure is not saved.\n",
    "    - show_plot (bool): Whether to display the plot. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    - fig: The matplotlib figure object.\n",
    "    - slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak\n",
    "    \"\"\"\n",
    "    if isinstance(snid, bytes):\n",
    "        snid = snid.decode(\"utf-8\")\n",
    "    elif isinstance(snid, (np.integer, int)):\n",
    "        snid = str(snid)\n",
    "\n",
    "    snid_numeric = re.sub(r'\\D', '', snid)  # Extract only numeric part of SNID\n",
    "        \n",
    "    if isinstance(rise_time, np.ndarray):\n",
    "        rise_time = rise_time[0]\n",
    "    if isinstance(fade_time, np.ndarray):\n",
    "        fade_time = fade_time[0]\n",
    "\n",
    "    # Assuming 'x' and 'flux' are defined elsewhere in your code\n",
    "    t_min, t_max = x[:, 0].min(), x[:, 0].max()\n",
    "    x1 = np.linspace(t_min - 10, t_max + 75, 1000)\n",
    "    \n",
    "    # Pre-Peak Indices and Data\n",
    "    indices_pre_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                       (sub['MJD'] >= t_peak - rise_time) & (sub['MJD'] <= t_peak)\n",
    "    mjd_pre_peak = sub['MJD'][indices_pre_peak]\n",
    "    time_pre_peak = mjd_pre_peak - t_peak  # Time since peak\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = calc_color(gp, flux, sub, mjd_pre_peak, band1, band2)\n",
    "\n",
    "    mask_pre = ~np.isnan(x_pre_peak) & ~np.isnan(color_pre_peak) & (color_err_pre_peak < 1)\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = time_pre_peak[mask_pre], color_pre_peak[mask_pre], color_err_pre_peak[mask_pre]\n",
    "\n",
    "    # Post-Peak Indices and Data\n",
    "    indices_post_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                        (sub['MJD'] > t_peak) & (sub['MJD'] <= t_peak + fade_time)\n",
    "    mjd_post_peak = sub['MJD'][indices_post_peak]\n",
    "    time_post_peak = mjd_post_peak - t_peak  # Time since peak\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = calc_color(gp, flux, sub, mjd_post_peak, band1, band2)\n",
    "\n",
    "    mask_post = ~np.isnan(x_post_peak) & ~np.isnan(color_post_peak) & (color_err_post_peak < 1)\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = time_post_peak[mask_post], color_post_peak[mask_post], color_err_post_peak[mask_post]  \n",
    "    \n",
    "    slope_pre_peak, intercept_pre_peak, slope_err_pre_peak = None, None, None\n",
    "    slope_post_peak, intercept_post_peak, slope_err_post_peak = None, None, None\n",
    "\n",
    "    try:\n",
    "        if len(x_pre_peak) >= 2:\n",
    "            weights_pre = 1 / color_err_pre_peak**2\n",
    "            p_pre_peak, cov_pre_peak = np.polyfit(x_pre_peak, color_pre_peak, 1, w=weights_pre, cov=True)\n",
    "            slope_pre_peak, intercept_pre_peak = p_pre_peak\n",
    "            slope_err_pre_peak = np.sqrt(cov_pre_peak[0, 0])\n",
    "\n",
    "        if len(x_post_peak) >= 2:\n",
    "            weights_post = 1 / color_err_post_peak**2\n",
    "            p_post_peak, cov_post_peak = np.polyfit(x_post_peak, color_post_peak, 1, w=weights_post, cov=True)\n",
    "            slope_post_peak, intercept_post_peak = p_post_peak\n",
    "            slope_err_post_peak = np.sqrt(cov_post_peak[0, 0])\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Linear fit failed for SNID {snid_numeric}: {e}\")\n",
    "\n",
    "    # Create subplots with adjusted height ratios\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), sharex=True, \n",
    "                           gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    for band, color in [('u', 'blue'), ('g', 'green'), ('r', 'red'), \n",
    "                        ('i', 'orange'), ('z', 'purple'), ('Y', 'pink')]:\n",
    "        wavelength = lsst_bands[band]\n",
    "        x_band = np.vstack([x1, wavelength * np.ones_like(x1)]).T\n",
    "        flux_band, fluxerr_band = gp.predict(flux, x_band, return_var=True)\n",
    "        flux_band = np.maximum(flux_band, 0)  # Ensure positive predictions\n",
    "        \n",
    "        ax[0].plot(x_band[:, 0] - t_peak, flux_band, color=color, lw=1.5, alpha=0.5)\n",
    "        ax[0].fill_between(x_band[:, 0] - t_peak, \n",
    "                           flux_band - np.sqrt(fluxerr_band), \n",
    "                           flux_band + np.sqrt(fluxerr_band), \n",
    "                           color=color, alpha=0.2)\n",
    "\n",
    "    band_colors = {'u': 'blue', 'g': 'green', 'r': 'red', \n",
    "                  'i': 'orange', 'z': 'purple', 'Y': 'pink'}    \n",
    "    for band in ['u', 'g', 'r', 'i', 'z', 'Y']:\n",
    "        idx = (sub['BAND'] == band)\n",
    "        ax[0].errorbar(sub['MJD'][idx] - t_peak, \n",
    "                      sub['FLUXCAL'][idx], \n",
    "                      sub['FLUXCALERR'][idx], \n",
    "                      fmt='.', \n",
    "                      color=band_colors[band], \n",
    "                      label=f'{band}')\n",
    "\n",
    "    ax[0].axvline(0, color='green', linestyle='dashed', label='GP-g Peak')\n",
    "    # ax[0].axvline(shead['PEAKMJD'] - t_peak, color='orange', linestyle='dashed', label='True Peak')\n",
    "\n",
    "    # Plotting rise and fade times on the flux plot relative to peak\n",
    "    if rise_time is not None:\n",
    "        ax[0].axvline(-rise_time, color='blue', linestyle='dashed', label='Rise Time')  # Negative because it's before the peak\n",
    "        print(f\"Plotting rise time relative to peak at: {-rise_time}\")\n",
    "    \n",
    "    if fade_time is not None:\n",
    "        ax[0].axvline(fade_time, color='red', linestyle='dashed', label='Fade Time')  # Positive because it's after the peak\n",
    "        print(f\"Plotting fade time relative to peak at: {fade_time}\")\n",
    "\n",
    "    # Set Y-axis limit for the top plot\n",
    "    ax[0].set_ylim(bottom=-50)  # Prevent y-axis from going below -50\n",
    "\n",
    "    # Add text labels \"rise\" and \"fade\" inside between the lines and peak\n",
    "    # Define an offset for positioning the text labels closer to the peak\n",
    "    delta_x = 5  # days; adjust as needed\n",
    "\n",
    "    # Calculate the y-position for the text labels based on y-limits\n",
    "    y_max_flux = ax[0].get_ylim()[1]\n",
    "    y_min_flux = ax[0].get_ylim()[0]\n",
    "    text_y_position = y_max_flux * 0.05  # 90% of the y-axis maximum\n",
    "\n",
    "    if rise_time is not None:\n",
    "        # Position the \"rise\" label slightly to the right of the rise_time line\n",
    "        x_rise_label = -rise_time + delta_x\n",
    "        ax[0].text(x_rise_label, text_y_position, 'rise', \n",
    "                   color='blue', fontsize=9, ha='left', va='bottom', \n",
    "                   bbox=dict(facecolor='white', edgecolor='blue', boxstyle='round,pad=0.2'))\n",
    "    \n",
    "    if fade_time is not None:\n",
    "        # Position the \"fade\" label slightly to the left of the fade_time line\n",
    "        x_fade_label = fade_time - delta_x\n",
    "        ax[0].text(x_fade_label, text_y_position, 'fade', \n",
    "                   color='red', fontsize=9, ha='right', va='bottom', \n",
    "                   bbox=dict(facecolor='white', edgecolor='red', boxstyle='round,pad=0.2'))\n",
    "\n",
    "\n",
    "    ax[0].grid(alpha=0.3)\n",
    "    ax[0].set_ylabel('Flux (ADU)')\n",
    "    ax[0].set_title(f'ELAsTiCC2 {object_type} {snid_numeric} {classification}')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot Pre-Peak Color\n",
    "    if len(x_pre_peak) == len(color_pre_peak) and len(color_pre_peak) == len(color_err_pre_peak):\n",
    "        ax[1].errorbar(x_pre_peak, color_pre_peak, yerr=color_err_pre_peak, fmt='.', \n",
    "                      label='Pre-Peak Color (g-r)')\n",
    "    else:\n",
    "        print(\"Mismatch in lengths for color pre-peak.\")\n",
    "\n",
    "    # Plot Post-Peak Color\n",
    "    if len(x_post_peak) == len(color_post_peak) and len(color_post_peak) == len(color_err_post_peak):\n",
    "        ax[1].errorbar(x_post_peak, color_post_peak, yerr=color_err_post_peak, fmt='.', \n",
    "                      label='Post-Peak Color (g-r)')\n",
    "    else:\n",
    "        print(\"Mismatch in lengths for color post-peak.\")\n",
    "\n",
    "    # Plot Pre-Peak Fit\n",
    "    if slope_pre_peak is not None and slope_err_pre_peak is not None:\n",
    "        line_pre_peak = slope_pre_peak * np.array(x_pre_peak) + intercept_pre_peak\n",
    "        ax[1].plot(x_pre_peak, line_pre_peak, 'r-', label='Pre-Peak Fit')\n",
    "\n",
    "    # Plot Post-Peak Fit\n",
    "    if slope_post_peak is not None and slope_err_post_peak is not None:\n",
    "        line_post_peak = slope_post_peak * np.array(x_post_peak) + intercept_post_peak\n",
    "        ax[1].plot(x_post_peak, line_post_peak, 'g-', label='Post-Peak Fit')\n",
    "\n",
    "    ax[1].set_xlabel('Time Since Peak (days)')\n",
    "    ax[1].set_ylabel('g-r (mag)')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlim(-70, 200)\n",
    "    ax[1].set_ylim(-1, 1)\n",
    "    ax[1].grid(alpha=0.3)\n",
    "    ax[1].axvline(0, color='green', linestyle='dashed', label='GP-g Peak')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure if save_path is provided\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "    # Show the plot if requested\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2d86e-3132-4d9e-93df-f9d3ed8c847a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calc_mean_colors_and_slope(sub, shead, gp, band1, band2, object_type, snid, rise_time, fade_time, classification, t_peak, save_path=None, show_plot=True):\n",
    "    \"\"\"\n",
    "    Calculate mean colors, slopes, and plot GP fit for a given SNID using rise and fade times.\n",
    "    Optionally save the figure to a specified path and control plot display.\n",
    "    \n",
    "    Parameters:\n",
    "    - save_path (str or None): Path to save the figure. If None, the figure is not saved.\n",
    "    - show_plot (bool): Whether to display the plot. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    - fig: The matplotlib figure object.\n",
    "    - slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak\n",
    "    \"\"\"\n",
    "    if isinstance(snid, bytes):\n",
    "        snid = snid.decode(\"utf-8\")\n",
    "    elif isinstance(snid, (np.integer, int)):\n",
    "        snid = str(snid)\n",
    "\n",
    "    snid_numeric = re.sub(r'\\D', '', snid)  # Extract only numeric part of SNID\n",
    "        \n",
    "    if isinstance(rise_time, np.ndarray):\n",
    "        rise_time = rise_time[0]\n",
    "    if isinstance(fade_time, np.ndarray):\n",
    "        fade_time = fade_time[0]\n",
    "\n",
    "    # Assuming 'x' and 'flux' are defined elsewhere in your code\n",
    "    t_min, t_max = x[:, 0].min(), x[:, 0].max()\n",
    "    x1 = np.linspace(t_min - 10, t_max + 75, 1000)\n",
    "    \n",
    "    # Pre-Peak Indices and Data\n",
    "    indices_pre_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                       (sub['MJD'] >= t_peak - rise_time) & (sub['MJD'] <= t_peak)\n",
    "    mjd_pre_peak = sub['MJD'][indices_pre_peak]\n",
    "    time_pre_peak = mjd_pre_peak - t_peak  # Time since peak\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = calc_color(gp, flux, sub, mjd_pre_peak, band1, band2)\n",
    "\n",
    "    mask_pre = ~np.isnan(x_pre_peak) & ~np.isnan(color_pre_peak) & (color_err_pre_peak < 1)\n",
    "    x_pre_peak, color_pre_peak, color_err_pre_peak = time_pre_peak[mask_pre], color_pre_peak[mask_pre], color_err_pre_peak[mask_pre]\n",
    "\n",
    "    # Post-Peak Indices and Data\n",
    "    indices_post_peak = ((sub['BAND'] == band1) | (sub['BAND'] == band2)) & \\\n",
    "                        (sub['MJD'] > t_peak) & (sub['MJD'] <= t_peak + fade_time)\n",
    "    mjd_post_peak = sub['MJD'][indices_post_peak]\n",
    "    time_post_peak = mjd_post_peak - t_peak  # Time since peak\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = calc_color(gp, flux, sub, mjd_post_peak, band1, band2)\n",
    "\n",
    "    mask_post = ~np.isnan(x_post_peak) & ~np.isnan(color_post_peak) & (color_err_post_peak < 1)\n",
    "    x_post_peak, color_post_peak, color_err_post_peak = time_post_peak[mask_post], color_post_peak[mask_post], color_err_post_peak[mask_post]  \n",
    "    \n",
    "    slope_pre_peak, intercept_pre_peak, slope_err_pre_peak = None, None, None\n",
    "    slope_post_peak, intercept_post_peak, slope_err_post_peak = None, None, None\n",
    "\n",
    "    try:\n",
    "        if len(x_pre_peak) >= 2:\n",
    "            weights_pre = 1 / color_err_pre_peak**2\n",
    "            p_pre_peak, cov_pre_peak = np.polyfit(x_pre_peak, color_pre_peak, 1, w=weights_pre, cov=True)\n",
    "            slope_pre_peak, intercept_pre_peak = p_pre_peak\n",
    "            slope_err_pre_peak = np.sqrt(cov_pre_peak[0, 0])\n",
    "\n",
    "        if len(x_post_peak) >= 2:\n",
    "            weights_post = 1 / color_err_post_peak**2\n",
    "            p_post_peak, cov_post_peak = np.polyfit(x_post_peak, color_post_peak, 1, w=weights_post, cov=True)\n",
    "            slope_post_peak, intercept_post_peak = p_post_peak\n",
    "            slope_err_post_peak = np.sqrt(cov_post_peak[0, 0])\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Linear fit failed for SNID {snid_numeric}: {e}\")\n",
    "\n",
    "    # Create a single subplot for GP fit\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 4))\n",
    "    \n",
    "    # Plot GP fits for each band\n",
    "    for band, color in [('u', 'blue'), ('g', 'green'), ('r', 'red'), \n",
    "                        ('i', 'orange'), ('z', 'purple'), ('Y', 'pink')]:\n",
    "        wavelength = lsst_bands[band]\n",
    "        x_band = np.vstack([x1, wavelength * np.ones_like(x1)]).T\n",
    "        flux_band, fluxerr_band = gp.predict(flux, x_band, return_var=True)\n",
    "        flux_band = np.maximum(flux_band, 0)  # Ensure positive predictions\n",
    "        \n",
    "        ax.plot(x_band[:, 0] - t_peak, flux_band, color=color, lw=1.5, alpha=0.7)\n",
    "        ax.fill_between(x_band[:, 0] - t_peak, \n",
    "                        flux_band - np.sqrt(fluxerr_band), \n",
    "                        flux_band + np.sqrt(fluxerr_band), \n",
    "                        color=color, alpha=0.2)\n",
    "    \n",
    "    # Optionally, plot the observational data points (commented out if not needed)\n",
    "    band_colors = {'u': 'blue', 'g': 'green', 'r': 'red', \n",
    "                   'i': 'orange', 'z': 'purple', 'Y': 'pink'}    \n",
    "    for band in ['u', 'g', 'r', 'i', 'z', 'Y']:\n",
    "        idx = (sub['BAND'] == band)\n",
    "        ax.errorbar(sub['MJD'][idx] - t_peak, \n",
    "                  sub['FLUXCAL'][idx], \n",
    "                  sub['FLUXCALERR'][idx], \n",
    "                  fmt='.', \n",
    "                  color=band_colors[band], \n",
    "                  label=f'{band}')\n",
    "\n",
    "    # Set Y-axis limit for the plot\n",
    "    ax.set_ylim(bottom=-50)  # Prevent y-axis from going below -50\n",
    "\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylabel('Flux (ADU)')\n",
    "    ax.set_xlabel('Time since peak (days)')\n",
    "    ax.set_title(f'ELAsTiCC2 {object_type} {snid_numeric} {classification}')\n",
    "    ax.legend()\n",
    "\n",
    "    # Remove any vertical lines related to peak, rise, and fade\n",
    "    # Removed ax.axvline calls and associated text labels\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure if save_path is provided\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "    # Show the plot if requested\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1839d9-0aeb-477c-a6d2-c0b9f529c065",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# True positives\n",
    "# (Assuming 'true_positive_snids' is defined elsewhere in your code)\n",
    "\n",
    "# Base path and template for file names\n",
    "base_path = \"../../../karpov/ELASTICC2/\"\n",
    "filename_template = \"ELASTICC2_FINAL_{object_type}/ELASTICC2_FINAL_NONIaMODEL0-{index}_HEAD.FITS.gz\"\n",
    "\n",
    "# Object types and model names\n",
    "object_info = [\n",
    "    'TDE', #'AGN', 'SLSN-I+host', 'SLSN-I_no_host', 'SNIa-SALT3', 'SNIa-91bg',\n",
    "    #'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19', 'SNIIn-MOSFIT', 'SNII-NMF',\n",
    "    #'SNII+HostXT_V19', 'SNIIb+HostXT_V19', 'KN_B19', 'KN_K17'\n",
    "]\n",
    "\n",
    "# Generate all filenames and corresponding object types\n",
    "all_filenames = []\n",
    "object_types = []\n",
    "for object_type in object_info:\n",
    "    filenames = [os.path.join(base_path, filename_template.format(object_type=object_type, index=str(i).zfill(4))) for i in range(5, 7)]\n",
    "    all_filenames.extend(filenames)\n",
    "    object_types.extend([object_type] * len(filenames))\n",
    "\n",
    "# Define the SNID(s) you want to save the figure for\n",
    "save_snids = [137495236, 110044328]  # Replace with your specific SNID(s)\n",
    "\n",
    "# Directory to save figures\n",
    "save_directory = \"home/bhardwaj/notebooksLSST/tdes-fzu/notebooksLSST/ELAsTiCC2_processed/results-images\"\n",
    "os.makedirs(save_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Process files\n",
    "for filename, object_type in zip(all_filenames, object_types):\n",
    "    table, head = read_elasticc_file(filename)\n",
    "    snids, shead_list, sub_list = get_snid_head_sub(table, head)\n",
    "\n",
    "    for snid, shead, sub in zip(snids, shead_list, sub_list):\n",
    "        snid = int(snid)  # Convert to standard integer\n",
    "\n",
    "        # Check if the SNID is in the TP TDE list\n",
    "        if snid not in true_positive_snids:\n",
    "            continue  # Skip if not in TP TDE list\n",
    "\n",
    "        # Compute GP for the data set of the SNID\n",
    "        gp, flux, x, params = compute_gp(sub, shead)\n",
    "        \n",
    "        if gp is None:\n",
    "            print(f\"Skipping SNID {shead['SNID']} due to GP optimization failure.\")\n",
    "            continue  # Skip processing this SNID if GP failed\n",
    "\n",
    "        # Calculate peak, rise, and fade times using the combined function\n",
    "        rise_time, fade_time, t_peak, fpeak = peak_and_risefade(gp, x, flux)\n",
    "\n",
    "        # Determine if the current SNID needs to have its figure saved\n",
    "        if snid in save_snids:\n",
    "            # Define the save path for this SNID\n",
    "            save_path = os.path.join(save_directory, f\"SNID_{snid}.png\")\n",
    "            show_plot = False  # Don't display the plot immediately\n",
    "            print(f\"Saving figure for SNID {snid} to {save_path}\")\n",
    "        else:\n",
    "            save_path = None\n",
    "            show_plot = True  # Display the plot normally\n",
    "\n",
    "        # Plot using the modified calc_mean_colors_and_slope function\n",
    "        fig, slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak = calc_mean_colors_and_slope(\n",
    "            sub, shead, gp, 'g', 'r', object_type, snid, rise_time, fade_time, \"GP fit with 1 C.I.\", t_peak,\n",
    "            save_path=save_path,\n",
    "            show_plot=show_plot\n",
    "        )\n",
    "\n",
    "        # If you want to further manipulate or close the figure after saving/showing\n",
    "        if save_path is not None:\n",
    "            plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "# End the timer and print the elapsed time\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df84e1-a407-4d1b-875b-b921e89125d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ee13f-5cc0-4be2-9327-20ab9ccb85f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc9ad2-3df5-4dd5-b1a1-cc9dea10508a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#True negatives\n",
    "\n",
    "# Base path and template for file names\n",
    "base_path = \"../../../karpov/ELASTICC2/\"\n",
    "filename_template = \"ELASTICC2_FINAL_{object_type}/ELASTICC2_FINAL_NONIaMODEL0-{index}_HEAD.FITS.gz\"\n",
    "\n",
    "# Object types and model names\n",
    "object_info = [\n",
    "    'TDE', 'AGN', 'SLSN-I+host', 'SLSN-I_no_host', #'SNIa-SALT3', \n",
    "    'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19', 'SNIIn-MOSFIT', 'SNII-NMF',\n",
    "    'SNII+HostXT_V19', 'SNIIb+HostXT_V19', 'KN_B19', 'KN_K17'\n",
    "]\n",
    "\n",
    "# Generate all filenames and corresponding object types\n",
    "all_filenames = []\n",
    "object_types = []\n",
    "for object_type in object_info:\n",
    "    filenames = [os.path.join(base_path, filename_template.format(object_type=object_type, index=str(i).zfill(4))) for i in range(2, 3)]\n",
    "    all_filenames.extend(filenames)\n",
    "    object_types.extend([object_type] * len(filenames))\n",
    "\n",
    "# Process files\n",
    "for filename, object_type in zip(all_filenames, object_types):\n",
    "    table, head = read_elasticc_file(filename)\n",
    "    snids, shead_list, sub_list = get_snid_head_sub(table, head)\n",
    "\n",
    "    for snid, shead, sub in zip(snids, shead_list, sub_list):\n",
    "        snid = int(snid)  # Convert to standard integer\n",
    "\n",
    "        # Check if the SNID is in the TN TDE list\n",
    "        if snid not in true_negative_snids:\n",
    "            continue  # Skip if not in TN TDE list\n",
    "\n",
    "        # Compute GP for the data set of the SNID\n",
    "        gp, flux, x, params = compute_gp(sub, shead)\n",
    "        \n",
    "        if gp is None:\n",
    "            print(f\"Skipping SNID {shead['SNID']} due to GP optimization failure.\")\n",
    "            continue  # Skip processing this SNID if GP failed\n",
    "\n",
    "            \n",
    "        # Calculate peak, rise, and fade times using the combined function\n",
    "        rise_time, fade_time, t_peak, fpeak = peak_and_risefade(gp, x, flux)\n",
    "\n",
    "        # Plot using the calc_mean_colors_and_slope function\n",
    "        slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak = calc_mean_colors_and_slope(\n",
    "            sub, shead, gp, 'g', 'r', object_type, snid, rise_time, fade_time, \"TN\", t_peak)\n",
    "\n",
    "        plt.show()  # Display the plot immediately\n",
    "        \n",
    "\n",
    "# End the timer and print the elapsed time\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c74b59-4048-4994-b42d-a84b665e78fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#False positives\n",
    "\n",
    "# Base path and template for file names\n",
    "base_path = \"../../../karpov/ELASTICC2/\"\n",
    "filename_template = \"ELASTICC2_FINAL_{object_type}/ELASTICC2_FINAL_NONIaMODEL0-{index}_HEAD.FITS.gz\"\n",
    "\n",
    "# Object types and model names\n",
    "object_info = [\n",
    "    #'TDE', 'AGN',\n",
    "    'SLSN-I+host', 'SLSN-I_no_host', #'SNIa-SALT3',\n",
    "    'SNIa-91bg', 'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19', 'SNIIn-MOSFIT', 'SNII-NMF',\n",
    "    'SNII+HostXT_V19', 'SNIIb+HostXT_V19'\n",
    "]\n",
    "\n",
    "# Generate all filenames and corresponding object types\n",
    "all_filenames = []\n",
    "object_types = []\n",
    "for object_type in object_info:\n",
    "    filenames = [os.path.join(base_path, filename_template.format(object_type=object_type, index=str(i).zfill(4))) for i in range(2, 4)]\n",
    "    all_filenames.extend(filenames)\n",
    "    object_types.extend([object_type] * len(filenames))\n",
    "\n",
    "# Process files\n",
    "for filename, object_type in zip(all_filenames, object_types):\n",
    "    table, head = read_elasticc_file(filename)\n",
    "    snids, shead_list, sub_list = get_snid_head_sub(table, head)\n",
    "\n",
    "    for snid, shead, sub in zip(snids, shead_list, sub_list):\n",
    "        snid = int(snid)  # Convert to standard integer\n",
    "\n",
    "        # Check if the SNID is in the FP TDE list\n",
    "        if snid not in false_positive_snids:\n",
    "            continue  # Skip if not in FP TDE list\n",
    "\n",
    "        # Compute GP for the data set of the SNID\n",
    "        gp, flux, x, params = compute_gp(sub, shead)\n",
    "        \n",
    "        if gp is None:\n",
    "            print(f\"Skipping SNID {shead['SNID']} due to GP optimization failure.\")\n",
    "            continue  # Skip processing this SNID if GP failed\n",
    "\n",
    "            \n",
    "        # Calculate peak, rise, and fade times using the combined function\n",
    "        rise_time, fade_time, t_peak, fpeak = peak_and_risefade(gp, x, flux)\n",
    "\n",
    "        # Plot using the calc_mean_colors_and_slope function\n",
    "        slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak = calc_mean_colors_and_slope(\n",
    "            sub, shead, gp, 'g', 'r', object_type, snid, rise_time, fade_time, \"FP\", t_peak)\n",
    "\n",
    "        plt.show()  # Display the plot immediately\n",
    "\n",
    "# End the timer and print the elapsed time\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35a316-50c9-4141-a7cd-6244e841151e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#False negatives\n",
    "\n",
    "# Base path and template for file names\n",
    "base_path = \"../../../karpov/ELASTICC2/\"\n",
    "filename_template = \"ELASTICC2_FINAL_{object_type}/ELASTICC2_FINAL_NONIaMODEL0-{index}_HEAD.FITS.gz\"\n",
    "\n",
    "# Object types and model names\n",
    "object_info = [\n",
    "    'TDE', 'AGN', 'SLSN-I+host', 'SLSN-I_no_host', 'SNIa-SALT3', 'SNIa-91bg',\n",
    "    'SNIax', 'SNIcBL+HostXT_V19', 'SNIb+HostXT_V19', 'SNIIn-MOSFIT', 'SNII-NMF',\n",
    "    'SNII+HostXT_V19', 'SNIIb+HostXT_V19', 'KN_B19', 'KN_K17'\n",
    "]\n",
    "\n",
    "# Generate all filenames and corresponding object types\n",
    "all_filenames = []\n",
    "object_types = []\n",
    "for object_type in object_info:\n",
    "    filenames = [os.path.join(base_path, filename_template.format(object_type=object_type, index=str(i).zfill(4))) for i in range(1, 41)]\n",
    "    all_filenames.extend(filenames)\n",
    "    object_types.extend([object_type] * len(filenames))\n",
    "\n",
    "# Process files\n",
    "for filename, object_type in zip(all_filenames, object_types):\n",
    "    table, head = read_elasticc_file(filename)\n",
    "    snids, shead_list, sub_list = get_snid_head_sub(table, head)\n",
    "\n",
    "    for snid, shead, sub in zip(snids, shead_list, sub_list):\n",
    "        snid = int(snid)  # Convert to standard integer\n",
    "\n",
    "        # Check if the SNID is in the FN TDE list\n",
    "        if snid not in false_negative_snids:\n",
    "            continue  # Skip if not in FN TDE list\n",
    "\n",
    "        # Compute GP for the data set of the SNID\n",
    "        gp, flux, x, params = compute_gp(sub, shead)\n",
    "        \n",
    "        if gp is None:\n",
    "            print(f\"Skipping SNID {shead['SNID']} due to GP optimization failure.\")\n",
    "            continue  # Skip processing this SNID if GP failed\n",
    "\n",
    "            \n",
    "        # Calculate peak, rise, and fade times using the combined function\n",
    "        rise_time, fade_time, t_peak, fpeak = peak_and_risefade(gp, x, flux)\n",
    "\n",
    "        # Plot using the calc_mean_colors_and_slope function\n",
    "        slope_pre_peak, slope_post_peak, slope_err_pre_peak, slope_err_post_peak = calc_mean_colors_and_slope(\n",
    "            sub, shead, gp, 'g', 'r', object_type, snid, rise_time, fade_time, \"FN\", t_peak)\n",
    "\n",
    "        plt.show()  # Display the plot immediately\n",
    "\n",
    "# End the timer and print the elapsed time\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e24b5-085b-4282-a6b1-8a899790d74f",
   "metadata": {},
   "source": [
    "### search and plot gp by snid and object_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8921b-7e2b-438d-96e4-b018732b1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_for_snid(82038200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a9ce8-8cdf-4491-a509-f3bd3298ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Interactive waterfall plot\n",
    "shap.initjs()\n",
    "plot_shap_for_snid(68314204)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9c4cc-1804-413e-8366-8e6b27dd03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate SNIDs\n",
    "duplicate_snids = results['SNID'][results['SNID'].duplicated()]\n",
    "if not duplicate_snids.empty:\n",
    "    print(f\"Duplicate SNIDs found: {duplicate_snids.tolist()}\")\n",
    "    # Handle duplicates as necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef6ce3-972f-41ec-bc8f-8c48a99b4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If X_test_scaled is a NumPy array, ensure feature order matches feature_columns\n",
    "if isinstance(X_test_scaled, np.ndarray):\n",
    "    assert X_test_scaled.shape[1] == len(feature_columns), \"Mismatch in number of features.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad04d4-df7e-4a5e-bb02-04a39edc3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dependence scatter plot to show the effect of a single feature across the whole dataset\n",
    "shap.plots.scatter(shap_values[:, \"Mean Color Post Peak (r-i)\"], color=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be60da4-af59-4e88-b6ea-937506b4d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.plots.beeswarm(shap_values, max_display=20, color=plt.get_cmap(\"cool\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e7be1-de1f-4a9a-8efb-f01fed2bb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, order=shap_values.abs.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37cc1f-8217-437c-ab5b-32e685acb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d8b9a-239b-4c1f-bae3-1ccd30cc5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8a9c2-05c7-4a80-8d6f-cb095f0541c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1659f-4bf7-43ce-a15d-7f6e57992035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdfed0-a8d3-4eb7-8155-d479d6381bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c7b3c-cf8b-4dc7-abdb-d0f53ea74faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# Prepare the data\n",
    "df0['Simple_Object_Type'] = df0['Object_Type'].apply(lambda x: 'TDE' if x == 'TDE' else 'Other')\n",
    "\n",
    "# Select numeric columns for the model and exclude 'SNID', any 'peak', and 'redshift' related columns\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakMagErr', 'REDSHIFT_FINAL', 'REDSHIFT_FINAL_ERR']\n",
    "\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower() or 'flux' in col.lower() or 'mjd' in col.lower()] #or 'slope' in col.lower()]\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "X = df0[feature_columns].fillna(-999)  # Handle missing values by filling with -999\n",
    "y = df0['Simple_Object_Type'].apply(lambda x: 1 if x == 'TDE' else 0)  # Target variable\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the datasetbb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train the XGBoost classifier on the original dataset\n",
    "xgb_classifier_original = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.5, max_depth=3, random_state=42)\n",
    "xgb_classifier_original.fit(X_train, y_train)\n",
    "y_pred_xgb_original = xgb_classifier_original.predict(X_test)\n",
    "y_probs_xgb_original = xgb_classifier_original.predict_proba(X_test)\n",
    "\n",
    "# Get the corresponding SNIDs, Object_Types, and predictions\n",
    "test_snids = df0.iloc[y_test.index]['SNID'].apply(int).tolist()\n",
    "test_object_types = df0.iloc[y_test.index]['Object_Type'].tolist()\n",
    "\n",
    "# Combine SNID, Object_Type, and predictions\n",
    "results = pd.DataFrame({\n",
    "    'SNID': test_snids,\n",
    "    'Object_Type': test_object_types,\n",
    "    'True_Label': y_test,\n",
    "    'Predicted_Label': y_pred_xgb_original\n",
    "})\n",
    "\n",
    "# Identify false positives and false negatives\n",
    "false_positives = results[(results['True_Label'] == 0) & (results['Predicted_Label'] == 1)]\n",
    "false_negatives = results[(results['True_Label'] == 1) & (results['Predicted_Label'] == 0)]\n",
    "true_negatives = results[(results['True_Label'] == 0) & (results['Predicted_Label'] == 0)]\n",
    "true_positives = results[(results['True_Label'] == 1) & (results['Predicted_Label'] == 1)]\n",
    "\n",
    "\n",
    "# Output the SNIDs and Object_Types for FNs\n",
    "false_negative_snids = false_negatives['SNID'].tolist()\n",
    "false_negative_types = false_negatives['Object_Type'].tolist()\n",
    "false_positive_snids = false_positives['SNID'].tolist()\n",
    "false_positive_types = false_positives['Object_Type'].tolist()\n",
    "true_negative_snids = true_negatives['SNID'].tolist()\n",
    "true_negative_types = true_negatives['Object_Type'].tolist()\n",
    "true_positive_snids = true_positives['SNID'].tolist()\n",
    "true_positive_types = true_positives['Object_Type'].tolist()\n",
    "\n",
    "print(f\"Total False Negative TDEs: {len(false_negative_snids)}\")\n",
    "print(f\"Total False Positive TDEs: {len(false_positive_snids)}\")\n",
    "print(f\"Total True Negative TDEs: {len(true_negative_snids)}\")\n",
    "print(f\"Total True Positive TDEs: {len(true_positive_snids)}\")\n",
    "\n",
    "print(f\"Sample FN TDE SNIDs: {false_negative_snids[:10]}\")\n",
    "print(f\"Sample FN TDE Object Types: {false_negative_types[:10]}\")\n",
    "print(f\"Sample FP TDE SNIDs: {false_positive_snids[:10]}\")\n",
    "print(f\"Sample FP TDE Object Types: {false_positive_types[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e33ebb-b725-4fc3-83f0-440cfc58c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222017f-153a-42e2-a2e9-d50f4d5da07c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# ---------------------------------------\n",
    "\n",
    "# Prepare the data\n",
    "df0['Simple_Object_Type'] = df0['Object_Type'].apply(lambda x: 'TDE' if x == 'TDE' else 'Other')\n",
    "\n",
    "# Select numeric columns for the model and exclude specified columns\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakMagErr', 'REDSHIFT_FINAL', 'REDSHIFT_FINAL_ERR']\n",
    "\n",
    "# Exclude columns containing 'err', 'flux', or 'mjd'\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower() or 'flux' in col.lower() or 'mjd' in col.lower()]\n",
    "\n",
    "# Select feature columns\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "# Define a renaming map for clarity (if applicable)\n",
    "rename_map = {\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time'\n",
    "}\n",
    "\n",
    "# Rename columns for clarity (if necessary)\n",
    "df0.rename(columns=rename_map, inplace=True)\n",
    "feature_columns = [rename_map.get(col, col) for col in feature_columns]\n",
    "\n",
    "# Define features and target\n",
    "X = df0[feature_columns]\n",
    "y = df0['Simple_Object_Type'].apply(lambda x: 1 if x == 'TDE' else 0)  # Binary encoding\n",
    "\n",
    "# ---------------------------------------\n",
    "# Handling Missing Values\n",
    "# ---------------------------------------\n",
    "\n",
    "# Split the dataset first to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Compute the mean of each feature from the training set\n",
    "feature_means = X_train.mean()\n",
    "\n",
    "# Fill missing values in both training and test sets with the training set means\n",
    "X_train_filled = X_train.fillna(feature_means)\n",
    "X_test_filled = X_test.fillna(feature_means)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature Scaling\n",
    "# ---------------------------------------\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train_filled)\n",
    "X_test_scaled = scaler.transform(X_test_filled)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Model Training\n",
    "# ---------------------------------------\n",
    "\n",
    "# Initialize the XGBoost classifier with specified hyperparameters\n",
    "xgb_classifier_original = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.5,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,  # Suppress warning in newer XGBoost versions\n",
    "    eval_metric='logloss'     # Specify evaluation metric to avoid warnings\n",
    ")\n",
    "\n",
    "# Train the classifier on the original training data\n",
    "xgb_classifier_original.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb_original = xgb_classifier_original.predict(X_test_scaled)\n",
    "y_probs_xgb_original = xgb_classifier_original.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Extracting Test Set Information\n",
    "# ---------------------------------------\n",
    "\n",
    "# Get the corresponding SNIDs and Object_Types for the test set\n",
    "test_snids = df0.iloc[y_test.index]['SNID'].apply(int).tolist()\n",
    "test_object_types = df0.iloc[y_test.index]['Object_Type'].tolist()\n",
    "\n",
    "# Combine SNID, Object_Type, and predictions into a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'SNID': test_snids,\n",
    "    'Object_Type': test_object_types,\n",
    "    'True_Label': y_test,\n",
    "    'Predicted_Label': y_pred_xgb_original\n",
    "})\n",
    "\n",
    "# ---------------------------------------\n",
    "# Identifying Prediction Outcomes\n",
    "# ---------------------------------------\n",
    "\n",
    "# Identify false positives, false negatives, true positives, and true negatives\n",
    "false_positives = results[(results['True_Label'] == 0) & (results['Predicted_Label'] == 1)]\n",
    "false_negatives = results[(results['True_Label'] == 1) & (results['Predicted_Label'] == 0)]\n",
    "true_negatives = results[(results['True_Label'] == 0) & (results['Predicted_Label'] == 0)]\n",
    "true_positives = results[(results['True_Label'] == 1) & (results['Predicted_Label'] == 1)]\n",
    "\n",
    "# Extract SNIDs and Object_Types for each category\n",
    "false_negative_snids = false_negatives['SNID'].tolist()\n",
    "false_negative_types = false_negatives['Object_Type'].tolist()\n",
    "false_positive_snids = false_positives['SNID'].tolist()\n",
    "false_positive_types = false_positives['Object_Type'].tolist()\n",
    "true_negative_snids = true_negatives['SNID'].tolist()\n",
    "true_negative_types = true_negatives['Object_Type'].tolist()\n",
    "true_positive_snids = true_positives['SNID'].tolist()\n",
    "true_positive_types = true_positives['Object_Type'].tolist()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Displaying Results\n",
    "# ---------------------------------------\n",
    "\n",
    "print(f\"Total False Negative TDEs: {len(false_negative_snids)}\")\n",
    "print(f\"Total False Positive TDEs: {len(false_positive_snids)}\")\n",
    "print(f\"Total True Negative TDEs: {len(true_negative_snids)}\")\n",
    "print(f\"Total True Positive TDEs: {len(true_positive_snids)}\\n\")\n",
    "\n",
    "print(f\"Sample FN TDE SNIDs: {false_negative_snids[:10]}\")\n",
    "print(f\"Sample FN TDE Object Types: {false_negative_types[:10]}\")\n",
    "print(f\"Sample FP TDE SNIDs: {false_positive_snids[:10]}\")\n",
    "print(f\"Sample FP TDE Object Types: {false_positive_types[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0b8a8-e8e8-4057-9f58-1e343045aab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0fc7b-8f8e-4870-bc6b-bbb8a317acb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e3f00-0ef2-49b4-9b18-7b38d31c8e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare SHAP explainer\n",
    "explainer_data = X_test  # Using the test dataset\n",
    "explainer = shap.Explainer(xgb_classifier_original, explainer_data)\n",
    "shap_values = explainer(explainer_data, check_additivity=False)  # Disable additivity check\n",
    "snid_to_shap_index = {snid: i for i, snid in enumerate(results['SNID'])} # Map SNIDs to SHAP values indices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2993622d-ba5e-41c0-8660-e2be02cad9e7",
   "metadata": {},
   "source": [
    "# build a Permutation explainer and explain the model predictions on the given dataset\n",
    "explainer = shap.explainers.GPUTree(model, X)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# get just the explanations for the positive class\n",
    "shap_values = shap_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477f5d7-27a1-40ca-b203-cef14675e9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c5197-2f4b-49aa-9e83-d7c87fa689a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb728b3e-8fe7-41b0-9922-194cf18e00bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_shap_for_snid(snid):\n",
    "    try:\n",
    "        # Determine the classification based on SNID\n",
    "        if snid in false_negative_snids:\n",
    "            classification = \"FN\"\n",
    "        elif snid in false_positive_snids:\n",
    "            classification = \"FP\"\n",
    "        elif snid in true_negative_snids:\n",
    "            classification = \"TN\"\n",
    "        elif snid in true_positive_snids:\n",
    "            classification = \"TP\"    \n",
    "        else:\n",
    "            print(f\"SNID {snid} not found FP/FN/TP/TN lists.\")\n",
    "            return\n",
    "\n",
    "        # Find the index of the SNID\n",
    "        index = snid_to_shap_index[snid]\n",
    "\n",
    "        # Access the SHAP values for this SNID\n",
    "        shap_value = shap_values[index]\n",
    "        print(f\"Successfully accessed SHAP value for SNID {snid} at index {index}.\")\n",
    "\n",
    "        # Map the SHAP values to their corresponding feature names\n",
    "        feature_mapping = {i: name for i, name in enumerate(feature_columns)}\n",
    "        shap_value.feature_names = [feature_mapping.get(i, f\"Feature {i}\") for i in range(len(feature_columns))]\n",
    "\n",
    "        # Plot the SHAP waterfall plot\n",
    "        shap.plots.waterfall(shap_value, max_display=5, show=False)\n",
    "        plt.title(f\"SNID: {snid}, Object Type: {results.iloc[index]['Object_Type']}, Classification: {classification}\")\n",
    "        plt.show()\n",
    "\n",
    "    except IndexError as e:\n",
    "        print(f\"IndexError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77ba04-c074-4d92-91b8-e7b2e6b3dc38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Verify Data Integrity\n",
    "def verify_data(X_train, X_test):\n",
    "    if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n",
    "        raise ValueError(\"Training data contains NaN or infinite values.\")\n",
    "    if np.any(np.isnan(X_test)) or np.any(np.isinf(X_test)):\n",
    "        raise ValueError(\"Test data contains NaN or infinite values.\")\n",
    "    \n",
    "    zero_variance_features = [col for col in X_train.columns if X_train[col].nunique() <= 1]\n",
    "    if zero_variance_features:\n",
    "        print(f\"Features with zero or one unique value: {zero_variance_features}\")\n",
    "        # Handle accordingly\n",
    "\n",
    "# 2. Initialize LIME Explainer\n",
    "def initialize_lime(X_train, feature_columns, categorical_features=[], categorical_names={}):\n",
    "    lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train.values,\n",
    "        feature_names=feature_columns,\n",
    "        class_names=['Non-TDE', 'TDE'],\n",
    "        categorical_features=categorical_features,\n",
    "        categorical_names=categorical_names,\n",
    "        mode='classification',\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "    return lime_explainer\n",
    "\n",
    "# 3. Define SNID to Index Mapping\n",
    "def create_snid_mapping(results):\n",
    "    snid_to_index = {snid: i for i, snid in enumerate(results['SNID'])}\n",
    "    return snid_to_index\n",
    "\n",
    "# 4. Define the LIME Plotting Function\n",
    "def plot_lime_for_snid(snid, lime_explainer, snid_to_index, pipeline, aligned_results,\n",
    "                      false_negative_snids, false_positive_snids, true_negative_snids, true_positive_snids):\n",
    "    try:\n",
    "        # Determine the classification based on SNID\n",
    "        if snid in false_negative_snids:\n",
    "            classification = \"FN\"\n",
    "        elif snid in false_positive_snids:\n",
    "            classification = \"FP\"\n",
    "        elif snid in true_negative_snids:\n",
    "            classification = \"TN\"\n",
    "        elif snid in true_positive_snids:\n",
    "            classification = \"TP\"    \n",
    "        else:\n",
    "            print(f\"SNID {snid} not found in FP/FN/TP/TN lists.\")\n",
    "            return\n",
    "\n",
    "        # Find the index of the SNID\n",
    "        index = snid_to_index[snid]\n",
    "\n",
    "        # Extract the instance for explanation\n",
    "        instance = X_test_imputed.iloc[index].values\n",
    "\n",
    "        # Debug: Inspect the instance\n",
    "        print(f\"Inspecting SNID {snid} at index {index}:\")\n",
    "        print(instance)\n",
    "\n",
    "        # Generate LIME explanation\n",
    "        explanation = lime_explainer.explain_instance(\n",
    "            data_row=instance,\n",
    "            predict_fn=pipeline.predict_proba,  # Use the pipeline's predict_proba\n",
    "            num_features=5,\n",
    "            top_labels=1\n",
    "        )\n",
    "\n",
    "        # Get the predicted class\n",
    "        predicted_class = pipeline.predict(instance.reshape(1, -1))[0]\n",
    "        class_names = ['Non-TDE', 'TDE']\n",
    "        predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "        # Plot the explanation\n",
    "        fig = explanation.as_pyplot_figure(label=predicted_class)\n",
    "        plt.title(f\"SNID: {snid}, Object Type: {aligned_results.iloc[index]['Object_Type']}, Classification: {classification}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - SNID {snid} not found.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# 5. Execute the Workflow with Debugging\n",
    "def main():\n",
    "    # Step 1: Handle Data Cleaning\n",
    "    # Replace infinite values with NaN\n",
    "    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Impute NaN values\n",
    "    imputer = SimpleImputer(strategy='mean')  # Ensure this matches your pipeline's strategy\n",
    "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    # Step 2: Verify Data\n",
    "    verify_data(X_train_imputed, X_test_imputed)\n",
    "\n",
    "    # Step 3: Initialize LIME Explainer with imputed data\n",
    "    lime_explainer = initialize_lime(\n",
    "        X_train=X_train_imputed,\n",
    "        feature_columns=feature_columns,\n",
    "        categorical_features=[],  # Update if necessary\n",
    "        categorical_names={}      # Update if necessary\n",
    "    )\n",
    "\n",
    "    # Step 4: Create SNID to Index Mapping\n",
    "    snid_to_index = create_snid_mapping(results)\n",
    "\n",
    "    # Step 5: Define SNIDs to Explain\n",
    "    example_snids = ['14291270']  # Replace with actual SNIDs\n",
    "\n",
    "    # Step 6: Plot Explanations\n",
    "    for snid in example_snids:\n",
    "        plot_lime_for_snid(\n",
    "            snid=snid,\n",
    "            lime_explainer=lime_explainer,\n",
    "            snid_to_index=snid_to_index,\n",
    "            pipeline=pipeline_xgb_smote,  # Replace with the appropriate pipeline\n",
    "            aligned_results=results,  # Ensure this is correctly defined\n",
    "            false_negative_snids=false_negative_snids,\n",
    "            false_positive_snids=false_positive_snids,\n",
    "            true_negative_snids=true_negative_snids,\n",
    "            true_positive_snids=true_positive_snids\n",
    "        )\n",
    "\n",
    "# Run the main function\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c60ff3-2a91-4d96-ae26-d019c219ca66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Replace infinite values with NaN\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Step 2: Impute NaN values\n",
    "imputer = SimpleImputer(strategy='mean')  # Ensure this matches your pipeline's strategy\n",
    "\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Step 3: Verify data\n",
    "verify_data(X_train_imputed, X_test_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea797f45-9dc7-4744-ac6c-b2c53a932804",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data using the combined pipeline's preprocessor\n",
    "X_train_processed = combined_pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "X_test_processed = combined_pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=feature_columns, index=X_train.index)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=feature_columns, index=X_test.index)\n",
    "\n",
    "# Verify the processed data\n",
    "verify_data(X_train_processed, X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0cdca-20a8-40e6-816f-dd59728ceb17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "# Define categorical features if any\n",
    "categorical_features = []  # Update based on your data\n",
    "categorical_names = {}     # Update based on your data\n",
    "\n",
    "# Initialize LIME Tabular Explainer\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train_processed.values,\n",
    "    feature_names=feature_columns,\n",
    "    class_names=['Non-TDE', 'TDE'],\n",
    "    categorical_features=categorical_features,\n",
    "    categorical_names=categorical_names,\n",
    "    mode='classification',\n",
    "    discretize_continuous=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ba7fb-4b8d-4df9-a906-714c4ecb4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'results' is your DataFrame containing 'SNID'\n",
    "snid_to_index = {snid: i for i, snid in enumerate(results['SNID'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9c3af-4050-4a83-b32d-5f4655d780ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_lime_for_snid(snid, lime_explainer, snid_to_index, pipeline, aligned_results,\n",
    "                      false_negative_snids, false_positive_snids, true_negative_snids, true_positive_snids):\n",
    "    try:\n",
    "        # Determine the classification based on SNID\n",
    "        if snid in false_negative_snids:\n",
    "            classification = \"FN\"\n",
    "        elif snid in false_positive_snids:\n",
    "            classification = \"FP\"\n",
    "        elif snid in true_negative_snids:\n",
    "            classification = \"TN\"\n",
    "        elif snid in true_positive_snids:\n",
    "            classification = \"TP\"    \n",
    "        else:\n",
    "            print(f\"SNID {snid} not found in FP/FN/TP/TN lists.\")\n",
    "            return\n",
    "\n",
    "        # Find the index of the SNID\n",
    "        index = snid_to_index[snid]\n",
    "\n",
    "        # Extract the instance for explanation (raw data)\n",
    "        instance = X_test.iloc[index].values\n",
    "\n",
    "        # Generate LIME explanation\n",
    "        explanation = lime_explainer.explain_instance(\n",
    "            data_row=instance,\n",
    "            predict_fn=combined_pipeline.predict_proba,  # Use the combined pipeline's predict_proba\n",
    "            num_features=5,\n",
    "            top_labels=1\n",
    "        )\n",
    "\n",
    "        # Get the predicted class\n",
    "        predicted_class = combined_pipeline.predict(instance.reshape(1, -1))[0]\n",
    "        class_names = ['Non-TDE', 'TDE']\n",
    "        predicted_class_name = class_names[predicted_class]\n",
    "\n",
    "        # Plot the explanation\n",
    "        fig = explanation.as_pyplot_figure(label=predicted_class)\n",
    "        plt.title(f\"SNID: {snid}, Object Type: {aligned_results.iloc[index]['Object_Type']}, Classification: {classification}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - SNID {snid} not found.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178a3f2-b355-4a89-8580-2f8394c88292",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step 1: Define and fit the combined pipeline (already done earlier)\n",
    "    \n",
    "    # Step 2: Transform the data using the combined pipeline's preprocessor\n",
    "    X_train_processed = combined_pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "    X_test_processed = combined_pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=feature_columns, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=feature_columns, index=X_test.index)\n",
    "    \n",
    "    # Step 3: Verify the processed data\n",
    "    verify_data(X_train_processed, X_test_processed)\n",
    "    \n",
    "    # Step 4: Initialize LIME Explainer with processed training data\n",
    "    lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train_processed.values,\n",
    "        feature_names=feature_columns,\n",
    "        class_names=['Non-TDE', 'TDE'],\n",
    "        categorical_features=categorical_features,\n",
    "        categorical_names=categorical_names,\n",
    "        mode='classification',\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "    \n",
    "    # Step 5: Create SNID to Index Mapping\n",
    "    snid_to_index = {snid: i for i, snid in enumerate(results['SNID'])}\n",
    "    \n",
    "    # Step 6: Define SNIDs to Explain\n",
    "    example_snids = ['14291270']  # Replace with actual SNIDs\n",
    "    \n",
    "    # Step 7: Plot Explanations\n",
    "    for snid in example_snids:\n",
    "        plot_lime_for_snid(\n",
    "            snid=snid,\n",
    "            lime_explainer=lime_explainer,\n",
    "            snid_to_index=snid_to_index,\n",
    "            pipeline=combined_pipeline,\n",
    "            aligned_results=results,  # Ensure this is correctly defined\n",
    "            false_negative_snids=false_negative_snids,\n",
    "            false_positive_snids=false_positive_snids,\n",
    "            true_negative_snids=true_negative_snids,\n",
    "            true_positive_snids=true_positive_snids\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fe4d8-da6d-4024-a38b-b9104e474617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c0239-6db1-40a3-a722-0589c4a5f156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7deb5-547d-46e0-a4f4-3ac064a6c925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6752c-f223-4553-9bd6-0b0c728de3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2dd4403-f3aa-43d5-a52a-e5d41bd3f770",
   "metadata": {},
   "source": [
    "## Understanding Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696bc74-6596-4d81-8225-2e7f49e71243",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score, precision_recall_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # To include SMOTE/ADASYN in the pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# ---------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------\n",
    "\n",
    "def find_threshold_for_precision(y_true, y_probs, desired_precision):\n",
    "    \"\"\"\n",
    "    Find the threshold that achieves at least the desired precision.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels.\n",
    "    - y_probs: Predicted probabilities for the positive class.\n",
    "    - desired_precision: Desired precision level (e.g., 0.80 for 80%).\n",
    "\n",
    "    Returns:\n",
    "    - threshold: Threshold value achieving at least the desired precision.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    # Exclude the last precision value which has no corresponding threshold\n",
    "    precision = precision[:-1]\n",
    "    thresholds = thresholds\n",
    "\n",
    "    # Find indices where precision is >= desired_precision\n",
    "    indices = np.where(precision >= desired_precision)[0]\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        # Choose the first threshold where precision >= desired_precision\n",
    "        # This corresponds to the smallest threshold achieving desired precision\n",
    "        return thresholds[indices[0]]\n",
    "    else:\n",
    "        # If desired precision is not achievable, return None\n",
    "        print(f\"No threshold found to achieve {desired_precision*100}% precision.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, strategy_name, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with percentages and counts.\n",
    "\n",
    "    Parameters:\n",
    "    - cm: Confusion matrix.\n",
    "    - strategy_name: Name of the strategy for the title.\n",
    "    - normalize: Whether to normalize the confusion matrix per true label.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        cm_normalized = cm\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(\n",
    "        cm_normalized, annot=False, fmt='.2f', cmap='Blues',\n",
    "        xticklabels=['Non-TDE', 'TDE'],\n",
    "        yticklabels=['Non-TDE', 'TDE'],\n",
    "        cbar=False\n",
    "    )\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if normalize:\n",
    "                percentage = cm_normalized[i, j] * 100\n",
    "                count = cm[i, j]\n",
    "                text = f'{percentage:.1f}%\\n({count})'\n",
    "            else:\n",
    "                text = f'{cm[i, j]}'\n",
    "            plt.text(\n",
    "                j + 0.5, i + 0.5, text,\n",
    "                ha='center', va='center',\n",
    "                color='black', fontsize=12,\n",
    "                bbox=dict(facecolor='white', edgecolor='white')\n",
    "            )\n",
    "\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix - {strategy_name}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve_with_thresholds(y_true, y_probs, strategy_name, desired_precisions=[0.80, 0.95]):\n",
    "    \"\"\"\n",
    "    Plot Precision and Recall as functions of the threshold, marking specified thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels.\n",
    "    - y_probs: Predicted probabilities for the positive class.\n",
    "    - strategy_name: Name of the strategy for identification.\n",
    "    - desired_precisions: List of desired precision levels to mark.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    # Exclude the last precision and recall values which have no corresponding threshold\n",
    "    precision = precision[:-1]\n",
    "    recall = recall[:-1]\n",
    "    thresholds = thresholds\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precision, label='Precision', color='blue')\n",
    "    plt.plot(thresholds, recall, label='Recall', color='green')\n",
    "\n",
    "    # Find and plot thresholds for desired precisions\n",
    "    for dp in desired_precisions:\n",
    "        thresh = find_threshold_for_precision(y_true, y_probs, dp)\n",
    "        if thresh is not None:\n",
    "            label = f'{int(dp*100)}% Precision Threshold ({thresh:.2f})'\n",
    "            color = 'red' if dp == 0.80 else 'purple'\n",
    "            plt.axvline(x=thresh, linestyle='--', color=color, label=label)\n",
    "        else:\n",
    "            print(f\"{strategy_name}: Desired precision of {int(dp*100)}% not achievable.\")\n",
    "\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Precision and Recall vs. Threshold - {strategy_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(pipeline, param_grid, X_train, y_train, X_test, y_test, strategy_name):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning, train the model, make predictions, and evaluate performance.\n",
    "\n",
    "    Parameters:\n",
    "    - pipeline: scikit-learn or imblearn pipeline.\n",
    "    - param_grid: Dictionary of hyperparameters for GridSearchCV.\n",
    "    - X_train, y_train: Training data.\n",
    "    - X_test, y_test: Testing data.\n",
    "    - strategy_name: Name of the strategy for identification.\n",
    "\n",
    "    Returns:\n",
    "    - best_estimator: The best pipeline after GridSearchCV.\n",
    "    - metrics: Dictionary containing evaluation metrics and predictions.\n",
    "    \"\"\"\n",
    "    # Initialize Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters for {strategy_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best ROC AUC for {strategy_name}: {grid_search.best_score_:.4f}\\n\")\n",
    "\n",
    "    # Make predictions with the best estimator\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    y_probs = best_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Non-TDE', 'TDE'])\n",
    "    roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(cm, strategy_name, normalize=True)\n",
    "\n",
    "    # Print classification report and ROC AUC\n",
    "    print(f\"Classification Report for {strategy_name}:\\n{report}\")\n",
    "    print(f\"ROC AUC for {strategy_name}: {roc_auc:.4f}\\n\")\n",
    "\n",
    "    return best_estimator, {\n",
    "        'Confusion Matrix': cm,\n",
    "        'Classification Report': report,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'Predicted Probabilities': y_probs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fba4f-40af-45f5-8114-52d75640bbd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "df0['Simple_Object_Type'] = df0['Object_Type'].apply(lambda x: 1 if x == 'TDE' else 0)\n",
    "\n",
    "# Select numeric columns for the model and exclude specified columns\n",
    "excluded_columns = ['SNID', 'PeakMag', 'PeakMagErr', 'REDSHIFT_FINAL', 'REDSHIFT_FINAL_ERR']\n",
    "excluded_columns += [col for col in df0.columns if 'err' in col.lower() or 'flux' in col.lower() or 'mjd' in col.lower()]\n",
    "\n",
    "feature_columns = [col for col in df0.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
    "\n",
    "# Define a renaming map for clarity (if applicable)\n",
    "rename_map = {\n",
    "    'Mean_Color_Pre_Peak_gr': 'Mean Color Pre Peak (g-r)',\n",
    "    'Mean_Color_Post_Peak_gr': 'Mean Color Post Peak (g-r)',\n",
    "    'Mean_Color_Pre_Peak_ri': 'Mean Color Pre Peak (r-i)',\n",
    "    'Mean_Color_Post_Peak_ri': 'Mean Color Post Peak (r-i)',\n",
    "    'Slope_Pre_Peak_gr': 'Slope Pre Peak (g-r)',\n",
    "    'Slope_Post_Peak_gr': 'Slope Post Peak (g-r)',\n",
    "    'Slope_Pre_Peak_ri': 'Slope Pre Peak (r-i)',\n",
    "    'Slope_Post_Peak_ri': 'Slope Post Peak (r-i)',\n",
    "    'Rise_Time': 'Rise time',\n",
    "    'Fade_Time': 'Fade time'\n",
    "}\n",
    "\n",
    "# Rename columns for clarity (if necessary)\n",
    "df0.rename(columns=rename_map, inplace=True)\n",
    "feature_columns = [rename_map.get(col, col) for col in feature_columns]\n",
    "\n",
    "# Define features and target\n",
    "X = df0[feature_columns]\n",
    "y = df0['Simple_Object_Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11f5c4-876f-4f45-a347-dcc77cc908b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93beab6f-a6ed-4b33-ac9f-41e1e3327d70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Strategy 1: XGBoost with Mean Imputation and SMOTE\n",
    "pipeline_xgb_smote = ImbPipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [100, 500, 1000],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c353dc-806b-45b1-a82f-d117bf336043",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Strategy 2: Random Forest with Median Imputation and Class Weights\n",
    "pipeline_rf_class_weights = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 300, 500],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__bootstrap': [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b39db-8a94-4058-b517-392197664dff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Strategy 3: Logistic Regression with KNN Imputation and ADASYN\n",
    "pipeline_lr_adasyn = ImbPipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('adasyn', ADASYN(random_state=42)),\n",
    "    ('classifier', LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'classifier__solver': ['lbfgs', 'saga'],\n",
    "    'classifier__penalty': ['l2']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e598a-a986-44d3-a2ee-d4d8b2df0978",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Strategy 4: XGBoost without SMOTE\n",
    "pipeline_xgb_no_smote = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Mean imputation\n",
    "    ('scaler', StandardScaler()),                 # Feature scaling\n",
    "    ('classifier', xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for XGBoost without SMOTE\n",
    "param_grid_xgb_no_smote = {\n",
    "    'classifier__n_estimators': [100, 500, 1000],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e9c65-016a-497a-943b-78caabbd02ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Strategy 5: Neural Network Classifier with ADASYN\n",
    "pipeline_nn_adasyn = ImbPipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),      # KNN imputation\n",
    "    ('scaler', StandardScaler()),                 # Feature scaling\n",
    "    ('adasyn', ADASYN(random_state=42)),         # ADASYN for class imbalance\n",
    "    ('classifier', MLPClassifier(\n",
    "        hidden_layer_sizes=(100,),                # Default architecture\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for Neural Network Classifier\n",
    "param_grid_nn = {\n",
    "    'classifier__hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "    'classifier__activation': ['relu', 'tanh'],\n",
    "    'classifier__solver': ['adam', 'sgd'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800291c-0963-4f73-ab72-16487789f11b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Strategy 1: XGBoost with Mean Imputation and SMOTE\n",
    "print(\"=== Strategy 1: XGBoost with Mean Imputation and SMOTE ===\\n\")\n",
    "best_xgb_smote, metrics_xgb_smote = train_and_evaluate(\n",
    "    pipeline=pipeline_xgb_smote,\n",
    "    param_grid=param_grid_xgb,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    strategy_name='XGBoost with SMOTE'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4a817-fc77-43b4-8f2b-0f767c9721d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Strategy 2: Random Forest with Median Imputation and Class Weights\n",
    "print(\"=== Strategy 2: Random Forest with Median Imputation and Class Weights ===\\n\")\n",
    "best_rf_class_weights, metrics_rf_class_weights = train_and_evaluate(\n",
    "    pipeline=pipeline_rf_class_weights,\n",
    "    param_grid=param_grid_rf,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    strategy_name='Random Forest with Class Weights'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b7115-ab36-41cc-aa43-790db5e002c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Strategy 3: Logistic Regression with KNN Imputation and ADASYN\n",
    "print(\"=== Strategy 3: Logistic Regression with KNN Imputation and ADASYN ===\\n\")\n",
    "best_lr_adasyn, metrics_lr_adasyn = train_and_evaluate(\n",
    "    pipeline=pipeline_lr_adasyn,\n",
    "    param_grid=param_grid_lr,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    strategy_name='Logistic Regression with ADASYN'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fc592-2488-406d-9ac9-df9a1e7f913e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Strategy 4: XGBoost without SMOTE\n",
    "print(\"=== Strategy 4: XGBoost without SMOTE ===\\n\")\n",
    "best_xgb_no_smote, metrics_xgb_no_smote = train_and_evaluate(\n",
    "    pipeline=pipeline_xgb_no_smote,\n",
    "    param_grid=param_grid_xgb_no_smote,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    strategy_name='XGBoost without SMOTE'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b80ac-948d-4a11-bb2c-c928da799f07",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Strategy 5: Neural Network Classifier with ADASYN\n",
    "print(\"=== Strategy 5: Neural Network Classifier with ADASYN ===\\n\")\n",
    "best_nn_adasyn, metrics_nn_adasyn = train_and_evaluate(\n",
    "    pipeline=pipeline_nn_adasyn,\n",
    "    param_grid=param_grid_nn,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    strategy_name='Neural Network with ADASYN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38369b8-9f64-4e6f-9616-f814bdb6b594",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Precision-Recall Curves for each strategy with threshold markings\n",
    "plot_precision_recall_curve_with_thresholds(\n",
    "    y_test,\n",
    "    metrics_xgb_smote['Predicted Probabilities'],\n",
    "    'XGBoost with SMOTE'\n",
    ")\n",
    "\n",
    "plot_precision_recall_curve_with_thresholds(\n",
    "    y_test,\n",
    "    metrics_rf_class_weights['Predicted Probabilities'],\n",
    "    'Random Forest with Class Weights'\n",
    ")\n",
    "\n",
    "#plot_precision_recall_curve_with_thresholds(\n",
    "#    y_test,\n",
    "#    metrics_lr_adasyn['Predicted Probabilities'],\n",
    "#    'Logistic Regression with ADASYN'\n",
    "#)\n",
    "\n",
    "plot_precision_recall_curve_with_thresholds(\n",
    "    y_test,\n",
    "    metrics_xgb_no_smote['Predicted Probabilities'],\n",
    "    'XGBoost without SMOTE'\n",
    ")\n",
    "\n",
    "#plot_precision_recall_curve_with_thresholds(\n",
    "#    y_test,\n",
    "#    metrics_nn_adasyn['Predicted Probabilities'],\n",
    "#    'Neural Network with ADASYN'\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c103f0-ddbc-414f-8d14-1ed7fd6856a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Extend the summary DataFrame\n",
    "summary_extended = pd.DataFrame({\n",
    "    'Strategy': [\n",
    "        'XGBoost with SMOTE',\n",
    "        'Random Forest with Class Weights',\n",
    "   #     'Logistic Regression with ADASYN',\n",
    "        'XGBoost without SMOTE',\n",
    "   #     'Neural Network with ADASYN'\n",
    "    ],\n",
    "    'ROC AUC': [\n",
    "        metrics_xgb_smote['ROC AUC'],\n",
    "        metrics_rf_class_weights['ROC AUC'],\n",
    "   #     metrics_lr_adasyn['ROC AUC'],\n",
    "        metrics_xgb_no_smote['ROC AUC'],\n",
    "   #     metrics_nn_adasyn['ROC AUC']\n",
    "    ],\n",
    "    'Precision (TDE)': [\n",
    "        float(metrics_xgb_smote['Classification Report'].split('\\n')[2].split()[1]),\n",
    "        float(metrics_rf_class_weights['Classification Report'].split('\\n')[2].split()[1]),\n",
    "    #    float(metrics_lr_adasyn['Classification Report'].split('\\n')[2].split()[1]),\n",
    "        float(metrics_xgb_no_smote['Classification Report'].split('\\n')[2].split()[1]),\n",
    "    #    float(metrics_nn_adasyn['Classification Report'].split('\\n')[2].split()[1])\n",
    "    ],\n",
    "    'Recall (TDE)': [\n",
    "        float(metrics_xgb_smote['Classification Report'].split('\\n')[2].split()[2]),\n",
    "        float(metrics_rf_class_weights['Classification Report'].split('\\n')[2].split()[2]),\n",
    "   #     float(metrics_lr_adasyn['Classification Report'].split('\\n')[2].split()[2]),\n",
    "        float(metrics_xgb_no_smote['Classification Report'].split('\\n')[2].split()[2]),\n",
    "   #     float(metrics_nn_adasyn['Classification Report'].split('\\n')[2].split()[2])\n",
    "    ],\n",
    "    'F1-Score (TDE)': [\n",
    "        float(metrics_xgb_smote['Classification Report'].split('\\n')[2].split()[3]),\n",
    "        float(metrics_rf_class_weights['Classification Report'].split('\\n')[2].split()[3]),\n",
    "    #    float(metrics_lr_adasyn['Classification Report'].split('\\n')[2].split()[3]),\n",
    "        float(metrics_xgb_no_smote['Classification Report'].split('\\n')[2].split()[3]),\n",
    "   #     float(metrics_nn_adasyn['Classification Report'].split('\\n')[2].split()[3])\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Extended Performance Summary ===\")\n",
    "print(summary_extended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5da47-0311-472c-91aa-111e66ee182b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: Verify Thresholds for 80% and 95% Precision in Random Forest with Class Weights\n",
    "desired_precisions = [0.80, 0.95]\n",
    "strategy_name = 'Random Forest with Class Weights'\n",
    "\n",
    "for dp in desired_precisions:\n",
    "    thresh = find_threshold_for_precision(y_test, metrics_rf_class_weights['Predicted Probabilities'], dp)\n",
    "    if thresh is not None:\n",
    "        print(f\"{strategy_name}: Threshold for {int(dp*100)}% precision = {thresh:.4f}\")\n",
    "    else:\n",
    "        print(f\"{strategy_name}: {int(dp*100)}% precision not achievable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020ae4e-fadc-4975-ae31-abd9bc06c13f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Feature Importance for XGBoost without SMOTE\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "xgb.plot_importance(\n",
    "    best_xgb_no_smote.named_steps['classifier'],\n",
    "    max_num_features=20,\n",
    "    importance_type='weight',\n",
    "    ax=ax,\n",
    "    show_values=False\n",
    ")\n",
    "# Set feature names\n",
    "feature_names = feature_columns  # Ensure feature names are correctly mapped\n",
    "ax.set_yticklabels(feature_names, fontsize=10)\n",
    "ax.set_xlabel('Feature Importance (Weight)', fontsize=12)\n",
    "ax.set_title('XGBoost Feature Importance - Without SMOTE Classifier', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64bc67-2240-497c-9b72-b4580e820515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71081090-c43a-465f-a55c-5aa09ac82a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7c23d-7dd5-41d8-8587-26d54d9dc3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384a505-ff10-4652-a3a0-02a5ea85a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4bffd-ebe5-4fd6-8dd1-336d732209c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
